{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Feedforward Neural Network Model\n",
    "This notebook provides instructions on how to build a feedforward neural network model. It will introduce a couple of classes defined in the [FFBP package](https://github.com/alex-ten/pdpyflow/tree/master/FFBP): `FFBP.Layer` and `FFBP.Model` as well as describe some Tensorflow objects needed to successfully implement a feedforward neural network model with `FFBP`.\n",
    "\n",
    "To demonstrate the various ways in which FFBP layers can be connected together, our examle will be the [semantic cognition](https://stanford.edu/~jlmcc/papers/RogersMcC08BBSFinalProof.pdf) by Rogers and McClelland. The model is a feedforward neural network with two input layers, two hidden layers, and an output layer (as shown in the figure below).\n",
    "\n",
    "<img src=\"materials/semantic_network.png\" width=60%>\n",
    "\n",
    "Based on these interconnected layers we will construct a working neural network model.\n",
    "\n",
    "We begin by importing the packages required for this tutorial and creating a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs). We want to make sure that we are adding network elements in the context of this graph (see cells below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import FFBP\n",
    "\n",
    "FFBP_GRAPH = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "\n",
    "Input layers are implemented as [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) objects. `InputData` (covered in a [different tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb)) data from a text document into the `tf.float32` format. Therefore, the first argument inside the `tf.placeholder()` call is `dtype = tf.float32` (but it can be anything as long as it is consistent with the data that is actually fed into it). The `shape` parameter is optional, but we strictly indicate *at least* the second dimension because other objects in the software expect an integer value there. The `None` in the place of the first dimension of the `shape` parameter means that any size of the that dimension is allowed. For instance, `inp_1` is a placeholder for a 2-dimensional data tensor (of type `tf.float32`) with 3 columns and any number of rows. This is useful when we train a model with batched up input but test it on single examples.\n",
    "\n",
    "The target placeholder needs to be the same size as the final (output) layer of the network. It is a good practice to name tensorflow nodes informatively, so we add an optional `name` argument accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    MODEL_NAME = 'semantic_net'\n",
    "    with tf.name_scope(MODEL_NAME):\n",
    "        \n",
    "        # Create input placeholders\n",
    "        ITEM = tf.placeholder(dtype = tf.float32, shape=[None, 3], name='item_input')\n",
    "        REL  = tf.placeholder(dtype = tf.float32, shape=[None, 2], name='relation_input')\n",
    "\n",
    "        # Create a target placeholder\n",
    "        TARG = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden and output layers\n",
    "The transforming layers are created via the `FFBP.BasicLayer` interface. The `FFBP.BasicLayer` constructor takes the following arguments:\n",
    "- **`layer_name`** : a (string) name of the layer (this will be displayed in visualization)\n",
    "- **`layer_input`** : input to the layer. If input is a placeholder, just pass the corresponding variable handle to this parameter. If input comes from another `BasicLayer` object, we need to point to its `output` attribute (e.g. `hidden_layer.output`). Finally, if there are multiple layers / placeholders feeding into a single `BasicLayer` simply wrap the inputs into a list or a tuple (e.g. `(inp_1, inp_2, ..., inp_N)`)\n",
    "- **`size`** : the number of units in the layer\n",
    "- **`wrange`** : a (listed or tupled) pair of values corresponding to, respectively, the lower and upper bounds of the distribution from which weight values will be sampled uniformly upon initialization.\n",
    "- **`nonlin`** : (*`default`*`= None`) a function that takes in and outputs a tensor. If omitted, input to the layer will be transformed linearly (i.e. $Wx + b$). Tensorflow provides a number of widely used nonlinear functions documented [here](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_).\n",
    "- **`seed`** : (*`default`*`= None`) the seed for random weight initialization. If given a negative value or omited, the initialization will be irreproducible.\n",
    "\n",
    "`FFBP.BasicLayer`s are modular and can be configured with different sizes, nonlinearities, weight values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    with tf.name_scope(MODEL_NAME):\n",
    "\n",
    "        REPR = FFBP.BasicLayer(\n",
    "            layer_name = 'representation_layer', \n",
    "            layer_input = ITEM, \n",
    "            size = 8, \n",
    "            wrange = [-.1, .1], \n",
    "            nonlin = tf.nn.sigmoid, \n",
    "        )\n",
    "        \n",
    "        HID = FFBP.BasicLayer(\n",
    "            layer_name = 'hidden_layer', \n",
    "            layer_input = (REPR.output, REL), \n",
    "            size = 15, \n",
    "            wrange = [-.5, .5], \n",
    "            nonlin = None,\n",
    "        )\n",
    "        \n",
    "        ATTR = FFBP.BasicLayer(\n",
    "            layer_name = 'attribute_layer', \n",
    "            layer_input = HID.output, \n",
    "            size = 5, \n",
    "            wrange = [-.01, .02], \n",
    "            nonlin = tf.nn.sigmoid\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFBP Model\n",
    "We can now create a feedforward model by instantiating an `FFBP.Model` class. The class constructor takes the following arguments:\n",
    "- **`name`** : the name of the model (used for storage and visualization).\n",
    "- **`loss`** : the loss (objective) function of the model.\n",
    "- **`optimizer`** : model optimizer. This can be provided separately with `train_setup` or `set_optimizer` method. It is possible to write your own optimizers, but Tensorflow includes many widely used [optimizers](https://www.tensorflow.org/api_guides/python/train), which we use throughout the examples and tutorials in pdpyflow.\n",
    "- **`layers`** : a list of model layers. The order is consequential for visualization. The last layer should be the output layer so that the viewer displays target output information for this layer, but not the others.\n",
    "- **`inp`** : input placeholder or a list (or tuple) of input placeholders. The order of placeholders must correspond to the order of `inp_size` values set for `FFBP.InputData` (see the [input data tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb)).\n",
    "- **`train_data`** : `(`*`default`*`= None)` An instance of `FFBP.InputData` set up for training.\n",
    "- **`test_data`** : `(`*`default`*`= None)` An instance of `FFBP.InputData` set up for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    MODEL = FFBP.Model(\n",
    "        name = MODEL_NAME,\n",
    "        layers = [REPR, HID, ATTR], \n",
    "        inp    = [ITEM, REL],\n",
    "        targ   = TARG,\n",
    "        loss   = tf.squared_difference(TARG, OUT.output, name='loss_function')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing can also be set up separately:\n",
    "- **`test_setup`**`(`*`data`*`)` : sets up the model for testing separately from class initialization.\n",
    "- **`train_setup`**`(`*`data, optimizer=None`*`)` : sets up the model for training separately from class initialization.\n",
    "\n",
    "Finally, there are two special methods to actually run an epoch of training or testing. \n",
    "- **`test_epoch`**`(`*`session, verbose=False`*`)`: runs a single epoch of testing by feeding each item from the test set into the model and evaluating its state. This method returns two values: the loss accumulated across test items, and the model snapshot which contains information about the model state at the time of test.  If `verbose` is `True`, the loss value will be printed to the output along with the corresponding `Epoch`.\n",
    "- **`train_epoch`**`(`*`session, verbose=False`*`)` : runs a single epoch of training, optimizing parameters after processing each mini-batch of training examples. Returns loss accumulated over input mini-batches. If `verbose` is `True`, this value will be printed to the output along with the corresponding `Epoch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The main purpose of this tutorial was to explain how a model is constructed with the FFBP package and Tensorflow. The next step is to actually run your model by training it on some data. Go to the [main tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/main_tutorial.ipynb) to see how this can be approached."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
