{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network Model with FFBP\n",
    "This notebook provides instructions on how to create, train, test, and analyze a feedforward neural network using pdpyflow's FFBP package and Tensorflow. If you don't have any experience with Tensorflow, try following through the [getting started tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/getting_started/getting_started.ipynb) or visiting the [website](https://www.tensorflow.org/get_started/) for more information. The FFBP package is intended to simplify the process of constructing a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs) for neural network modeling. A Tensorflow graph is a computational structure that *describes* the flow of data (tensors) through various computational operations. Thus, the processes of constructing a graph and running it are separate.\n",
    "\n",
    "In order to train or test a neural network we need to complete a few necessary steps:\n",
    "- Preparing input data ([tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb)) [&#x21F1;](#step1)\n",
    "- Constructing a model ([tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb)) [&#x21F1;](#step2)\n",
    "- Running the model [&#x21F1;](#step3)\n",
    "\n",
    "This tutorial gives an overview of these steps, but more information on some of them is provided in the corresponding links. See those if you feel like you need more understanding of what is laid out below. To illustrate how a neural network model can be created, trained, tested, and analyzed, we use as an example the model of [semantic cognition](https://mitpress.mit.edu/books/semantic-cognition) by Rogers and McClelland. The model is a feedforward neural network with two input layers, two hidden layers, and an output layer\n",
    "\n",
    "We begin by importing the packages required for this tutorial and creating a Tensorflow graph. We want to make sure that we are adding various network and data [enqueueing](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops/queues) elements to this graph. To achieve, this we will use the graph's handle with Python's **`with`** statements, threreby creating a [context manager](https://docs.python.org/3/reference/datamodel.html#context-managers) which allows elaborating the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexten/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import FFBP\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # Prevent unwanted logging messages by tensorflow\n",
    "\n",
    "FFBP_GRAPH = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'> </a>\n",
    "## Preparing input data\n",
    "A detailed documentation of the `FFBP.InputData` class is provided in a separate [tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb). Here, we create two instances of the `FFBP.InputData` class, one for training (`TRAIN_DATA`) and one for testing (`TEST_DATA`). The `shuffle_seed` argument is set to `1` (for reproducibility) only for the training data set; we don't want to shuffle the test items (so we omit it for `TEST_DATA`). Note that the `inp_size` argument is a tuple of two integers corresponding to the sizes of item and relation layers, respectively. Although we are using the same data set for training and testing, it is possible to use different ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    # Create data for training\n",
    "    TRAIN_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/semantic_data.txt',\n",
    "        batch_size = 1,\n",
    "        inp_size = (8,4), # same as the sizes of item and relation layers\n",
    "        targ_size = 36,   # same as the size of the attribute layer\n",
    "        data_len = 32,    # number of training examples\n",
    "        shuffle_seed = -1  # -N for random seed, N for non-random seed, None to turn off randomization\n",
    "    )\n",
    "    \n",
    "    # Create data for testing\n",
    "    TEST_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/semantic_data.txt',\n",
    "        batch_size = 1,\n",
    "        inp_size = (8,4), \n",
    "        targ_size = 36,\n",
    "        data_len = 32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'> </a>\n",
    "## Constructing a model\n",
    "A detailed documentation of the objects specified further, including `FFBP.BasicLayer` and `FFBP.Model` is provided in a separate [tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb). Below is an example of how to outline the network structure and specify the flow of data for the semantic network. We specify arbitrary random initialization seeds (`2`, `3`, and `4`, respectively) for reproducibility. Note that `hidden_layer` takes the concatenation of tensors computed by `representation_layer.output` and `relation_inp` as input, so the `layer_input` argument to this layer is given as a tuple containing the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = 0.25 # weight initialization range\n",
    "lr = .2   # learning rate\n",
    "m = 0     # momentum\n",
    "\n",
    "# Add network components to the graph\n",
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    MODEL_NAME = 'semantic_net'\n",
    "    with tf.name_scope(MODEL_NAME):\n",
    "        \n",
    "        # Input placeholders (input layers)\n",
    "        ITEM_INP     = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='item_inp')\n",
    "        RELATION_INP = tf.placeholder(dtype = tf.float32, shape=[None, 4], name='relation_inp')\n",
    "        \n",
    "        # First hidden layer (representation)\n",
    "        REPRESENTATION_LAYER = FFBP.BasicLayer(\n",
    "            layer_name = 'representation_layer', \n",
    "            layer_input = ITEM_INP, \n",
    "            size = 8, \n",
    "            wrange = [-wr, wr], \n",
    "            nonlin=tf.nn.sigmoid, \n",
    "            seed=2, # -N for random seed, N for seed = N\n",
    "        )\n",
    "        \n",
    "        # Second hidden layer\n",
    "        HIDDEN_LAYER = FFBP.BasicLayer(\n",
    "            layer_name = 'hidden_layer', \n",
    "            layer_input = (REPRESENTATION_LAYER.output, RELATION_INP),\n",
    "            size = 12, \n",
    "            wrange = [-wr, wr], \n",
    "            nonlin=tf.nn.sigmoid, \n",
    "            seed=3\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        ATTRIBUTE_LAYER = FFBP.BasicLayer(\n",
    "            layer_name = 'attribute_layer', \n",
    "            layer_input = HIDDEN_LAYER.output, \n",
    "            size = 36, \n",
    "            wrange = [-wr, wr], \n",
    "            nonlin=tf.nn.sigmoid, \n",
    "            seed=4\n",
    "        )\n",
    "        \n",
    "        # Target placeholder\n",
    "        TARGET = tf.placeholder(dtype = tf.float32, shape=[None, 36], name='targets')\n",
    "        \n",
    "        # Optimization specs\n",
    "        OPTIMIZER = tf.train.MomentumOptimizer(lr, m)\n",
    "        LOSS = tf.reduce_sum(tf.squared_difference(TARGET, ATTRIBUTE_LAYER.output))\n",
    "        \n",
    "        # Model\n",
    "        MODEL = FFBP.Model(\n",
    "            name       = MODEL_NAME,\n",
    "            layers     = [REPRESENTATION_LAYER, HIDDEN_LAYER, ATTRIBUTE_LAYER],\n",
    "            train_data = TRAIN_DATA, \n",
    "            test_data  = TEST_DATA,\n",
    "            inp        = [ITEM_INP, RELATION_INP],\n",
    "            targ       = TARGET,\n",
    "            loss       = LOSS,\n",
    "            optimizer  = OPTIMIZER,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'> </a>\n",
    "## Running the model\n",
    "The model is run inside two for-loops (one nested inside the other): the (inner) train loop and the (outer) run loop. In a single iteration of the run loop, model parameters are initialized either randomly or by restoration from an existing checkpoint directory. A single iteration of the inner train loop corresponds to a single epoch of training/testing. Thus, minimally, a valid run routine would look something like this:\n",
    "```python\n",
    "# start outer run loop\n",
    "for run_ind in range(NUM_RUNS):\n",
    "    \n",
    "    # open new session for existing graph\n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "        \n",
    "        # initialize variables and start queue\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "        \n",
    "        # start inner epoch loop\n",
    "        for i in range(NUM_EPOCHS):\n",
    "            test_loss, snapshot = MODEL.test_epoch(session=sess, verbose=True)\n",
    "            train_loss = MODEL.train_epoch(session=sess, verbose=False)\n",
    "        \n",
    "        # stop queues\n",
    "        coordinator.request_stop()\n",
    "        coordinator.join(threads)\n",
    "\n",
    "```\n",
    "Here, we start a new Tensorflow session for a `NUM_RUNS` number of iterations and iterate over the `range(NUM_RUNS)`. Within each session, we randomly initilize all global and local variables (this reinitializes model parameters and local queue variables), start the queues, and run the train loop. Inside the train loop we test and train the model for a `NUM_EPOCHS` number of times.\n",
    "\n",
    "In order to store test data, as well as to [save/restore](https://www.tensorflow.org/programmers_guide/saved_model) it we need to make a few additions. For this the `FFBP.ModelSaver` class is available. The constructor takes up to two arguments:\n",
    "- **`restore_from`** : (*default*=`None`) a string path to a restoration directory which contains model checkpoint files. The path can be either absolute (e.g. `/Users/username/path/to/file`) or relative to the working directory of the current notebook.\n",
    "- **`logdir`** : (*default*=`None`) a string path to the log directory where model parameters and test data will be saved. If `None` a new directory (e.g. `/logdirs/logdir_000`) is automatically created in the same location as the current notebook. Depending on what one decides to save, this directory will contain a checkpoints subdirectory from which model parameters could be restored and `runlog.pkl` file(s) containing test data from a single model run. Detailed instructions on how to access data from runlogs can be found [here](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/analyze_data.ipynb).\n",
    "\n",
    "The saver includes several methods for saving and/or restoring model parameters as well as for storing test data:\n",
    "- **`init_model`**`(`*`session, init_epoch=0`*`)` : initializes model parameters. If the saver contains a restore path, the parameters will be restored from the existing checkpoint directory. Otherwise, the parameters will be initialized anew, based on the value of `wrange` for each individual layer. The `session` argument is required and should be the current session of the run. The user has control over indexing the initial epoch via the `init_epoch` parameter. The value of `init_epoch` is returned if the model is initialized from scratch, and a restored value is returned if the model is restored from a checkpoint.\n",
    "- **`save_model`**`(`*`session, model`*`)` : saves a given *`model`* to the saver's `logdir` and returns the string path of the log directory.\n",
    "- **`save_test`**`(`*`snap, run_ind`*`)` : saves a given *`snapshot`* to the corresponding runlog indexed by *`run_ind`* and returns the string path of the log directory.\n",
    "- **`save_loss`**`(`*`loss, run_ind`*`)` : saves a given *`loss`* to the corresponding runlog indexed by *`run_ind`* and returns the string path of the log directory.\n",
    "\n",
    "Below provide the run routine that executes a run of training, intermitted by occasional pre-tests. The training will proceed for the given number of epochs or until the train loss goes below the `ECRIT`. Regardless of when the training ends, the model will be tested one final time and saved (if `SAVE_FINAL=True`). Model parameters and test data will be stored at a new log directory, producing a new file indexed by `run_ind` (e.g. `checkpoints_directory_0` and `runlog_0.pkl`).\n",
    "\n",
    "All of the run parameters are brought to the top of the cell for presentation. We specify `TEST_EPOCHS` outside the run loop as a list of integer values. The model will be pre-tested on the 1<sup>st</sup> (epoch 0), 500<sup>th</sup>, 1000<sup>th</sup>, 2000<sup>th</sup>, and 5000<sup>th</sup> epochs. If the user wants to checkpoint the model occasionally, `SAVE_EPOCHS` should be a similar list specifying when the model parameters are to be saved. The code below does not checkpoint the model at any point, but the corresponding run parameters (`SAVE_EPOCHS=[None]` and `SAVE_FINAL=False`) are provided for illustration. Running the following cell will print out the test losses, indexed by the corresponding epoch and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf logdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFBP Saver: logdir path: /Users/alexten/Projects/pdpyflow/tutorials/building_models/logdirs/logdir_000\n",
      ">>> RUN 0\n",
      "FFBP Saver: initializing local and global variables from scratch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6914b858f6e44b5882d6dbe587306a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 279.54316997528076\n",
      "Epoch 499: 3.6676764809526503\n",
      "Epoch 999: 0.5689543002226856\n",
      "Epoch 1999: 0.1898130486079026\n",
      "Final test (loss < ecrit)\n",
      "Epoch 4972: 0.05843828448996646\n"
     ]
    }
   ],
   "source": [
    "# Set up run parameters\n",
    "NUM_RUNS = 1\n",
    "NUM_EPOCHS = 5000\n",
    "TEST_EPOCHS = [0, 499, 999, 1999, 4999]\n",
    "SAVE_EPOCHS = [None]\n",
    "SAVE_FINAL = False\n",
    "ECRIT = 0.05\n",
    "\n",
    "# Create a model saver\n",
    "saver = FFBP.ModelSaver(restore_from=None, logdir=None)\n",
    "\n",
    "# Start run loop\n",
    "for run_ind in range(NUM_RUNS):\n",
    "    print('>>> RUN {}'.format(run_ind))\n",
    "    \n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "\n",
    "        # restore or initialize FFBP_GRAPH variables:\n",
    "        start_epoch = saver.init_model(session=sess)\n",
    "\n",
    "        # create coordinator and start queue runners\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "        \n",
    "        # Start train loop\n",
    "        for i in FFBP.prog_bar(\n",
    "            sequence=range(start_epoch, start_epoch + NUM_EPOCHS), \n",
    "            name='Run {}/{}, Epoch'.format(run_ind,NUM_RUNS),\n",
    "            every=1):\n",
    "            \n",
    "            # Test model occasionally\n",
    "            if any([i==test_epoch for test_epoch in TEST_EPOCHS]):\n",
    "                test_loss, snapshot = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.save_test(snapshot, run_ind)\n",
    "\n",
    "            # Run one training epoch\n",
    "            train_loss = MODEL.train_epoch(session=sess, verbose=False)\n",
    "            saver.save_loss(train_loss, run_ind)\n",
    "\n",
    "            # Save model occasionally\n",
    "            if any([i==save_epoch for save_epoch in SAVE_EPOCHS]):\n",
    "                saver.save_model(session=sess, model=MODEL, run_ind=run_ind)\n",
    "\n",
    "            # Do final test, stop queues, and break out from training loop\n",
    "            if train_loss < ECRIT or i == start_epoch + (NUM_EPOCHS - 1): \n",
    "                print('Final test ({})'.format(\n",
    "                    'loss < ecrit' if train_loss < ECRIT else 'reached last epoch'))\n",
    "\n",
    "                test_loss, snapshot = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.save_test(snapshot, run_ind)\n",
    "\n",
    "                coordinator.request_stop()\n",
    "                coordinator.join(threads)\n",
    "\n",
    "                if SAVE_FINAL: saver.save_model(session=sess, model=MODEL, run_ind=run_ind)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
