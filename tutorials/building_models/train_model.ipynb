{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network Model with FFBP\n",
    "This notebook provides instructions on how to train a feedforward neural network using pdpyflow's FFBP package and Tensorflow. The FFBP package is intended to simplify the process of constructing a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs) for neural network modeling. Tensorflow graph is a computational structure that *describes* the flow of data (tensors) through various computational operations. Thus, the processes of constructing a graph and running it are separate. \n",
    "\n",
    "In order to train or test a neural network we need to follow three steps:\n",
    "- Prepare input data ([tutorial notebook](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb)) [&#x21F1;](#step1)\n",
    "- Construct model ([tutorial notebook](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb)) [&#x21F1;](#step2)\n",
    "- Run model [&#x21F1;](#step3)\n",
    "\n",
    "We begin by importing the packages required for this tutorial and creating a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs). We want to make sure that we are adding network and data [queue](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops/queues) elements to this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import FFBP\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # Prevent unwanted logging messages by tensorflow\n",
    "\n",
    "FFBP_GRAPH = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'> </a>\n",
    "## Prepare Input Data\n",
    "Next, we need to create `FFBP.InputData` for training and testing. Refer to the corresponding [notebook tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb) to learn more on how to create an `InputData` object. In the cell below we create two `InputData` objects, `TRAIN_DATA` and `TEST_DATA` that will be used for training and testing, respectively. Note, by convention we capitalize the names of variables that are referenced across notebook cells (e.g. `FFBP_GRAPH`, `NUM_EPOCHS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "\n",
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    # Create data for training\n",
    "    TRAIN_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/auto_data_train.txt',\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        batch_size = 4,\n",
    "        data_len = 8,\n",
    "        inp_size = 8, \n",
    "        targ_size = 8,\n",
    "        shuffle_seed = None\n",
    "    )\n",
    "    # Create data for testing\n",
    "    TEST_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/auto_data_test.txt',\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        batch_size = 1,\n",
    "        inp_size = 8, \n",
    "        targ_size = 8,\n",
    "        data_len = 15\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'> </a>\n",
    "## Construct Model\n",
    "Further, let's outline the network structure and specify the flow of data through it. A more detailed description of how this is done is provided in a separate [tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb). Here we set the same weight initialization range for the hidden and output layers by defining and referencing a single weight range (`wr`) variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wr = 0.25\n",
    "lr = .4\n",
    "m = 0\n",
    "\n",
    "# Add network components to the graph\n",
    "with FFBP_GRAPH.as_default():\n",
    "    model_name = 'autoencoder'\n",
    "    with tf.name_scope(model_name):\n",
    "        \n",
    "        # Create input and target placeholder\n",
    "        input_  = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='model_inp')\n",
    "        target = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='targets')\n",
    "        \n",
    "        # Create first hidden layer\n",
    "        hidden_layer = FFBP.BasicLayer(\n",
    "            layer_name = 'hidden_layer', \n",
    "            layer_input = input_, \n",
    "            size = 3, \n",
    "            wrange = [-wr, wr], \n",
    "            nonlin = tf.nn.sigmoid, \n",
    "            bias = True, \n",
    "            seed = None\n",
    "        )\n",
    "        \n",
    "        # Create another first-level hidden layer\n",
    "        output_layer = FFBP.BasicLayer(\n",
    "            layer_name = 'output_layer', \n",
    "            layer_input = hidden_layer.output, \n",
    "            size = 8, \n",
    "            wrange = [-wr, wr], \n",
    "            nonlin = tf.nn.sigmoid, \n",
    "            bias = True, \n",
    "            seed = None\n",
    "        )\n",
    "        \n",
    "        MODEL = FFBP.Model(\n",
    "            name = model_name,\n",
    "            layers = [hidden_layer, output_layer],\n",
    "            train_data = TRAIN_DATA, \n",
    "            inp        = input_,\n",
    "            targ       = target,\n",
    "            loss       = tf.squared_difference(target, output_layer.output, name='loss_function'),\n",
    "            optimizer  = tf.train.MomentumOptimizer(lr, m),\n",
    "            test_data  = TEST_DATA\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'> </a>\n",
    "## Run Model\n",
    "The model is run inside two for-loops (one nested inside the other): the (inner) train loop and the (outer) run loop. In a single iteration of the run loop, model parameters are initialized either randomly or by restoration from an existing checkpoint directory. A single iteration of the inner train loop corresponds to a single epoch of training/testing. Thus, minimally, a valid run routine would look something like this (in pseudocode):\n",
    "```python\n",
    "# start outer run loop\n",
    "for run_ind in range(NUM_RUNS):\n",
    "    \n",
    "    # open new session for existing graph\n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "        \n",
    "        # initialize variables and start queue\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "        \n",
    "        # start inner epoch loop\n",
    "        for i in range(NUM_EPOCHS):\n",
    "            testloss, snap = MODEL.test_epoch(session=sess, verbose=True)\n",
    "            loss = MODEL.train_epoch(session=sess, verbose=False)\n",
    "        \n",
    "        # stop queues\n",
    "        coordinator.request_stop()\n",
    "        coordinator.join(threads)\n",
    "\n",
    "```\n",
    "Here, we start a new Tensorflow session for each run iteration and iterate over the `range(NUM_RUNS)`. Within each session we initilize global and local variables (this reinitializes model parameters and local queue variables),  start the queues, and run the train loop. Inside the train loop we test and train the model for a `NUM_EPOCHS` number of times.\n",
    "\n",
    "In order to store test data, as well as to [save/restore](https://www.tensorflow.org/programmers_guide/saved_model) it we need to make a few additions. For this the `FFBP.ModelSaver` class is available. The constructor takes up to two arguments:\n",
    "- **`restore_from`** : (*default*=`None`) a string path to a restoration directory which contains model checkpoint files. The path can be either absolute (e.g. `/Users/username/path/to/file`) or relative to the working directory of the current notebook.\n",
    "- **`logdir`** : (*default*=`None`) a string path to the log directory where model parameters and test data will be saved. If `None` a new directory (e.g. `/logdirs/logdir_000`) is automatically created in the same location as the current notebook. Depending on what one decides to save, this directory will contain a checkpoints subdirectory from which model parameters could be restored and `runlog.pkl` file(s) containing test data from a single model run. Some instructions on how to access data from runlogs can be found [here](link).\n",
    "\n",
    "The saver includes several methods for saving and/or restoring model parameters as well as for storing test data:\n",
    "- **`init_model`**`(`*`session, init_epoch=0`*`)` : initializes model parameters. If the saver contains a restore path, the parameters will be restored from the existing checkpoint directory. Otherwise, the parameters will be initialized anew based on the value of `wrange` for each individual layer. The `session` argument is required and should be the current session of the run. The user has control over indexing the initial epoch through the `init_epoch` parameter. The value of `init_epoch` is returned if the model is initialized from scratch, and a restored value is returned if the model is restored from a checkpoint.\n",
    "- **`save_model`**`(`*`session, model`*`)` : saves a given *`model`* to the saver's `logdir`.\n",
    "- **`save_test`**`(`*`snap, run_ind`*`)` : saves a given *`snapshot`* to the corresponding runlog indexed by *`run_ind`*.\n",
    "- **`save_loss`**`(`*`loss, run_ind`*`)` : saves a given *`loss`* to the corresponding runlog indexed by *`run_ind`*.\n",
    "\n",
    "Below provide the run routine that executes `3` runs of training, intermitted by occasional tests. The training will run for the given number of epochs or until the train loss goes below the `ECRIT`. Regardless of when the training ends, the model will be tested one final time and saved. Model parameters and test data go to the corresponding log directory, each run producing new files indexed by `run_ind` (e.g. for `run_ind=0`, the model is saved to `checkpoints_directory_0`, and tast data is written to `runlog_0.pkl`).\n",
    "\n",
    "We define `TEST_EPOCHS` beforehand as a list of integer values: `[i for i in range(0,NUM_EPOCHS,100)]` (if you are not familiar with Python's list comprehensions, consult this [page](http://www.pythonforbeginners.com/basics/list-comprehensions-in-python) or search for 'python list comprehensions'). Thus, the model will be tested on every hundredth epoch. `SAVE_EPOCHS` is a similar list with a single element corresponding to the last epoch. Try out the code below, tweaking parameters as you please. Take note of new files that get added to this notebooks working directory in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up run parameters\n",
    "NUM_RUNS = 3\n",
    "TEST_EPOCHS = [i for i in range(0,NUM_EPOCHS,100)]\n",
    "SAVE_EPOCHS = [NUM_EPOCHS-1]\n",
    "ECRIT = 0.01\n",
    "\n",
    "# Create ModelSaver\n",
    "saver = FFBP.ModelSaver(restore_from=None, logdir=None)\n",
    "\n",
    "for run_ind in range(NUM_RUNS):\n",
    "    print('>>> RUN {}'.format(run_ind))\n",
    "    \n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "\n",
    "        # restore or initialize FFBP_GRAPH variables:\n",
    "        start_epoch = saver.init_model(session=sess)\n",
    "\n",
    "        # create coordinator and start queue runners\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "\n",
    "        for i in range(start_epoch, start_epoch + NUM_EPOCHS):\n",
    "            # Test model occasionally\n",
    "            if any([i==test_epoch for test_epoch in TEST_EPOCHS]):\n",
    "                testloss, snap = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.save_test(snap, run_ind)\n",
    "\n",
    "            # Run one training epoch\n",
    "            trainloss = MODEL.train_epoch(session=sess, verbose=False)\n",
    "            saver.save_loss(trainloss, run_ind)\n",
    "\n",
    "            # Save model occasionally\n",
    "            if any([i==save_epoch for save_epoch in SAVE_EPOCHS]):\n",
    "                saver.save_model(session=sess, model=MODEL, run_ind=run_ind)\n",
    "\n",
    "            # Do final test, stop queues, and break out from training loop\n",
    "            if trainloss < ECRIT or i == start_epoch + (NUM_EPOCHS - 1): \n",
    "                print('Final test ({})'.format(\n",
    "                    'loss < ecrit' if trainloss < ECRIT else 'num_epochs reached'))\n",
    "\n",
    "                testloss, snap = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.save_test(snap, run_ind)\n",
    "\n",
    "                coordinator.request_stop()\n",
    "                coordinator.join(threads)\n",
    "\n",
    "                saver.save_model(session=sess, model=MODEL, run_ind=run_ind)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
