{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network Model with FFBP\n",
    "This notebook provides instructions on how to train a feedforward neural network using pdpyflow's FFBP package and Tensorflow. The FFBP package is intended to simplify the process of constructing a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs) for neural network modeling. Tensorflow graph is a computational structure that *describes* the flow of data (tensors) through various computational operations. Thus, the processes of constructing a graph and running it are separate. \n",
    "\n",
    "In order to train or test a neural network we need to follow three steps:\n",
    "- Prepare input data ([tutorial notebook](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb)) [&#x21F1;](#step1)\n",
    "- Construct model ([tutorial notebook](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb)) [&#x21F1;](#step2)\n",
    "- Run model [&#x21F1;](#step3)\n",
    "\n",
    "We begin by importing the packages required for this tutorial and creating a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs). We want to make sure that we are adding network and data queue elements to this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import FFBP\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # Prevent unwanted logging messages by tensorflow\n",
    "\n",
    "FFBP_GRAPH = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'> </a>\n",
    "## Prepare Input Data\n",
    "Next, we need to create `FFBP.InputData` for training and testing. Refer to the corresponding [notebook tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/FFBP_network/prepare_input.ipynb) to learn more on how to create an `InputData` object. In the cell below we create two `InputData` objects, `TRAIN_DATA` and `TEST_DATA` that will be used for training and testing, respectively. Note, by convention we capitalize the names of variables that are referenced accross notebook cells (e.g. `FFBP_GRAPH`, `NUM_EPOCHS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "\n",
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    # Create data for training\n",
    "    TRAIN_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'auto_data_train.txt',\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        batch_size = 4,\n",
    "        data_len = 8,\n",
    "        inp_size = 8, \n",
    "        targ_size = 8,\n",
    "        shuffle_seed = SHUFFLE\n",
    "    )\n",
    "    # Create data for testing\n",
    "    TEST_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'auto_data_test.txt',\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        batch_size = 15,\n",
    "        inp_size = 8, \n",
    "        targ_size = 8,\n",
    "        data_len = 15\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'> </a>\n",
    "## Construct Model\n",
    "Now we outline the network structure and specify the flow of data through it. A more detailed description of how this is done is provided in a separate [tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_and_training_FFBP_network/connect_layers.ipynb). We set the same weight initialization range for the hidden and output layers, but it can be controlled individually for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wr = 0.25\n",
    "\n",
    "# Add network components to the graph\n",
    "with FFBP_GRAPH.as_default():\n",
    "    model_name = 'autoencoder'\n",
    "    with tf.name_scope(model_name):\n",
    "        \n",
    "        # Create input and target placeholder\n",
    "        input_  = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='model_inp')\n",
    "        target = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='targets')\n",
    "        \n",
    "        # Create first hidden layer\n",
    "        hidden_layer = FFBP.BasicLayer(\n",
    "            layer_name = 'hidden_layer', \n",
    "            layer_input = input_, \n",
    "            size = 3, \n",
    "            wrange = [-wr, wr], \n",
    "            nonlin = tf.nn.sigmoid, \n",
    "            bias = True, \n",
    "            seed = None\n",
    "        )\n",
    "        \n",
    "        # Create another first-level hidden layer\n",
    "        output_layer = FFBP.BasicLayer(\n",
    "            layer_name = 'output_layer', \n",
    "            layer_input = hidden_layer.output, \n",
    "            size = 8, \n",
    "            wrange = [-wr, wr], \n",
    "            nonlin = tf.nn.sigmoid, \n",
    "            bias = True, \n",
    "            seed = None\n",
    "        )\n",
    "        \n",
    "        MODEL = FFBP.Model(\n",
    "            name = model_name,\n",
    "            layers = [hidden_layer, output_layer],\n",
    "            train_data = train_data, \n",
    "            inp        = input_,\n",
    "            targ       = target,\n",
    "            loss       = tf.reduce_sum(tf.squared_difference(target, output_layer.output), name='loss_function'),\n",
    "            optimizer  = tf.train.MomentumOptimizer(lr, m),\n",
    "            test_data  = test_data\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'> </a>\n",
    "## Run Model\n",
    "The model is run inside two for-loops (one nested inside the other): the (inner) train loop and the (outer) run loop. In a single iteration of the run loop, model parameters will be initialized either randomly or by a restoration from an existing checkpoint directory. A single iteration of the inner train loop corresponds to a single epoch of training/testing. Thus, minimally, a valid run code would look something like:\n",
    "```python\n",
    "for run_ind in range(NUM_RUNS):\n",
    "\n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "        \n",
    "        for i in range(NUM_EPOCHS):\n",
    "            testloss, snap = MODEL.test_epoch(session=sess, verbose=True)\n",
    "            loss = MODEL.train_epoch(session=sess, verbose=False)\n",
    "\n",
    "        coordinator.request_stop()\n",
    "        coordinator.join(threads)\n",
    "\n",
    "```\n",
    "Here, we start a new Tensorflow session for each run iteration and iterate over NUM_RUNS. Within each session we initilize global and local variables (this reinitializes model parameters and local queue variables),  start the queues, and run the train loop. Inside the train loop we test and train the model for NUM_EPOCHS number of times.\n",
    "\n",
    "In order to make the runs a bit more nuanced we need to add a few elements to this code.  like saving test data and/or saving and restoring a model from a checkpoint, we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up run parameters\n",
    "NUM_RUNS = 1\n",
    "TEST_EPOCHS = [0,1,3,5,30,60,120,180,270,300]\n",
    "SAVE_EPOCHS = [NUM_EPOCHS-1]\n",
    "ECRIT = 0.01\n",
    "\n",
    "# Create ModelSaver\n",
    "saver = FFBP.ModelSaver(restore_from=None, make_new_logdir=True)\n",
    "\n",
    "for run_ind in range(NUM_RUNS):\n",
    "    print('>>> RUN {}'.format(run_ind))\n",
    "    \n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "\n",
    "        # restore or initialize FFBP_GRAPH variables:\n",
    "        start_epoch = saver.init_model(session=sess)\n",
    "\n",
    "        # create coordinator and start queue runners\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "\n",
    "        for i in range(start_epoch, start_epoch + NUM_EPOCHS):\n",
    "            # Test model occasionally\n",
    "            if any([i==test_epoch for test_epoch in TEST_EPOCHS]):\n",
    "                testloss, snap = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.save_test(snap, run_ind)\n",
    "\n",
    "            # Run one training epoch\n",
    "            loss = MODEL.train_epoch(session=sess, verbose=False)\n",
    "            saver.save_loss(loss, run_ind)\n",
    "\n",
    "            # Save model occasionally\n",
    "            if any([i==save_epoch for save_epoch in SAVE_EPOCHS]):\n",
    "                saver.save_model(session=sess, model=MODEL)\n",
    "\n",
    "            # Do final test, stop queues, and break out from training loop\n",
    "            if loss < ECRIT or i == start_epoch + (NUM_EPOCHS - 1): \n",
    "                print('Final test ({})'.format(\n",
    "                    'loss < ecrit' if loss < ECRIT else 'num_epochs reached'))\n",
    "\n",
    "                testloss, snap = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.save_test(snap, run_ind)\n",
    "\n",
    "                coordinator.request_stop()\n",
    "                coordinator.join(threads)\n",
    "\n",
    "                saver.save_model(session=sess, model=MODEL)\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
