{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network Model with FFBP\n",
    "This notebook provides instructions on how to create, train, test, and analyze a feedforward neural network model using pdpyflow's FFBP package and Tensorflow. If you don't have any experience with Tensorflow, try the [getting started tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/getting_started/getting_started.ipynb) or visit the [website](https://www.tensorflow.org/get_started/) to get more information. The FFBP package is intended to simplify the process of constructing a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs) for neural network modeling. A Tensorflow graph is a computational structure that *describes* the flow of data (tensors) through various computational operations. Thus, the processes of constructing a graph and running it are separate, as is the analysis of data generated by running a model.\n",
    "\n",
    "Therefore, training or testing, and analyzing a neural network involves the following steps:\n",
    "- Preparing input data ([tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb)) [&#x21F1;](#step1)\n",
    "- Constructing a model ([tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb)) [&#x21F1;](#step2)\n",
    "- Running the model [&#x21F1;](#step3)\n",
    "- Accessing and analyzing data ([tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/analyzing_data.ipynb)) [&#x21F1;](#step4)\n",
    "\n",
    "This tutorial gives an overview of these steps, but more information on some of them is provided in the corresponding links. See those if you feel like you need more understanding of what is laid out below. To illustrate how a neural network model can be created, trained, tested, and analyzed, we use as an example the model of [semantic cognition](https://stanford.edu/~jlmcc/papers/RogersMcC08BBSFinalProof.pdf) by Rogers and McClelland. The model is a simple feedforward neural network illustrated in the figure below. \n",
    "\n",
    "<img src=\"materials/semantic_network.png\" width=60%>\n",
    "\n",
    "As you can see in this figure, the network consists of two input layers (one called item and the other called relation), two hidden layers, (one called representation and the other called just hidden) and one output layer (called attribute). Where there are arrows between layers, they represent a full matrix of connections from every sending unit in the pool on the left to every receiving unit in the pool on the right.\n",
    "\n",
    "We begin by importing the packages required for this tutorial and creating a Tensorflow graph. We want to make sure that we are adding various network and data [enqueueing](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops/queues) elements to this graph. To achieve, this we will use the graph's handle with Python's **`with`** statements, thereby creating a [context manager](https://docs.python.org/3/reference/datamodel.html#context-managers) which allows elaborating the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import FFBP\n",
    "\n",
    "FFBP_GRAPH = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'> </a>\n",
    "## Preparing input data\n",
    "A detailed documentation of the `FFBP.InputData` class is provided in a separate [tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/prepare_input.ipynb). Here, we create two instances of the `FFBP.InputData` class, one for training (`TRAIN_DATA`) and one for testing (`TEST_DATA`). The `shuffle_seed` argument is set to `1` (for reproducibility) only for the training data set; we don't want to shuffle the test items (so we omit it for `TEST_DATA`). Note that the `inp_size` argument is a tuple of two integers corresponding to the sizes of item and relation layers, respectively. Although we are using the same data set for training and testing, it is possible to use different ones.\n",
    "\n",
    "In this case, the training and testing data both consist of the same set of 32 training cases (so `data_len = 32`), each consisting of a one-hot item input vector and a one-hot relation input vector (for example specifying that the item is \"canary\" and the relation is \"can\", as shown in the figure) and a vector of associated outputs (in this case the set of things a canary can do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    # Create data for training\n",
    "    TRAIN_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/semantic_data.txt',\n",
    "        batch_size        = 1,\n",
    "        inp_size          = (8,4), # same as the sizes of item and relation layers\n",
    "        targ_size         = 36,    # same as the size of the attribute layer\n",
    "        data_len          = 32,    # number of training examples\n",
    "        shuffle_seed      = -1     # -N for random seed, N for non-random seed, None to turn off randomization\n",
    "    )\n",
    "    \n",
    "    # Create data for testing\n",
    "    TEST_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/semantic_data.txt',\n",
    "        batch_size        = 1,\n",
    "        inp_size          = (8,4), \n",
    "        targ_size         = 36,\n",
    "        data_len          = 32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'> </a>\n",
    "## Constructing a model\n",
    "A more thorough documentation of the objects specified below, including `FFBP.BasicLayer` and `FFBP.Model` is provided in a separate [tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb). Below is an example of how to outline the network structure and specify the flow of data for the semantic network. We specify arbitrary random initialization seeds (`2`, `3`, and `4`, respectively) for reproducibility. Note that `hidden_layer` takes the concatenation of tensors computed by `representation_layer.output` and `relation_inp` as input, so the `layer_input` argument to this layer is given as a tuple containing the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = .1    # weight initialization range will be set between -.1 and .1\n",
    "s = -1  # (pseudo)randomization seed for weight init will be random\n",
    "\n",
    "# Add network components to the graph\n",
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    MODEL_NAME = 'semantic_net'\n",
    "    with tf.name_scope(MODEL_NAME):\n",
    "        \n",
    "        # Input placeholders (input layers)\n",
    "        ITEM_INP     = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='item_inp')\n",
    "        RELATION_INP = tf.placeholder(dtype = tf.float32, shape=[None, 4], name='relation_inp')\n",
    "        \n",
    "        # First hidden layer (representation)\n",
    "        REPRESENTATION_LAYER = FFBP.BasicLayer(\n",
    "            layer_name  = 'representation_layer', \n",
    "            layer_input = ITEM_INP, \n",
    "            size        = 8, \n",
    "            wrange      = [-wr, wr], \n",
    "            nonlin      = tf.nn.sigmoid, \n",
    "            seed        = s\n",
    "        )\n",
    "        \n",
    "        # Second hidden layer\n",
    "        HIDDEN_LAYER = FFBP.BasicLayer(\n",
    "            layer_name  = 'hidden_layer', \n",
    "            layer_input = (REPRESENTATION_LAYER.output, RELATION_INP),\n",
    "            size        = 15, \n",
    "            wrange      = [-wr, wr], \n",
    "            nonlin      = tf.nn.sigmoid, \n",
    "            seed        = s\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        ATTRIBUTE_LAYER = FFBP.BasicLayer(\n",
    "            layer_name  = 'attribute_layer', \n",
    "            layer_input = HIDDEN_LAYER.output, \n",
    "            size        = 36, \n",
    "            wrange      = [-wr, wr], \n",
    "            nonlin      = tf.nn.sigmoid, \n",
    "            seed        = s\n",
    "        )\n",
    "        \n",
    "        # Target placeholder\n",
    "        TARGET = tf.placeholder(dtype = tf.float32, shape=[None, 36], name='targets')\n",
    "        \n",
    "        # Optimization specs\n",
    "        OPTIMIZER = tf.train.GradientDescentOptimizer(learning_rate=.1, name='SGD_optimizer')\n",
    "        LOSS = tf.reduce_sum(tf.squared_difference(TARGET, ATTRIBUTE_LAYER.output), name='loss')\n",
    "        \n",
    "        # Model\n",
    "        MODEL = FFBP.Model(\n",
    "            name       = MODEL_NAME,\n",
    "            layers     = [REPRESENTATION_LAYER, HIDDEN_LAYER, ATTRIBUTE_LAYER],\n",
    "            train_data = TRAIN_DATA, \n",
    "            test_data  = TEST_DATA,\n",
    "            inp        = [ITEM_INP, RELATION_INP],\n",
    "            targ       = TARGET,\n",
    "            loss       = LOSS,\n",
    "            optimizer  = OPTIMIZER,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'> </a>\n",
    "## Running the model\n",
    "The model is run inside two for-loops (one nested inside the other): the (inner) train loop and the (outer) run loop. In a single iteration of the run loop, model parameters are initialized either randomly or by restoration from an existing checkpoint directory. A single iteration of the inner train loop corresponds to a single epoch of training/testing. Thus, minimally, a valid run routine would look something like this:\n",
    "```python\n",
    "# start outer run loop\n",
    "for run_ind in range(NUM_RUNS):\n",
    "    \n",
    "    # open new session for existing graph\n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "        \n",
    "        # initialize variables and start queue\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "        \n",
    "        # start inner epoch loop\n",
    "        for i in range(NUM_EPOCHS):\n",
    "            test_loss, snapshot = MODEL.test_epoch(session=sess, verbose=True)\n",
    "            train_loss = MODEL.train_epoch(session=sess, verbose=False)\n",
    "        \n",
    "        # stop queues\n",
    "        coordinator.request_stop()\n",
    "        coordinator.join(threads)\n",
    "\n",
    "```\n",
    "Here, for each of the `NUM_RUNS` different runs of the model, we start a new Tensorflow session. Within each session, we randomly initilize all global and local variables (this reinitializes model parameters and local queue variables), start the queues, and run the train loop. Inside the train loop we test and train the model for a `NUM_EPOCHS` number of times.\n",
    "\n",
    "In order to store test data, as well as to [save/restore](https://www.tensorflow.org/programmers_guide/saved_model) the model parameters we need to make a few additions. For this the `FFBP.ModelSaver` class is available. `FFBP.ModelSaver`'s constructor takes up to two arguments:\n",
    "- **`restore_from`** : (*default*=`None`) a string path to a restoration directory which contains model checkpoint files. The path can be either absolute (e.g. `/Users/username/path/to/file`) or relative to the working directory of the current notebook.\n",
    "- **`logdir`** : (*default*=`None`) a string path to the log directory where model parameters and test data will be saved. If `None` a new directory (e.g. `/logdirs/logdir_000`) is automatically created in the same location as the current notebook. Depending on what one decides to save, this directory will contain a checkpoints subdirectory from which model parameters could be restored and `runlog.pkl` file(s) containing test data from a single model run. Detailed instructions on how to access data from runlogs can be found [here](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/analyze_data.ipynb).\n",
    "\n",
    "The saver includes two methods relating to saving and/or restoring model parameters, and two methods for storing model-generated data:\n",
    "- **`init_model`**`(`*`session, init_epoch=0`*`)` : initializes model parameters. If the saver was given a restore path upon construction, the parameters will be restored from the existing checkpoint directory. Otherwise, the parameters will be initialized anew, based on the value of `wrange` for each individual layer. The `session` argument is required and should be the current session of the run. The user has control over indexing the initial epoch via the `init_epoch` parameter. The value of `init_epoch` is returned if the model is initialized from scratch, and a restored value is returned if the model is restored from a checkpoint.\n",
    "- **`save_model`**`(`*`session, model`*`)` : saves a given *`model`* to the saver's `logdir` and returns the string path of the log directory.\n",
    "- **`log_test`**`(`*`snap, run_ind`*`)` : logs a given *`snapshot`*[&#x21F1;](#step4) by appending it to the corresponding runlog indexed by *`run_ind`* and returns the string path of the log directory.\n",
    "- **`log_loss`**`(`*`loss, enum, run_ind`*`)` : logs a given *`loss`* and `enum` by appending them to the corresponding runlog indexed by *`run_ind`* and returns the string path of the log directory.\n",
    "\n",
    "Below we provide the run routine that executes one run of training, with occasional pre-training tests. The training will proceed for the given number of epochs or until the train loss goes below the value of `ECRIT`. Regardless of when the training ends, the model will be tested one final time and saved (if `SAVE_FINAL=True`). Model parameters and test data will be stored at a new log directory, producing a new file indexed by `run_ind` (e.g. `checkpoints_directory_0` and `runlog_0.pkl`).\n",
    "\n",
    "All of the run parameters are brought to the top of the cell for presentation. We specify `TEST_EPOCHS` outside the run loop as a list of integer values. If the user wants to checkpoint the model occasionally, `SAVE_EPOCHS` should be a similar list specifying when the model parameters are to be checkpointed. Although the code below does not checkpoint the model at any point, the corresponding run parameters (`SAVE_EPOCHS=[None]` and `SAVE_FINAL=False`) are provided for illustration. Running the following cell will print out the test losses, indexed by the corresponding epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf logdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up run parameters\n",
    "NUM_RUNS    = 1\n",
    "NUM_EPOCHS  = 3000\n",
    "TEST_EPOCHS = [i for i in range(0,NUM_EPOCHS,500)]\n",
    "SAVE_EPOCHS = [None]\n",
    "SAVE_FINAL  = False\n",
    "ECRIT       = 0.05\n",
    "\n",
    "# Create a model saverhttp://localhost:8888/notebooks/tutorials/building_models/main_tutorial.ipynb#\n",
    "saver = FFBP.ModelSaver(restore_from=None, logdir=None)\n",
    "\n",
    "# Start run loop\n",
    "for run_ind in range(NUM_RUNS):\n",
    "    print('>>> RUN {}'.format(run_ind))\n",
    "    \n",
    "    with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "\n",
    "        # restore or initialize FFBP_GRAPH variables:\n",
    "        start_epoch = saver.init_model(session=sess)\n",
    "\n",
    "        # create coordinator and start queue runners\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "        \n",
    "        # Start train loop\n",
    "        for i in FFBP.prog_bar(\n",
    "            sequence=range(start_epoch, start_epoch + NUM_EPOCHS), \n",
    "            name='Run {}/{}, Epoch'.format(run_ind,NUM_RUNS),\n",
    "            every=1):\n",
    "            \n",
    "            # Test model occasionally\n",
    "            if any([i==test_epoch for test_epoch in TEST_EPOCHS]):\n",
    "                test_loss, snapshot = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.log_test(snapshot, run_ind)\n",
    "\n",
    "            # Run one training epoch\n",
    "            train_loss, enum = MODEL.train_epoch(session=sess, verbose=False)\n",
    "            saver.log_loss(train_loss, enum, run_ind)\n",
    "\n",
    "            # Save model occasionally\n",
    "            if any([i==save_epoch for save_epoch in SAVE_EPOCHS]):\n",
    "                saver.save_model(session=sess, model=MODEL, run_ind=run_ind)\n",
    "\n",
    "            # Do final test, stop queues, and break out from training loop\n",
    "            if train_loss < ECRIT or i == start_epoch + (NUM_EPOCHS - 1): \n",
    "                print('Final test ({})'.format(\n",
    "                    'loss < ecrit' if train_loss < ECRIT else 'reached last epoch'))\n",
    "\n",
    "                test_loss, snapshot = MODEL.test_epoch(session=sess, verbose=True)\n",
    "                saver.log_test(snapshot, run_ind)\n",
    "\n",
    "                coordinator.request_stop()\n",
    "                coordinator.join(threads)\n",
    "\n",
    "                if SAVE_FINAL: saver.save_model(session=sess, model=MODEL, run_ind=run_ind)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'> </a>\n",
    "## Accessing and analyzing the results\n",
    "\n",
    "In the previous section, an instance of `FFBP.ModelSaver()` was used to store two different kinds of data: (1) loss values accumulated over a each training epoch and (2) network snapshots produced by testing the model occasionally. Both of these data structures were stored into a single runlog file, which can be accessed independently of running the model. FFBP provides a couple of functions for viewing these data (see related [tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/analyze_data.ipynb)). \n",
    "\n",
    "Here we will explain the structure of the runlog file and provide an example of how data from several such files can be accessed programatically for analysis and visualization. We will accomplish the latter by looking into one of the FFBP's visualization functions, `FFBP.view_progress()`.\n",
    "\n",
    "Let us first understand what runlogs are. Information from different runs is captured in separate runlog files. The saver creates a new runlog whenever it encounters a new run index in one of its saving methods. If the file for the current run exists, the saver will append new information to it. The manner in which information is added depends on which method was used. \n",
    "\n",
    "Since there are only two methods for logging data, a runlog is a Python dictionary (dict) object that can have up to two keys: `'loss_data'` and `'test_data'`. These string keys point to different structures that can be understood separately.\n",
    "\n",
    "### Loss data\n",
    "The data structure `runlog['loss_data']` is itself a dict that stores two lists of numeric values, indexed, respectively, by keys `'vals'` and `'enums'`. The former list contains the loss value(s) accumulated over one epoch of training, and the latter contains the corresponding epoch number(s). Note that the epoch number is logged *after* being incremented due to the preceding training epoch. It helps to think of the epoch number as the number of times the network has experienced (i.e. learned from) every example in the training set. These two lists are always the same length within a single run, but both can differ from similar lists across runs.\n",
    "\n",
    "### Test data\n",
    "The data structure `runlog['test_data']` is a bit more elaborate, but about just as simple. It is a list of individual network state \"snapshots\" organized by the order of appending. Each snapshot is a dict that contains (1) model-level data and (2) layer-level data for each layer in the network. The model-level data keys point to numpy arrays of various shapes. The and descriptions of these arrays are provided in **Table 1** below; `DATA_LEN` and `M_INP_SIZE` correspond to the number of patterns in the testing set and the sum of sizes of all input layers (placeholders) of the model:\n",
    "\n",
    "<br><center><b>Table 1. Model-level data</b></center>\n",
    "\n",
    "| Key      |     Type      | Shape                  | Description                                         |\n",
    "|----------|:-------------:|------------------------|-----------------------------------------------------|\n",
    "| 'enum'   |  numpy.int32  | ()                     | epoch number of test                                |\n",
    "| 'loss'   | numpy.ndarray | (DATA_LEN,)            | a numpy array of per pattern loss values            |\n",
    "| 'labels' | numpy.ndarray | (DATA_LEN,)            | a numpy array of pattern labels (encoded in binary) |\n",
    "| 'input'  | numpy.ndarray | (DATA_LEN, M_INP_SIZE) | a numpy array of test input patterns                |\n",
    "| 'target' | numpy.ndarray | (DATA_LEN, M_INP_SIZE) | a numpy array of target patterns                    |\n",
    "<br>\n",
    "\n",
    "Other keys found in a snapshot point to a deeper structure and index layer-level data. For example, test information collected for the hidden layer in the semantic netowork example could be obtained by `snapshot['HIDDEN_LAYER']`. The object returned is a dict of layer-level data collected from the hidden layer of the network. This sub-dict's keys are also strings, and their values are also numpy arrays of various shapes. See the summary in **Table 2**. `L_INP_SIZE` is the number of sending units feeding into the layer, and `LAYER_SIZE` is the number of receiving units in the layer:\n",
    "\n",
    "\n",
    "<br><center><b>Table 2. Layer-level data</b></center>\n",
    "\n",
    "| Key          | Shape                              | Description                                                                                                                                                                      |\n",
    "|--------------|------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 'input_'     | (DATA_LEN, LAYER_SIZE)             | Array of inputs to the layer. `axis=0` (first dimension) is used to index test items, `axis=1` indexes layer's units.                                                            |\n",
    "| 'weights'    | (LAYER_SIZE, L_INP_SIZE)           | Layer weight matrix. `axis=0` indexes receiving units, `axis=1` indexes sending units.                              |\n",
    "| 'biases'     | (LAYER_SIZE, )                     | Layer biases. `axis=0` indexes receiving units.                                                                                                                                  |\n",
    "| 'net_input'  | (DATA_LEN, LAYER_SIZE)             | Array of net input values across the layer's units. `axis=0` indexes test items, `axis=1` indexes layer units.                                                                   |\n",
    "| 'output'     | (DATA_LEN, LAYER_SIZE)             | Array of activation (output) values across the layer's units. `axis=0` indexes test items, `axis=1` indexes layer units.                                                         |\n",
    "| 'gweights'   | (DATA_LEN, L_INP_SIZE, LAYER_SIZE) | Array of the gradient matrices with respect to weights indexed by test items. `axis=0` indexes test item, `axis=1` indexes receiving units, and  `axis=2` indexes sending units. |\n",
    "| 'gbiases'    | (DATA_LEN, LAYER_SIZE)             | Array of gradients with respect to biases indexed by test items. `axis=0` indexes test item, `axis=1` indexes layer's units                                                      |\n",
    "| 'gnet_input' | (DATA_LEN, LAYER_SIZE)             | Array of gradients with respect to net input values indexed by test items. `axis=0` indexes test item, `axis=1` indexes layer's units                                            |\n",
    "| 'goutput'    | (DATA_LEN, LAYER_SIZE)             | Array of gradients with respect to activation values indexed by test items. `axis=0` indexes test item, `axis=1` indexes layer's units                                           |\n",
    "| 'sgweights'  | (LAYER_SIZE, L_INP_SIZE)           | The (reduced) sum of 'gweights' along `axis=0`.                                                                                                                                  |\n",
    "| 'sgbiases'   | (LAYER_SIZE, )                     | The (reduced) sum of 'gbiases' along `axis=0`.                                                                                                                                   |\n",
    "<br>\n",
    "\n",
    "Having the runlog structure under our belt we can do something useful with it. First, let us do a very basic visualization of our example network's learning over time. For this we will need to load the stored runlog file, access the appropriate values and use a plotting function to view the graph. To load the runlog, we will use the [`pickle`](https://docs.python.org/3/library/pickle.html) library and to plot the data we will require [`pyplot`](https://matplotlib.org/users/pyplot_tutorial.html), so let's import these tools first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a couple of functions for loading and plotting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_runlog(runlog_path):\n",
    "    ''' returns an unpickled runlog stored at runlog path '''\n",
    "    \n",
    "    with open(runlog_path, 'rb') as snap_file:\n",
    "        test_data = pickle.load(snap_file)\n",
    "    return test_data\n",
    "\n",
    "def plot_loss(runlog):\n",
    "    ''' plots the learning curve for data stored in runlog'''\n",
    "    \n",
    "    # get training loss and epochs data\n",
    "    data, enums = runlog['loss_data']['vals'], runlog['loss_data']['enums']\n",
    "    \n",
    "    # plot training data\n",
    "    plt.plot(enums, data)\n",
    "    plt.title('Plot of loss over time')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions we can view the training loss over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = saver.logdir + '/' + 'runlog_0.pkl'\n",
    "runlog = load_runlog(path)\n",
    "\n",
    "plot_loss(runlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now inspect the `'test_data'` part of the runlog. Below, for each `items` pair of the second (`[1]`) snapshot of the `test_data` log, we print out the key and its value's shape. However, if the value is a `dict` (i.e. if there's a deeper layer-level structure), we start a similar inner loop to print out the contents of this `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = runlog['test_data']\n",
    "\n",
    "# test_data is a list of individual snapshots\n",
    "print('test_data is and instance of {} of length {}\\n'.format(type(test_data), len(test_data)))\n",
    "\n",
    "# We can itirate over an arbitrary snapshot, summarizing each of its values:\n",
    "\n",
    "# loop through key, value pairs of the second entry in test_data\n",
    "for k, v in test_data[1].items():\n",
    "    \n",
    "    # if the value is a dict instance, start an inner loop that goes through that dict's items\n",
    "    if isinstance(v, dict):\n",
    "        \n",
    "        # print the name of the layer\n",
    "        print(k+': {')\n",
    "        \n",
    "        # for each key and value, print the key and the shape of the value\n",
    "        for sub_k, sub_v in v.items():\n",
    "            text = '\\t {:10} : {}'.format(sub_k, sub_v.shape)\n",
    "            print(text)\n",
    "        print('}')\n",
    "            \n",
    "    # if test_data value is not a dict, just print its key and shape\n",
    "    else:\n",
    "        print('{:10} : {}'.format(k, v.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
