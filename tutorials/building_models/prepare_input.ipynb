{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data\n",
    "Since the [FFBP package](https://github.com/alex-ten/pdpyflow/tree/master/FFBP) runs [Tensorflow](https://www.tensorflow.org/), input data must be made readable for the underlying computational graph. FFBP has a special class called `InputData` that lets you easily convert a csv data file into a structure that can be injected into a Tensorflow graph.\n",
    "\n",
    "The csv file must be structured in a certain way (see example below). Each row must contain a labeled input pattern, is organized as a sequence of entries, whereby the first entry contains the pattern label, and the following entries encode the actual input/target data. A sequence of input data comes first, followed by a sequence of target data. The example below is from the [text file](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/materials/semantic_data.txt) for our example of the semantic network (spaces are added for readability):\n",
    "\n",
    "```\n",
    "Pine_isa,   1,0,0,0,0,0,0,0,1,0,0,0,  1,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "Pine_is,    1,0,0,0,0,0,0,0,0,1,0,0,  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "Pine_can,   1,0,0,0,0,0,0,0,0,0,1,0,  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "...\n",
    "Salmon_is,  0,0,0,0,0,0,0,1,0,1,0,0,  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "Salmon_can, 0,0,0,0,0,0,0,1,0,0,1,0,  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "Salmon_has, 0,0,0,0,0,0,0,1,0,0,0,1,  0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1\n",
    "```\n",
    "**Importantly, each row must end with an implicit newline character \"`\\n`\", not a comma \"`,`\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `InputData` class\n",
    "It is straightforward to create an `InputData` instance from a correctly-formatted data file. You need to specify a few parameters to make it work:\n",
    "- **`num_epochs`** : the number of epochs you are planning to train/test the model for. An epoch is a single iteration inside which a network processes the entire data set (typically seeing each pattern exactly once).\n",
    "- **`batch_size`** : the size of a subset (mini-batch) of the data set. Together with `data_len`, this parameter determines the number of weight updates per epoch of training. Specifically, within a single epoch of training, weights will be updated `data_len / batch_size` times. For example, if `batch_size == data_len`, the network will accumulate gradients for each training pattern and perform a weight update once per epoch. In contrast, if `batch_size == 1`, each training pattern will cause a weight update, making the order of training examples consequential. Moreover, if `(batch_size > 1 & batch_size < data_len)`, several training patterns will cause a weight update and there will be more than one update per epoch. Thus, `batch_size` must divide `data_len`, that is, the remainder of `data_len / batch_size` must be zero.\n",
    "- **`data_len`** : the number of input patterns in the data set. Together with `batch_size`, this parameter determines the number of weight updates per epoch of training (see `batch_size`).\n",
    "- **`inp_size`** : the number of input data points (same as the size of input layer). You can pass a list or a tuple of integers if you have multiple input placeholders. Note that the order of input sizes must correspond to the order of input placeholders when an `FFBP.Model` class is instantiated with multiple input placeholders (see tutorials on [connecting layers](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/build_model.ipynb) and [training a model](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/train_model.ipynb)). For example `inp_size = (2,6)` will devide the row of 8 input items into two sub-rows of sizes 2 and 6.\n",
    "- **`targ_size`** : the number of target data points (same as the size of output layer).\n",
    "-  **`shuffle_seed`** : (*default*=`None`) the seed for a random number generator that dictates the pseudorandom shuffling of input patterns. If `None`, input patterns will be fed in the same order (top to bottom, unshuffled) as they appear in the csv file. If negative (e.g. `shuffle_seed = -1`), the shuffling of examples will be irreproducible.\n",
    "\n",
    "## Example: [Enqueueing](https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops/queues) data for training and testing\n",
    "In the example below, we set up training and testing data for a training simulation. **Note** that for testing, `batch_size == 1` since we want to compute gradient information individually for each training example. Also note that we don't want to shuffle testing patterns (so ommit the `shuffle_seed` parameter or set it to `None`).\n",
    "\n",
    "Feel free to change the parameters and observe the effects in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import FFBP\n",
    "\n",
    "FFBP_GRAPH = tf.Graph()\n",
    "\n",
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    # Create data for training\n",
    "    TRAIN_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/semantic_data.txt',\n",
    "        inp_size = (8,4),\n",
    "        targ_size = 36,\n",
    "        batch_size = 16,\n",
    "        data_len = 32,\n",
    "        shuffle_seed = -1,\n",
    "    )\n",
    "\n",
    "    # Create data for testing\n",
    "    TEST_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/semantic_data.txt',\n",
    "        inp_size = (8,4),\n",
    "        targ_size = 36,\n",
    "        batch_size = 1,\n",
    "        data_len = 32,\n",
    "    )\n",
    "\n",
    "# Simulate training/testing loop\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        \n",
    "        # create coordinator and start queue runners\n",
    "        coordinator = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "\n",
    "        for i in range(NUM_EPOCHS):\n",
    "            print('\\nEPOCH {}:\\n'.format(i))\n",
    "            \n",
    "            # Mock-test model in a test mini-loop\n",
    "            print('  TESTING:')\n",
    "            for j in range(TEST_DATA.data_len):\n",
    "                example = sess.run(TEST_DATA.examples_batch)\n",
    "                print('    testing pattern {}: \\'{}\\' {}'.format(\n",
    "                    j, example[0][0].decode('UTF-8'),\n",
    "                    ', '.join([str(x) for x in example[1:-1]])))\n",
    "\n",
    "            # Mock-train model in a train mini-loop\n",
    "            print('  TRAINING:')\n",
    "            num_updates = TRAIN_DATA.data_len // TRAIN_DATA.batch_size\n",
    "            for k in range(num_updates):\n",
    "                examples_batch = sess.run(TRAIN_DATA.examples_batch)\n",
    "                print(\n",
    "                    '    processing mini-batch {}/{}: {}'.format(\n",
    "                        k+1, num_updates, \n",
    "                        [x.decode('UTF-8') for x in examples_batch[0]]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                [print('\\t' + str(x).replace('\\n', '\\n\\t')) for x in examples_batch[1:-1]]\n",
    "\n",
    "        coordinator.request_stop()\n",
    "        coordinator.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "An `FFBP.InputData` object is used by `FFBP.Model` for training or testing. Detailed instructions on how to build a model are provided in [this tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/construct_model.ipynb) and an a full demonstration of how to run a model using `FFBP.InputData`, `FFBP.Model` and more are provided in the [main_tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/main_tutorial.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
