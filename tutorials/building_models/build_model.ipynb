{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Feedforward Neural Network Model\n",
    "This notebook provides instructions on how to build a feedforward neural network model. It will introduce a couple of classes defined in the [FFBP package](https://github.com/alex-ten/pdpyflow/tree/master/FFBP): `FFBP.Layer` and `FFBP.Model` as well as describe some of the Tensorflow's own objects needed to successfully implement a feedforward neural network model with `FFBP`.\n",
    "\n",
    "To demonstrate various ways in which FFBP layers can be connected together, we will be constructing an arbitrary architecture featuring 3 parallel input layers (`inp_1`,`inp_2`, and `inp_3`) feeding into 2 hidden layers (`hid_1` and `hid_2`), which together send activation to a single `deep` layer, which is finally transformed by the final observable layer (`out`).\n",
    "\n",
    "<img src=\"arb_net.png\">\n",
    "\n",
    "Based on these interconnected layers we will construct a working neural network and propagate test inputs through it (to see how to prepare test input for a model, consult the `prepare_input` [notebook tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/FFBP_network/prepare_input.ipynb)).\n",
    "\n",
    "We begin by importing the packages required for this tutorial and creating a [Tensorflow Graph](https://www.tensorflow.org/programmers_guide/graphs). We want to make sure that we are adding network elements in the context of this graph (see cells below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import FFBP\n",
    "\n",
    "FFBP_GRAPH = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "\n",
    "Input layers are implemented as [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) objects. `InputData` (covered in a [different tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_and_training_FFBP_network/prepare_input.ipynb)) data from a text document into the `tf.float32` format. Therefore, the first argument inside the `tf.placeholder()` call is `dtype = tf.float32` (but it can be anything as long as it is consistent with the data fed in a session). The `shape` parameter is optional, but we strictly indicate *at least* the second dimension because other objects in the software expect an integer value there. The `None` in the place of the first dimension of the `shape` parameter means that any size of the that dimension is allowed. For instance, `inp_1` is a placeholder for a 2-dimensional tensor (of type `tf.float32`) with 3 columns and any number of rows. This is useful when we train a model with batched up input but test it with single examples.\n",
    "\n",
    "The target placeholder needs to be the same size as the final (output) layer of the network. It is a good practice to name tensorflow nodes informatively, so we add an optional `name` argument accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    MODEL_NAME = 'arbitrary_model'\n",
    "    with tf.name_scope(MODEL_NAME):\n",
    "        \n",
    "        # Create input placeholders\n",
    "        INP_1  = tf.placeholder(dtype = tf.float32, shape=[None, 3], name='inp_1')\n",
    "        INP_2  = tf.placeholder(dtype = tf.float32, shape=[None, 2], name='inp_2')\n",
    "        INP_3  = tf.placeholder(dtype = tf.float32, shape=[None, 3], name='inp_3')\n",
    "        \n",
    "        # Create a target placeholder\n",
    "        TARG = tf.placeholder(dtype = tf.float32, shape=[None, 8], name='targ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden and output layers\n",
    "Hidden layers are created via the `FFBP.BasicLayer` interface. Each `BasicLayer` takes the following arguments:\n",
    "- **`layer_name`** : a (string) name of the layer (this will be displayed in visualization)\n",
    "- **`layer_input`** : input to the layer. If input is a placeholder, just pass  the corresponding handle to this parameter. If input comes from another `BasicLayer`, we need to access its `output` attribute (e.g. `hidden_layer.output`). Finally, if there are multiple layers / placeholders feeding into a single `BasicLayer` simply wrap the inputs into a list or a tuple (e.g. `(inp1, inp2, ..., inpN)`)\n",
    "- **`size`** : the number of units in the layer\n",
    "- **`wrange`** : a (listed or tupled) pair of values corresponding to, respectively, the lower and upper bounds of the distribution from which weight values will be sampled uniformly upon initialization.\n",
    "- **`nonlin`** : (optional, *`default`*`=None`) a function that takes in and outputs a tensor. If omitted, input to the layer will be transformed linearly (i.e. $Wx + b$).\n",
    "- **`bias`** : (optional, *`default`*`=False`) controls whether to bias layer output. If ommited, the bias terms will be constant at 0.\n",
    "- **`seed`** : (optional, *`default`*`=None`) the seed for random weight initialization. If omited, the initialization will be irreproducible.\n",
    "\n",
    "These `FFBP.BasicLayer`s are modular and can be configured with different sizes, nonlinearities, weight ranges etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    with tf.name_scope(MODEL_NAME):\n",
    "\n",
    "        HID_1 = FFBP.BasicLayer(\n",
    "            layer_name = 'hid_1', \n",
    "            layer_input = (INP_1, INP_2), \n",
    "            size = 3, \n",
    "            wrange = [0, 3], \n",
    "            nonlin = tf.nn.sigmoid, \n",
    "            bias = True\n",
    "        )\n",
    "        \n",
    "        HID_2 = FFBP.BasicLayer(\n",
    "            layer_name = 'hid_2', \n",
    "            layer_input = INP_3, \n",
    "            size = 2, \n",
    "            wrange = [-.5, .5], \n",
    "            nonlin = None, \n",
    "            bias = False\n",
    "        )\n",
    "        \n",
    "        DEEP = FFBP.BasicLayer(\n",
    "            layer_name = 'deep', \n",
    "            layer_input = (HID_1.output, HID_2.output, INP_2), \n",
    "            size = 5, \n",
    "            wrange = [-.01, .01], \n",
    "            nonlin = tf.nn.sigmoid, \n",
    "            bias = True\n",
    "        )\n",
    "        \n",
    "        OUT = FFBP.BasicLayer(\n",
    "            layer_name = 'out', \n",
    "            layer_input = DEEP.output, \n",
    "            size = 8, \n",
    "            wrange = [-1, 1], \n",
    "            nonlin = tf.nn.softmax, \n",
    "            bias = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFBP Model\n",
    "We can now create a feedforward model by instantiating an `FFBP.Model` class. In order to test the model later, we will need to create at least one `FFBP.InputData` object and either pass it to class initializer (`FFBP.Model.__init__()`) directly or use a separate dedicated method. An `FFBP.Model` is initialized with the following parameters:\n",
    "- **`name`** : the name of the model (used for storage and visualization).\n",
    "- **`loss`** : the loss (objective) function of the model.\n",
    "- **`optimizer`** : model optimizer.\n",
    "- **`layers`** : a list of model layers. The order is consequential for visualization. The last layer should be the output layer so that the viewer displays target output information for this layer, but not the others.\n",
    "- **`inp`** : input placeholder or a list (or tuple) of input placeholders. The order of placeholders must correspond to the order of `inp_size` values set for `FFBP.InputData` (see the [input data tutorial](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_and_training_FFBP_network/preparing_input.ipynb)).\n",
    "- **`train_data`** : (optional, *`default`*`=None`) `FFBP.InputData` instance set up for training.\n",
    "- **`test_data`** : (optional, *`default`*`=None`) `FFBP.InputData` instance set up for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    MODEL = FFBP.Model(\n",
    "        name = MODEL_NAME,\n",
    "        layers = [HID_1, HID_2, DEEP, OUT], \n",
    "        inp    = [INP_1, INP_2, INP_3],\n",
    "        targ   = TARG,\n",
    "        loss   = tf.squared_difference(TARG, OUT.output, name='loss_function')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FFBP.Model` class also defines a few useful methods for training and testing:\n",
    "- **`test_setup`** : sets up the model for testing separately from class initialization\n",
    "- **`test_epoch`** : runs a single epoch of testing by feeding each item from the test set into the model and evaluating its state. This method returns two values: the loss accumulated across test items, and the model snapshot which contains information about the model state at the time of test. The snapshots can be logged for later analyses (see the [tutorial on runlog structure](https://github.com/alex-ten/pdpyflow/blob/master/tutorials/building_models/runlog_structure.ipynb) to see how snapshot logs are organized).\n",
    "- **`train_setup`** : sets up the model for training separately from class initialization\n",
    "- **`train_epoch`** : runs a single epoch of training, optimizing parameters after processing each mini-batch of training examples. Returns loss accumulated over input mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Testing simple forward propagation\n",
    "Now let us test the network. By instantiating `FFBP.BasicLayer`s we implicitly added computational nodes to te underlying graph. In order to run this graph, we need to create a [session](https://www.tensorflow.org/api_docs/python/tf/Session).\n",
    "\n",
    "In the example below we will forward-propagate the input data from `'auto_data_test.txt'` and evalute the aggregate error (total sum of squares) on the output layer by comparing it to target activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with FFBP_GRAPH.as_default():\n",
    "    \n",
    "    # Define input data\n",
    "    TEST_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/auto_data_test.txt',\n",
    "        num_epochs = 2, # two epochs for pre- and post-test\n",
    "        batch_size = 1,\n",
    "        inp_size = [3,2,3], # sizes of INP_1, INP_2, INP_3 in order\n",
    "        targ_size = 8,\n",
    "        data_len = 15\n",
    "    )\n",
    "    \n",
    "    TRAIN_DATA = FFBP.InputData(\n",
    "        path_to_data_file = 'materials/auto_data_test.txt',\n",
    "        num_epochs = 1,\n",
    "        batch_size = 1,\n",
    "        inp_size = [3,2,3], # sizes of INP_1, INP_2, INP_3 in order\n",
    "        targ_size = 8,\n",
    "        data_len = 15\n",
    "    )\n",
    "    \n",
    "    # Setup testing and training\n",
    "    MODEL.test_setup(TEST_DATA)\n",
    "    MODEL.train_setup(TRAIN_DATA, optimizer=tf.train.GradientDescentOptimizer(learning_rate=.2))\n",
    "\n",
    "\n",
    "# Run a session and compute the activation on of the output layer given inputs\n",
    "with tf.Session(graph=FFBP_GRAPH) as sess:\n",
    "    \n",
    "    # Initialize FFBP_GRAPH variables:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    # create coordinator and start queue runners\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    \n",
    "    # Pre-test the model\n",
    "    pre_loss, pre_state = MODEL.test_epoch(session=sess, verbose=True)\n",
    "    \n",
    "    # Run one epoch of training\n",
    "    train_loss = MODEL.train_epoch(session=sess)\n",
    "\n",
    "    # Post-test the model\n",
    "    post_loss, post_state = MODEL.test_epoch(session=sess, verbose=True)\n",
    "    \n",
    "    # Stop queue runners\n",
    "    coordinator.request_stop()\n",
    "    coordinator.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
