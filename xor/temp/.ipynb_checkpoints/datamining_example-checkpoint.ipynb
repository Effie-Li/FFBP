{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "def extract_tensor_from_summary(summary_value, dt=None):\n",
    "    \"\"\"\n",
    "    De-serializes data from tensorflow event\n",
    "    :param summary_value: event.summary.value object created by summary writer\n",
    "    :return deci_tensor:  decimal numpy array\n",
    "    \"\"\"\n",
    "    _dt = dt if dt else tf.as_dtype(summary_value.tensor.dtype).as_numpy_dtype()\n",
    "    shape = tuple([i.size for i in summary_value.tensor.tensor_shape.dim])\n",
    "    print(shape)\n",
    "    binary_tensor = summary_value.tensor.tensor_content\n",
    "#     print(np.fromstring(binary_tensor, dtype=dt))\n",
    "    deci_tensor = np.fromstring(binary_tensor, dtype=dt).reshape(shape)\n",
    "    return deci_tensor\n",
    "\n",
    "\n",
    "def retrieve_model_params(path_to_event_file, layer_name, param_name):\n",
    "    \"\"\"\n",
    "    Retrieves parameter values from a tensorflow event file and structures it into an ordered dict\n",
    "    :param path_to_event_file: absolute path to event file\n",
    "    :param layer_name: string name of the layer\n",
    "    :param params_summary: stirng name of the parameter to retrieve (e.g. 'weights')\n",
    "    :param data_type (optional): data type of output numpy arrays (if ommited, original data type converted from \n",
    "        tensorflow DType to numpy dtype will be used\n",
    "    :return: params_dict: an ordered dict where keys are global steps (of type int) and values are numpy arrays\n",
    "    \"\"\"\n",
    "    params_dict = OrderedDict()\n",
    "    lookup = (layer_name, param_name)\n",
    "    for event in tf.train.summary_iterator(path_to_event_file):\n",
    "        for val in event.summary.value:\n",
    "            if all([item in val.tag for item in lookup]):\n",
    "                params_dict[event.step] = tf.contrib.util.make_ndarray(val.tensor)\n",
    "    return params_dict\n",
    "\n",
    "\n",
    "def retrieve_model_data(path_to_event_file, layer_name, tensor_name):\n",
    "    data_dict = OrderedDict()\n",
    "    SummaryData = namedtuple('SummaryData', ['labels','inputs','targets','data'])\n",
    "    lookup = '/'.join([layer_name,tensor_name])\n",
    "    for i, event in enumerate(tf.train.summary_iterator(event_file)):\n",
    "        data_dict.setdefault(event.step, SummaryData([],[],[],[]))\n",
    "        for val in event.summary.value:\n",
    "            if 'pattern_labels' in val.tag:\n",
    "                data_dict[event.step].labels.append(val.tensor.string_val[0].decode('utf-8'))\n",
    "            if 'input_patterns' in val.tag:\n",
    "                data_dict[event.step].inputs.append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if 'target_patterns' in val.tag:\n",
    "                data_dict[event.step].targets.append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if lookup in val.tag:\n",
    "                data_dict[event.step].data.append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def retrieve_loss(path_to_event_file):\n",
    "    eacc = get_event_accumulator(path_to_event_file)\n",
    "    loss_log = np.stack([np.asarray([scalar.step, scalar.value]) for scalar in eacc.Scalars('train/error_summary')])\n",
    "    return loss_log#    \n",
    "\n",
    "\n",
    "def display_model_data(data_dict):\n",
    "    for step, data in data_dict.items():\n",
    "        print('> Epoch',step)\n",
    "        print('  labels:')\n",
    "        for x in data.labels: print(x)\n",
    "        print('  inputs:')\n",
    "        for x in data.inputs: print(x)\n",
    "        print('  targets:')\n",
    "        for x in data.targets: print(x)\n",
    "        print('  data:')\n",
    "        for x in data.data: print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PATH VARS\n",
    "logdir = '/Users/alexten/Projects/pdpyflow/xor/train/log_000'\n",
    "event_file = os.path.join(logdir, os.listdir(logdir)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of all (unique) summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_list = []\n",
    "for event in tf.train.summary_iterator(event_file):\n",
    "    for val in event.summary.value:\n",
    "        tag_list.append(val.tag)\n",
    "for tag in sorted(set(tag_list)):\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get summary by tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = ['output_layer','gradient-weights']\n",
    "for event in tf.train.summary_iterator(event_file):\n",
    "    for val in event.summary.value:\n",
    "        if all([item in val.tag for item in lookup]):\n",
    "            print(event.step)\n",
    "            print(tf.contrib.util.make_ndarray(val.tensor))\n",
    "#             print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch 0\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.27658999 -0.4025    ]]\n",
      "[[ 0.17219102 -0.36601099]]\n",
      "[[ 0.155581   -0.44091299]]\n",
      "[[ 0.60436201 -0.40442401]]\n",
      "> Epoch 30\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.42694917 -0.54511285]]\n",
      "[[-0.04823729 -0.58433998]]\n",
      "[[-0.0642744  -0.65373987]]\n",
      "[[ 0.31443748 -0.692967  ]]\n",
      "> Epoch 60\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.41147745 -0.57077384]]\n",
      "[[-0.03443527 -0.63227183]]\n",
      "[[-0.05079231 -0.69490427]]\n",
      "[[ 0.32624984 -0.75640225]]\n",
      "> Epoch 90\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.38737994 -0.5736838 ]]\n",
      "[[-0.00198942 -0.64938712]]\n",
      "[[-0.01853979 -0.70533293]]\n",
      "[[ 0.36685073 -0.78103626]]\n",
      "> Epoch 120\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.37607521 -0.5756157 ]]\n",
      "[[ 0.01438618 -0.67142642]]\n",
      "[[-0.00222459 -0.71929711]]\n",
      "[[ 0.38823682 -0.81510782]]\n",
      "> Epoch 150\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.36954972 -0.57922864]]\n",
      "[[ 0.02415746 -0.70859337]]\n",
      "[[ 0.00751802 -0.74626404]]\n",
      "[[ 0.40122524 -0.87562883]]\n",
      "> Epoch 180\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.36292279 -0.58774513]]\n",
      "[[ 0.0341813  -0.78016913]]\n",
      "[[ 0.01751658 -0.80557972]]\n",
      "[[ 0.41462064 -0.99800378]]\n",
      "> Epoch 210\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.35514849 -0.61385977]]\n",
      "[[ 0.04585817 -0.95874327]]\n",
      "[[ 0.02917442 -0.97066957]]\n",
      "[[ 0.43018109 -1.31555307]]\n",
      "> Epoch 240\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.35352537 -0.69701254]]\n",
      "[[ 0.04896402 -1.69469559]]\n",
      "[[ 0.03227228 -1.69526291]]\n",
      "[[ 0.43476167 -2.69294596]]\n",
      "> Epoch 270\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-0.48642749  0.53666413]]\n",
      "[[ 0.15043032 -3.52017307]]\n",
      "[[ 0.13401562 -3.51922226]]\n",
      "[[ 0.77087337 -7.57605934]]\n",
      "> Epoch 300\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-4.79940462  1.67992556]]\n",
      "[[-1.72063112 -4.43304348]]\n",
      "[[-1.71714759 -4.42969465]]\n",
      "[[  1.36162615 -10.54266357]]\n",
      "> Epoch 317\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[-6.23137379  2.38093591]]\n",
      "[[-2.2918601  -3.98459888]]\n",
      "[[-2.29403281 -3.98194003]]\n",
      "[[  1.64548111 -10.34747505]]\n"
     ]
    }
   ],
   "source": [
    "hidden_weights = retrieve_model_params(event_file, 'hidden_layer', 'weights')\n",
    "hidden_bias = retrieve_model_params(event_file, 'hidden_layer', 'bias')\n",
    "hidden_acts = retrieve_model_data(event_file, 'hidden_layer', 'activation')\n",
    "# print(hidden_acts[0].labels)\n",
    "display_model_data(hidden_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
