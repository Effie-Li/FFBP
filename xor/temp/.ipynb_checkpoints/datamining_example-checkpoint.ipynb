{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from collections import OrderedDict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_model_params(path_to_event_file, layer_name, param_name):\n",
    "    \"\"\"\n",
    "    Retrieves parameter values from a tensorflow event file and structures it into an ordered dict\n",
    "    :param path_to_event_file: absolute path to event file\n",
    "    :param layer_name: string name of the layer\n",
    "    :param params_summary: stirng name of the parameter to retrieve (e.g. 'weights')\n",
    "    :return params_dict: an ordered dict where keys are global steps (of type int) and values are numpy arrays\n",
    "    \"\"\"\n",
    "    params_dict = OrderedDict()\n",
    "    lookup = '/'.join([layer_name, param_name])\n",
    "    for event in tf.train.summary_iterator(path_to_event_file):\n",
    "        for val in event.summary.value:\n",
    "            if lookup in val.tag:\n",
    "                params_dict[event.step] = tf.contrib.util.make_ndarray(val.tensor)\n",
    "    return params_dict\n",
    "\n",
    "\n",
    "def get_layer_snapshot(path_to_event_file, layer_name, epoch, pattern_label, target=False):\n",
    "    '''\n",
    "    \n",
    "    :param path_to_event_file: \n",
    "    :param epoch: \n",
    "    :param pattern_label: \n",
    "    :param target: \n",
    "    :return: snapshot = dict(input_pattern=[], weights=[], biases=[], net_input=[], activation=[],\n",
    "                    gweights=[], gbiases=[], gnet_input=[], gactivation=[], loss=[])\n",
    "    '''\n",
    "    snapshot = dict(input_pattern=[], weights=[], biases=[], net_input=[], activation=[],\n",
    "                    gweights=[], gbiases=[], gnet_input=[], gactivation=[], loss=[])\n",
    "    if target: fields.append('target')\n",
    "    \n",
    "    events = []\n",
    "    for event in tf.train.summary_iterator(path_to_event_file):\n",
    "        if event.step == epoch:\n",
    "            events.append(event)\n",
    "    \n",
    "    labels = []\n",
    "    for event in events:\n",
    "        for val in event.summary.value:\n",
    "            if 'pattern_labels' in val.tag: labels.append(val.tensor.string_val[0].decode('utf-8'))\n",
    "            if 'input_patterns' in val.tag: snapshot['input_pattern'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if 'target_patterns' in val.tag and target: snapshot['target_pattern'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if 'test_loss_summary' in val.tag: snapshot['loss'].append(val.simple_value)\n",
    "                \n",
    "            if '/'.join([layer_name,'weights']) in val.tag: snapshot['weights'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if '/'.join([layer_name, 'biases']) in val.tag: snapshot['biases'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "    \n",
    "            if '/'.join([layer_name, 'net_input']) in val.tag: snapshot['net_input'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if '/'.join([layer_name, 'activation']) in val.tag: snapshot['activation'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if '/'.join([layer_name, 'gradient_weights']) in val.tag: snapshot['gweights'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if '/'.join([layer_name, 'gradient_biases']) in val.tag: snapshot['gbiases'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if '/'.join([layer_name, 'gradient_net_input']) in val.tag: snapshot['gnet_input'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            if '/'.join([layer_name, 'gradient_activation']) in val.tag: snapshot['gactivation'].append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "    \n",
    "    ind = labels.index(pattern_label)\n",
    "    for key, snap_val in snapshot.items():\n",
    "        snapshot[key] = snap_val[ind]\n",
    "        \n",
    "    return snapshot\n",
    "\n",
    "\n",
    "def get_epochs_and_labels(path_to_event_file):\n",
    "    epochs = []\n",
    "    labels = []\n",
    "    got_labels = False\n",
    "    for event in tf.train.summary_iterator(path_to_event_file):\n",
    "        for val in event.summary.value:\n",
    "            if 'test_loss_summary' in val.tag: epochs.append(event.step)\n",
    "        if not got_labels:\n",
    "            for val in event.summary.value:\n",
    "                if 'pattern_labels' in val.tag:\n",
    "                    labels.append(val.tensor.string_val[0].decode('utf-8'))\n",
    "            got_labels = True\n",
    "    return list(set(epochs)), labels\n",
    "\n",
    "\n",
    "def retrieve_model_data(path_to_event_file, layer_name, tensor_name):\n",
    "    '''\n",
    "    Retrieves model data from a tensorflow event file and structures it into lists within a named tuple withina an \n",
    "    ordered dict\n",
    "    :param path_to_event_file: absolute path to event file\n",
    "    :param layer_name: string name of the layer\n",
    "    :param tensor_name: string name of the data to retrieve (e.g. 'net_input')\n",
    "    :return data_dict: \n",
    "    example: OrderedDict = {epoch_num: namedtuple(labels: [t1,t2], inputs: [t1,t2], targets: [t1,t2], data: [t1,t2])}\n",
    "    '''\n",
    "    data_dict = OrderedDict()\n",
    "    SummaryData = namedtuple('SummaryData', ['labels', 'inputs', 'targets', 'data'])\n",
    "    lookup = '/'.join([layer_name, tensor_name])\n",
    "    for event in tf.train.summary_iterator(path_to_event_file):\n",
    "        data_dict.setdefault(event.step, SummaryData([], [], [], []))\n",
    "        for val in event.summary.value:\n",
    "            if 'pattern_labels' in val.tag:\n",
    "                data_dict[event.step].labels.append(val.tensor.string_val[0].decode('utf-8'))\n",
    "            elif 'input_patterns' in val.tag:\n",
    "                data_dict[event.step].inputs.append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            elif 'target_patterns' in val.tag:\n",
    "                data_dict[event.step].targets.append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "            elif lookup in val.tag:\n",
    "                data_dict[event.step].data.append(tf.contrib.util.make_ndarray(val.tensor))\n",
    "        if not sum([len(field) for field in data_dict[event.step]]): data_dict.pop(event.step)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def retrieve_loss(path_to_event_file):\n",
    "    lookup = 'train_loss_summary'\n",
    "    epochs, loss_vals = [], []\n",
    "    \n",
    "    for event in tf.train.summary_iterator(path_to_event_file):\n",
    "        for val in event.summary.value:\n",
    "            if lookup in val.tag:\n",
    "                epochs.append(event.step)\n",
    "                loss_vals.append(val.simple_value)\n",
    "                \n",
    "    return np.column_stack([epochs, loss_vals])\n",
    "\n",
    "\n",
    "def display_model_data(data_dict):\n",
    "    for step, data in data_dict.items():\n",
    "        print('> Epoch',step)\n",
    "        print('  labels:')\n",
    "        for x in data.labels: print(x)\n",
    "        print('  inputs:')\n",
    "        for x in data.inputs: print(x)\n",
    "        print('  targets:')\n",
    "        for x in data.targets: print(x)\n",
    "        print('  data:')\n",
    "        for x in data.data: print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PATH VARS\n",
    "logdir = '/Users/alexten/Projects/pdpyflow/xor/train/log_000'\n",
    "event_file = os.path.join(logdir, os.listdir(logdir)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of all (unique) summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR_model/hidden_layer/activations/data_summary\n",
      "XOR_model/hidden_layer/biases/params_summary\n",
      "XOR_model/hidden_layer/gradient_activation/data_summary\n",
      "XOR_model/hidden_layer/gradient_biases/data_summary\n",
      "XOR_model/hidden_layer/gradient_net_input/data_summary\n",
      "XOR_model/hidden_layer/gradient_weights/data_summary\n",
      "XOR_model/hidden_layer/net_input/data_summary\n",
      "XOR_model/hidden_layer/weights/params_summary\n",
      "XOR_model/input_patterns/data_summary\n",
      "XOR_model/output_layer/activations/data_summary\n",
      "XOR_model/output_layer/biases/params_summary\n",
      "XOR_model/output_layer/gradient_activation/data_summary\n",
      "XOR_model/output_layer/gradient_biases/data_summary\n",
      "XOR_model/output_layer/gradient_net_input/data_summary\n",
      "XOR_model/output_layer/gradient_weights/data_summary\n",
      "XOR_model/output_layer/net_input/data_summary\n",
      "XOR_model/output_layer/weights/params_summary\n",
      "XOR_model/pattern_labels/data_summary\n",
      "XOR_model/target_patterns/data_summary\n",
      "XOR_model/test_loss_summary\n",
      "XOR_model/train_loss_summary\n",
      "test_data/batch/fraction_of_12_full\n",
      "test_data/input_producer/fraction_of_32_full\n",
      "train_data/batch/fraction_of_12_full\n",
      "train_data/input_producer/fraction_of_32_full\n"
     ]
    }
   ],
   "source": [
    "tag_list = []\n",
    "for event in tf.train.summary_iterator(event_file):\n",
    "    for val in event.summary.value:\n",
    "        tag_list.append(val.tag)\n",
    "for tag in sorted(set(tag_list)):\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get summary by tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup = 'train_loss_summary'\n",
    "for event in tf.train.summary_iterator(event_file):\n",
    "    for val in event.summary.value:\n",
    "        if lookup in val.tag:\n",
    "            print('epoch {}: {} ({})'.format(event.step,val.simple_value, val.tag))\n",
    "#             print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch 0\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.60572779]]\n",
      "[[ 0.61312944]]\n",
      "[[ 0.6125145]]\n",
      "[[ 0.61962277]]\n",
      "> Epoch 30\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.51892483]]\n",
      "[[ 0.52193105]]\n",
      "[[ 0.52186847]]\n",
      "[[ 0.52490199]]\n",
      "> Epoch 60\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.50083667]]\n",
      "[[ 0.50215185]]\n",
      "[[ 0.50222403]]\n",
      "[[ 0.50354332]]\n",
      "> Epoch 90\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.49904162]]\n",
      "[[ 0.4996919]]\n",
      "[[ 0.4998275]]\n",
      "[[ 0.50046974]]\n",
      "> Epoch 120\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.49910894]]\n",
      "[[ 0.49972317]]\n",
      "[[ 0.49990478]]\n",
      "[[ 0.50050032]]\n",
      "> Epoch 150\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.49881288]]\n",
      "[[ 0.49981695]]\n",
      "[[ 0.50002939]]\n",
      "[[ 0.50098854]]\n",
      "> Epoch 180\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.49776125]]\n",
      "[[ 0.49983662]]\n",
      "[[ 0.50005382]]\n",
      "[[ 0.50198638]]\n",
      "> Epoch 210\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.49402064]]\n",
      "[[ 0.49992192]]\n",
      "[[ 0.50009495]]\n",
      "[[ 0.50516236]]\n",
      "> Epoch 240\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.46489674]]\n",
      "[[ 0.50466144]]\n",
      "[[ 0.50472176]]\n",
      "[[ 0.52480406]]\n",
      "> Epoch 270\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.13171937]]\n",
      "[[ 0.63581765]]\n",
      "[[ 0.63652474]]\n",
      "[[ 0.63739127]]\n",
      "> Epoch 300\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.08366089]]\n",
      "[[ 0.89098078]]\n",
      "[[ 0.8906697]]\n",
      "[[ 0.11457232]]\n",
      "> Epoch 317\n",
      "  labels:\n",
      "p00\n",
      "p01\n",
      "p10\n",
      "p11\n",
      "  inputs:\n",
      "[[ 0.  0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  1.]]\n",
      "  targets:\n",
      "[[ 0.]]\n",
      "[[ 1.]]\n",
      "[[ 1.]]\n",
      "[[ 0.]]\n",
      "  data:\n",
      "[[ 0.0483415]]\n",
      "[[ 0.95037484]]\n",
      "[[ 0.95042837]]\n",
      "[[ 0.04463822]]\n"
     ]
    }
   ],
   "source": [
    "hidden_weights = retrieve_model_params(event_file, 'hidden_layer', 'weights'); hidden_weights\n",
    "# hidden_bias = retrieve_model_params(event_file, 'hidden_layer', 'bias')\n",
    "hidden_acts = retrieve_model_data(event_file, 'output_layer', 'activation')\n",
    "display_model_data(hidden_acts)\n",
    "# loss_log = retrieve_loss(event_file)\n",
    "# D = get_layer_snapshot(event_file, 'hidden_layer', 0, 'p00', target=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
