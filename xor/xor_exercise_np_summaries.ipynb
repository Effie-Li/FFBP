{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR problem\n",
    "\n",
    "## 1. Preliminaries\n",
    "### 1.1. Imports\n",
    "We begin by importing several python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf logdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.3.0\n",
      "numpy version: 1.12.1\n",
      "current working directory: /Users/alexten/Projects/pdpyflow/xor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('current working directory: {}'.format(os.getcwd()))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snap2pickle(logdir, snap):\n",
    "    path = '/'.join([logdir,'snap.pkl'])\n",
    "    try:\n",
    "        with open(path, 'rb') as old_file:\n",
    "            old_snap = pickle.load(old_file)\n",
    "        with open(path, 'wb') as old_file:\n",
    "            old_snap.append(snap)\n",
    "            pickle.dump(old_snap, old_file)\n",
    "    except FileNotFoundError:\n",
    "        with open(path, 'wb') as new_file:\n",
    "            out = pickle.dump([snap], new_file)\n",
    "\n",
    "\n",
    "class InputData(object):\n",
    "    '''\n",
    "    DOCUMENTATION\n",
    "    '''\n",
    "    def __init__(self, path_to_data_file, num_epochs, batch_size, inp_size, targ_size, data_len,\n",
    "                 shuffle = False, shuffle_seed = None):\n",
    "        # Store useful params\n",
    "        self.path = path_to_data_file\n",
    "        self.batch_size = batch_size\n",
    "        self.inp_size = [inp_size] if isinstance(inp_size, int) else inp_size\n",
    "        self.targ_size = targ_size\n",
    "        self.data_len = data_len\n",
    "\n",
    "        # setup filename queue\n",
    "        filename_queue = tf.train.string_input_producer(string_tensor = [path_to_data_file], shuffle = False)\n",
    "\n",
    "        # create reader and setup default values to read from files in the filename queue\n",
    "        reader = tf.TextLineReader(skip_header_lines=True, name='csv_reader')\n",
    "        _, record_strings = reader.read_up_to(filename_queue, num_records=data_len)\n",
    "        defaults = [[0.0] for x in range(sum(self.inp_size) + targ_size)]\n",
    "        defaults.insert(0,[''])\n",
    "        \n",
    "        # decode in all lines\n",
    "        examples = tf.decode_csv(record_strings, record_defaults=defaults)\n",
    "\n",
    "        # slice the decoded lines and stack them into respective tensors\n",
    "        pattern_labels = tf.transpose(examples.pop(0))\n",
    "        input_patterns = []\n",
    "        start = 0\n",
    "        for size in self.inp_size:\n",
    "            input_patterns.append(\n",
    "                tf.transpose(tf.stack(examples[start:start+size]))\n",
    "            )\n",
    "            start += size\n",
    "        target_patterns = tf.transpose(tf.stack(examples[sum(self.inp_size):sum(self.inp_size) + targ_size]))\n",
    "\n",
    "        # enqueue lines into an examples queue (optionally shuffle)\n",
    "        tensor_list =  [pattern_labels]+input_patterns+[target_patterns]\n",
    "        examples_slice = tf.train.slice_input_producer(\n",
    "            tensor_list = tensor_list,\n",
    "            num_epochs = num_epochs,\n",
    "            shuffle = shuffle,\n",
    "            seed = shuffle_seed,\n",
    "            capacity = data_len\n",
    "        )\n",
    "\n",
    "        # set up a batch queue using the enqueued (optionally shuffled) examples\n",
    "        self.examples_batch = tf.train.batch(\n",
    "            tensors = examples_slice,\n",
    "            batch_size = batch_size,\n",
    "            capacity = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "class BasicLayer(object):\n",
    "    '''\n",
    "    DOCUMENTATION\n",
    "    '''\n",
    "    def __init__(self, layer_name, layer_input, size, wrange, nonlin=None, bias=True, seed=None, sparse_inp=False):\n",
    "        self.name = layer_name\n",
    "        with tf.variable_scope(layer_name):\n",
    "\n",
    "            if isinstance(layer_input, (list, tuple)):\n",
    "                self.input_ = tf.concat(axis=1, values=[i for i in layer_input])\n",
    "                input_size = sum([inp._shape[1]._value for inp in layer_input])\n",
    "            else:\n",
    "                self.input_ = layer_input\n",
    "                input_size = layer_input._shape[1]._value\n",
    "\n",
    "            weight_init = tf.random_uniform(\n",
    "                minval = wrange[0], \n",
    "                maxval = wrange[1],\n",
    "                seed = seed,\n",
    "                shape = [input_size, size],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.weights = tf.get_variable(name='weights', initializer=weight_init)\n",
    "\n",
    "            self.biases = 0\n",
    "            if bias:\n",
    "                bias_init = tf.random_uniform(\n",
    "                    minval = wrange[0],\n",
    "                    maxval = wrange[1],\n",
    "                    seed = seed,\n",
    "                    shape = [size],\n",
    "                    dtype = tf.float32\n",
    "                )\n",
    "                self.biases = tf.get_variable('biases', initializer=bias_init)\n",
    "\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope('net_input'):\n",
    "                self.net_input = tf.matmul(self.input_, self.weights, a_is_sparse=sparse_inp) + self.biases\n",
    "\n",
    "            with tf.name_scope('activations'):\n",
    "                self.nonlin = nonlin\n",
    "                if nonlin:\n",
    "                    self.output = nonlin(self.net_input)\n",
    "                else:\n",
    "                    self.output = self.net_input\n",
    "    \n",
    "    def add_gradient_ops(self, loss):\n",
    "        with tf.name_scope(self.name):\n",
    "            item_keys = ['net_input', 'activation', 'weights']\n",
    "            items = [self.net_input, self.output, self.weights]\n",
    "            if self.biases: \n",
    "                item_keys.append('biases')\n",
    "                items.append(self.biases)\n",
    "            grad_list = tf.gradients(loss, items)\n",
    "            grad_list_with_keys = [val for pair in zip(item_keys, grad_list) for val in pair]\n",
    "            self.gradient = {k:v for k,v in zip(*[iter(grad_list_with_keys)]*2)}\n",
    "            \n",
    "            for grad_op, str_key in zip(grad_list, item_keys):\n",
    "                self.__dict__['g{}'.format(str_key)] = grad_op\n",
    "    \n",
    "    def fetch_test_ops(self):\n",
    "        fetch_items = ['weights', 'biases', 'net_input', 'activation',\n",
    "                       'gweights', 'gbiases', 'gnet_input', 'gactivation']\n",
    "        fetch_ops = {}\n",
    "        for fi in fetch_items:\n",
    "            if fi in self.__dict__.keys():\n",
    "                fetch_ops[fi] = self.__dict__[fi]\n",
    "        return fetch_ops, self.name\n",
    "\n",
    "\n",
    "class FFBPModel(object):\n",
    "    '''\n",
    "    DOCUMENTATION\n",
    "    '''\n",
    "    def __init__(self, name, loss, optimizer, layers, inp, targ, train_data=None, test_data=None):\n",
    "        self.name = name\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self._step_incrementer = tf.assign_add(self._global_step, 1, name='global_step_incrementer')\n",
    "        self._train_step = self.optimizer.minimize(loss=self.loss, global_step=None)\n",
    "\n",
    "        self.layers = layers\n",
    "        for layer in self.layers: \n",
    "            layer.add_gradient_ops(loss=self.loss)\n",
    "\n",
    "        self.inp = [inp] if not isinstance(inp, (list, tuple)) else inp\n",
    "        self.targ = targ\n",
    "        self.inp_labels = tf.placeholder(shape=(), dtype=tf.string)\n",
    "\n",
    "        self.data = {'Test': test_data, 'Train': train_data}\n",
    "\n",
    "        self._prev_param = {}\n",
    "\n",
    "        if train_data:  \n",
    "            self.data['Train'] = train_data\n",
    "            self._train_fetches = {\n",
    "                'loss': self.loss,\n",
    "                'train_step': self._train_step,\n",
    "            }\n",
    "\n",
    "        if test_data:\n",
    "            self.test_data = self.data['Test'] = test_data\n",
    "            self._test_fetches = {\n",
    "                'loss'  : self.loss,\n",
    "                'enum'  : self._global_step,\n",
    "                'labels': self.inp_labels,\n",
    "                'input' : tf.concat(self.inp, axis=1) if len(self.inp) > 1 else self.inp[0],\n",
    "                'target': self.targ\n",
    "            }\n",
    "\n",
    "    def test_epoch(self, session, verbose=False):\n",
    "        assert self.data['Test'] is not None, 'Provide test data to run a test epoch'\n",
    "        data = self.data['Test']\n",
    "        snap = {}\n",
    "        with tf.name_scope('Test'):\n",
    "            all_examples = session.run(data.examples_batch)\n",
    "            loss_sum = 0\n",
    "            for example in zip(*all_examples):\n",
    "\n",
    "                # Put together lists of placeholders and values\n",
    "                placeholders = [self.inp_labels]+self.inp+[self.targ]\n",
    "                values = [example[0]]+[np.expand_dims(vec,0) for vec in example[1:]]\n",
    "                \n",
    "                # Interleave the two lists to be comprehended by dict() constructor\n",
    "                feed_list = [val for pair in zip(placeholders, values) for val in pair]\n",
    "                \n",
    "                # Construct a feed_dict with appropriately paired placeholders and feed values\n",
    "                feed_dict = dict(feed_list[i:i + 2] for i in range(0, len(feed_list), 2))\n",
    "\n",
    "                # Run graph to evaluate test fetches\n",
    "                test_out = session.run(\n",
    "                    fetches = self._test_fetches, \n",
    "                    feed_dict = feed_dict\n",
    "                )\n",
    "\n",
    "                # Store network-level snap items: enum, loss, labels, input, target\n",
    "                for k, v in test_out.items():\n",
    "                    if k=='enum':\n",
    "                        snap[k] = v\n",
    "                    elif k not in snap.keys(): \n",
    "                        snap[k] = np.expand_dims(v, axis=0)\n",
    "                    else:\n",
    "                        snap[k] = np.concatenate([snap[k], np.expand_dims(v, axis=0)], axis=0)\n",
    "\n",
    "                # Store layer-level snap items: weights, biases, net_input, activations and gradients\n",
    "                for layer in self.layers:\n",
    "                    layer_fetches, layer_name = layer.fetch_test_ops()\n",
    "                    snap.setdefault(layer_name, {})\n",
    "                    layer_out = session.run(\n",
    "                        fetches = layer_fetches, \n",
    "                        feed_dict = feed_dict\n",
    "                    )\n",
    "\n",
    "                    for k, v in layer_out.items():\n",
    "                        if k=='weights' or k=='biases':\n",
    "                            snap[layer_name][k] = v\n",
    "                            # TODO: Include dweights and dbiases (weight change applied without the momentum term)\n",
    "                            # if snap['enum'] == 0:\n",
    "                            #     self._prev_param[k] = v\n",
    "                            #     snap[layer_name]['d{}'.format(k)] = v*0\n",
    "                            # else:\n",
    "                            #     snap[layer_name]['d{}'.format(k)] =  v - self._prev_param[k]\n",
    "                        elif k not in snap[layer_name].keys():\n",
    "                            snap[layer_name][k] = np.expand_dims(v,axis = 0)\n",
    "                        else:\n",
    "                            snap[layer_name][k] = np.concatenate([snap[layer_name][k], np.expand_dims(v, axis=0)], axis=0)\n",
    "                loss_sum += test_out['loss']\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), loss_sum))\n",
    "            return loss_sum, snap\n",
    "\n",
    "    def train_epoch(self, session, verbose=False):\n",
    "        assert self.data['Train'] is not None, 'Provide train data to run a train epoch'\n",
    "        data = self.data['Train']\n",
    "        epoch_loss = 0\n",
    "        with tf.name_scope('Train'):\n",
    "            for mini_batch in range(data.data_len // data.batch_size):\n",
    "                examples_batch = session.run(data.examples_batch)\n",
    "                feed_list = [val for pair in zip(self.inp + [self.targ], examples_batch[1:]) for val in pair]\n",
    "                feed_dict = dict(feed_list[i:i + 2] for i in range(0, len(feed_list), 2))\n",
    "                evaled_ops = session.run(\n",
    "                    fetches = self._train_fetches,\n",
    "                    feed_dict = feed_dict\n",
    "                )\n",
    "                epoch_loss += evaled_ops['loss']\n",
    "\n",
    "        if verbose:\n",
    "            print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), epoch_loss))\n",
    "\n",
    "        session.run(self._step_incrementer)\n",
    "        return epoch_loss\n",
    "        \n",
    "\n",
    "def use_exercise_params(use):\n",
    "    if use:\n",
    "        all_vars = tf.global_variables()\n",
    "        hidden_W = [v for v in all_vars if 'hidden_layer/weights' in v.name][0]\n",
    "        hidden_b = [v for v in all_vars if 'hidden_layer/biases' in v.name][0]\n",
    "        output_W = [v for v in all_vars if 'output_layer/weights' in v.name][0]\n",
    "        output_b = [v for v in all_vars if 'output_layer/biases' in v.name][0]\n",
    "        restore_dict = {'w_1': hidden_W,'b_1': hidden_b,'w_2': output_W,'b_2': output_b}\n",
    "        tf.train.Saver(restore_dict, name='xor_exercise_saver').restore(tf.get_default_session(), 'exercise_params_old/exercise_params')\n",
    "\n",
    "        \n",
    "def get_logdir(path=None):\n",
    "    if path:\n",
    "        logdir = os.path.join(os.getcwd(),path)\n",
    "    else:\n",
    "        i=0\n",
    "        logdir = os.getcwd() + '/logdirs/ffbp_logdir_000'\n",
    "        while os.path.exists(logdir):\n",
    "            i+=.001\n",
    "            logdir = os.getcwd() + '/logdirs/ffbp_logdir_{}'.format(str(i)[2:5])\n",
    "        os.makedirs(logdir)\n",
    "        print('logdir path: {}'.format(logdir))\n",
    "    return logdir\n",
    "\n",
    "\n",
    "def save(sess, saver, logdir, model):\n",
    "    save_to = '/'.join([logdir,'checkpoint_files',model.name])\n",
    "    save_path = saver.save(sess, save_to, global_step=model._global_step)\n",
    "    print(\"Model saved in: {}\".format(save_path))\n",
    "    \n",
    "    \n",
    "def init_vars(session, checkpoint_dir=None):\n",
    "#     tf.reset_default_graph()\n",
    "    '''\n",
    "    Returns tf.train.Saver that can be used to checkpoint the graph\n",
    "    '''\n",
    "    saver = tf.train.Saver(write_version=tf.train.SaverDef.V2, name='saver')\n",
    "    if checkpoint_dir:\n",
    "        checkpoint_dir = os.path.join(os.getcwd(), checkpoint_dir)\n",
    "        print('Initializing local variables and restoring global variables from: {}'.format(checkpoint_dir))\n",
    "        saved_files = os.listdir(checkpoint_dir)\n",
    "        for file in saved_files:\n",
    "            if '.meta' in file:\n",
    "                checkpoint_file = file.split(sep='.')[0]\n",
    "                saver.restore(session, os.path.join(checkpoint_dir, checkpoint_file))\n",
    "        session.run(tf.local_variables_initializer())\n",
    "    else:\n",
    "        print('Initializing local and global variables from scratch')\n",
    "        session.run(tf.local_variables_initializer())\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN CONFIGS\n",
    "num_epochs = 330\n",
    "batch_size = 2\n",
    "inp_size = 2\n",
    "targ_size = 1\n",
    "data_len = 4\n",
    "\n",
    "lr = 0.5\n",
    "m = 0.9\n",
    "\n",
    "xor_graph = tf.Graph()\n",
    "\n",
    "with xor_graph.as_default():\n",
    "    \n",
    "    with tf.name_scope('train_data'):\n",
    "        train_examples = InputData(\n",
    "            path_to_data_file = 'train_data_B.txt',\n",
    "            num_epochs = num_epochs,\n",
    "            batch_size = batch_size, \n",
    "            inp_size = 2, \n",
    "            targ_size = 1,\n",
    "            data_len = data_len,\n",
    "            shuffle = True, \n",
    "            shuffle_seed = 1\n",
    "        )\n",
    "\n",
    "    with tf.name_scope('test_data'):\n",
    "        test_examples = InputData(\n",
    "            path_to_data_file = 'train_data_B.txt',\n",
    "            num_epochs = num_epochs,\n",
    "            batch_size = data_len,\n",
    "            inp_size = 2, \n",
    "            targ_size = 1,\n",
    "            data_len = data_len,\n",
    "            shuffle = False\n",
    "        )\n",
    "\n",
    "    # NETWORK CONSTRUCTION\n",
    "    model_name = 'xor_model'\n",
    "    with tf.name_scope(model_name):\n",
    "\n",
    "        model_inp  = tf.placeholder(dtype = tf.float32, shape=[None, inp_size], name='model_inp')\n",
    "\n",
    "        hidden_layer = BasicLayer(\n",
    "            layer_name = 'hidden_layer', \n",
    "            layer_input = model_inp, \n",
    "            size = 2, \n",
    "            wrange = [-1,1], \n",
    "            nonlin=tf.nn.sigmoid, \n",
    "            bias=True, \n",
    "            seed=1, # Use None for random seed value\n",
    "            sparse_inp=False\n",
    "        )\n",
    "\n",
    "        output_layer = BasicLayer(\n",
    "            layer_name = 'output_layer', \n",
    "            layer_input = hidden_layer.output, \n",
    "            size = 1, \n",
    "            wrange = [-1,1], \n",
    "            nonlin=tf.nn.sigmoid, \n",
    "            bias=True, \n",
    "            seed=1, # Use None for random seed value\n",
    "            sparse_inp=False\n",
    "        )\n",
    "\n",
    "        target = tf.placeholder(dtype = tf.float32, shape=[None, targ_size], name='targets')\n",
    "\n",
    "        model = FFBPModel(\n",
    "            name = model_name,\n",
    "            layers = [hidden_layer, output_layer],\n",
    "            train_data = train_examples, \n",
    "            inp        = model_inp,\n",
    "            targ       = target,\n",
    "            loss       = tf.reduce_sum(tf.squared_difference(target, output_layer.output), name='loss_function'),\n",
    "            optimizer  = tf.train.MomentumOptimizer(lr, m),\n",
    "            test_data  = test_examples\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running and saving model\n",
    "This part should be general (not edited much by the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logdir path: /Users/alexten/Projects/pdpyflow/xor/logdirs/ffbp_logdir_000\n",
      "Initializing local variables and restoring global variables from: /Users/alexten/Projects/pdpyflow/xor/exercise_params\n",
      "INFO:tensorflow:Restoring parameters from /Users/alexten/Projects/pdpyflow/xor/exercise_params/xor_model-0\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 0: 1.0506523847579956\n"
     ]
    }
   ],
   "source": [
    "test_epochs = [0,1,3,5,30,60,180,300]\n",
    "restore = 'exercise_params' # Use None to init from scratch\n",
    "\n",
    "with tf.Session(graph=xor_graph) as sess:\n",
    "    \n",
    "    # make new logdir in logdirs directory\n",
    "    logdir = get_logdir(path=None)\n",
    "    \n",
    "    # initialize or restore variables\n",
    "    saver = init_vars(session=sess, checkpoint_dir=restore)\n",
    "    \n",
    "    # create coordinator and queue runners\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    \n",
    "    for i in range(num_epochs+1): #for i in range(start_epoch, num_epochs+1):\n",
    "        # Do planned tests and test when num_epochs is reached\n",
    "        if any([i==test_epoch for test_epoch in test_epochs]):\n",
    "            loss, snap = model.test_epoch(session=sess, verbose=True)\n",
    "            snap2pickle(logdir, snap)\n",
    "            \n",
    "#         # Run training epoch\n",
    "#         loss = model.train_epoch(session=sess, verbose=False)\n",
    "        \n",
    "#         # Test and break if loss < ecrit\n",
    "#         if loss < ecrit: \n",
    "#             loss, snap = model.test_epoch(session=sess, verbose=True)\n",
    "#             snap2pickle(logdir, snap)\n",
    "            \n",
    "#             print('Stopped training due to loss < ecrit')\n",
    "#             break\n",
    "\n",
    "    coordinator.request_stop()\n",
    "    coordinator.join(threads)\n",
    "\n",
    "#     save(sess, saver, logdir, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
