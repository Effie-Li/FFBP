{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR problem\n",
    "\n",
    "## 1. Preliminaries\n",
    "### 1.1. Imports\n",
    "We begin by importing several python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf logdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.3.0\n",
      "numpy version: 1.12.1\n",
      "current working directory: /Users/alexten/Projects/pdpyflow/xor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('current working directory: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snap2pickle(logdir, snap):\n",
    "    path = '/'.join([logdir,'snap.pkl'])\n",
    "    try:\n",
    "        with open(path, 'rb') as old_file:\n",
    "            old_snap = pickle.load(old_file)\n",
    "        with open(path, 'wb') as old_file:\n",
    "            old_snap.append(snap)\n",
    "            pickle.dump(old_snap, old_file)\n",
    "    except FileNotFoundError:\n",
    "        with open(path, 'wb') as new_file:\n",
    "            out = pickle.dump([snap], new_file)\n",
    "\n",
    "\n",
    "class InputData(object):\n",
    "    '''\n",
    "    DOCUMENTATION\n",
    "    '''\n",
    "    def __init__(self, path_to_data_file, num_epochs, batch_size, inp_size, targ_size, data_len,\n",
    "                 shuffle = False, shuffle_seed = None):\n",
    "        # Store useful params\n",
    "        self.path = path_to_data_file\n",
    "        self.batch_size = batch_size\n",
    "        self.inp_size = [inp_size] if isinstance(inp_size, int) else inp_size\n",
    "        self.targ_size = targ_size\n",
    "        self.data_len = data_len\n",
    "\n",
    "        # setup filename queue\n",
    "        filename_queue = tf.train.string_input_producer(string_tensor = [path_to_data_file], shuffle = False)\n",
    "\n",
    "        # create reader and setup default values to read from files in the filename queue\n",
    "        reader = tf.TextLineReader(skip_header_lines=True, name='csv_reader')\n",
    "        _, record_strings = reader.read_up_to(filename_queue, num_records=data_len)\n",
    "        defaults = [[0.0] for x in range(sum(self.inp_size) + targ_size)]\n",
    "        defaults.insert(0,[''])\n",
    "        \n",
    "        # decode in all lines\n",
    "        examples = tf.decode_csv(record_strings, record_defaults=defaults)\n",
    "\n",
    "        # slice the decoded lines and stack them into respective tensors\n",
    "        pattern_labels = tf.transpose(examples.pop(0))\n",
    "        input_patterns = []\n",
    "        start = 0\n",
    "        for size in self.inp_size:\n",
    "            input_patterns.append(\n",
    "                tf.transpose(tf.stack(examples[start:start+size]))\n",
    "            )\n",
    "            start += size\n",
    "        target_patterns = tf.transpose(tf.stack(examples[sum(self.inp_size):sum(self.inp_size) + targ_size]))\n",
    "\n",
    "        # enqueue lines into an examples queue (optionally shuffle)\n",
    "        tensor_list =  [pattern_labels]+input_patterns+[target_patterns]\n",
    "        examples_slice = tf.train.slice_input_producer(\n",
    "            tensor_list = tensor_list,\n",
    "            num_epochs = num_epochs,\n",
    "            shuffle = shuffle,\n",
    "            seed = shuffle_seed,\n",
    "            capacity = data_len\n",
    "        )\n",
    "\n",
    "        # set up a batch queue using the enqueued (optionally shuffled) examples\n",
    "        self.examples_batch = tf.train.batch(\n",
    "            tensors = examples_slice,\n",
    "            batch_size = batch_size,\n",
    "            capacity = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "class BasicLayer(object):\n",
    "    '''\n",
    "    DOCUMENTATION\n",
    "    '''\n",
    "    def __init__(self, layer_name, layer_input, size, wrange, nonlin=None, bias=True, seed=None, sparse_inp=False):\n",
    "        self.name = layer_name\n",
    "        with tf.variable_scope(layer_name):\n",
    "\n",
    "            if isinstance(layer_input, (list, tuple)):\n",
    "                self.input_ = tf.concat(axis=1, values=[i for i in layer_input])\n",
    "                input_size = sum([inp._shape[1]._value for inp in layer_input])\n",
    "            else:\n",
    "                self.input_ = layer_input\n",
    "                input_size = layer_input._shape[1]._value\n",
    "\n",
    "            weight_init = tf.random_uniform(\n",
    "                minval = wrange[0], \n",
    "                maxval = wrange[1],\n",
    "                seed = seed,\n",
    "                shape = [input_size, size],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.weights = tf.get_variable(name='weights', initializer=weight_init)\n",
    "\n",
    "            self.biases = 0\n",
    "            if bias:\n",
    "                bias_init = tf.random_uniform(\n",
    "                    minval = wrange[0],\n",
    "                    maxval = wrange[1],\n",
    "                    seed = seed,\n",
    "                    shape = [size],\n",
    "                    dtype = tf.float32\n",
    "                )\n",
    "                self.biases = tf.get_variable('biases', initializer=bias_init)\n",
    "\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope('net_input'):\n",
    "                self.net_input = tf.matmul(self.input_, self.weights, a_is_sparse=sparse_inp) + self.biases\n",
    "\n",
    "            with tf.name_scope('activations'):\n",
    "                self.nonlin = nonlin\n",
    "                if nonlin:\n",
    "                    self.output = nonlin(self.net_input)\n",
    "                else:\n",
    "                    self.output = self.net_input\n",
    "    \n",
    "    def add_gradient_ops(self, loss):\n",
    "        with tf.name_scope(self.name):\n",
    "            item_keys = ['net_input', 'activation', 'weights']\n",
    "            items = [self.net_input, self.output, self.weights]\n",
    "            if self.biases: \n",
    "                item_keys.append('biases')\n",
    "                items.append(self.biases)\n",
    "            grad_list = tf.gradients(loss, items)\n",
    "            grad_list_with_keys = [val for pair in zip(item_keys, grad_list) for val in pair]\n",
    "            self.gradient = {k:v for k,v in zip(*[iter(grad_list_with_keys)]*2)}\n",
    "            \n",
    "            for grad_op, str_key in zip(grad_list, item_keys):\n",
    "                self.__dict__['g{}'.format(str_key)] = grad_op\n",
    "    \n",
    "    def fetch_test_ops(self):\n",
    "        fetch_items = ['weights', 'biases', 'net_input', 'activation',\n",
    "                       'gweights', 'gbiases', 'gnet_input', 'gactivation']\n",
    "        fetch_ops = {}\n",
    "        for fi in fetch_items:\n",
    "            if fi in self.__dict__.keys():\n",
    "                fetch_ops[fi] = self.__dict__[fi]\n",
    "        return fetch_ops, self.name\n",
    "\n",
    "\n",
    "class FFBPModel(object):\n",
    "    '''\n",
    "    DOCUMENTATION\n",
    "    '''\n",
    "    def __init__(self, name, loss, optimizer, layers, inp, targ, train_data=None, test_data=None):\n",
    "        self.name = name\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self._step_incrementer = tf.assign_add(self._global_step, 1, name='global_step_incrementer')\n",
    "        self._train_step = self.optimizer.minimize(loss=self.loss, global_step=None)\n",
    "\n",
    "        self.layers = layers\n",
    "        for layer in self.layers: \n",
    "            layer.add_gradient_ops(loss=self.loss)\n",
    "\n",
    "        self.inp = [inp] if not isinstance(inp, (list, tuple)) else inp\n",
    "        self.targ = targ\n",
    "        self.inp_labels = tf.placeholder(shape=(), dtype=tf.string)\n",
    "\n",
    "        self.data = {'Test': test_data, 'Train': train_data}\n",
    "\n",
    "        self._prev_param = {}\n",
    "\n",
    "        if train_data:  \n",
    "            self.data['Train'] = train_data\n",
    "            self._train_fetches = {\n",
    "                'loss': self.loss,\n",
    "                'train_step': self._train_step,\n",
    "            }\n",
    "\n",
    "        if test_data:\n",
    "            self.test_data = self.data['Test'] = test_data\n",
    "            self._test_fetches = {\n",
    "                'loss'  : self.loss,\n",
    "                'enum'  : self._global_step,\n",
    "                'labels': self.inp_labels,\n",
    "                'input' : tf.concat(self.inp, axis=1) if len(self.inp) > 1 else self.inp[0],\n",
    "                'target': self.targ\n",
    "            }\n",
    "\n",
    "    def test_epoch(self, session, verbose=False):\n",
    "        assert self.data['Test'] is not None, 'Provide test data to run a test epoch'\n",
    "        data = self.data['Test']\n",
    "        snap = {}\n",
    "        with tf.name_scope('Test'):\n",
    "            all_examples = session.run(data.examples_batch)\n",
    "            loss_sum = 0\n",
    "            for example in zip(*all_examples):\n",
    "\n",
    "                # Put together lists of placeholders and values\n",
    "                placeholders = [self.inp_labels]+self.inp+[self.targ]\n",
    "                values = [example[0]]+[np.expand_dims(vec,0) for vec in example[1:]]\n",
    "                # Interleave the two lists to be comprehended by dict() constructor\n",
    "                feed_list = [val for pair in zip(placeholders, values) for val in pair]\n",
    "                # Construct a feed_dict with appropriately paired placeholders and feed values\n",
    "                feed_dict = dict(feed_list[i:i + 2] for i in range(0, len(feed_list), 2))\n",
    "\n",
    "                # Run graph to evaluate test fetches\n",
    "                test_out = session.run(\n",
    "                    fetches = self._test_fetches, \n",
    "                    feed_dict = feed_dict\n",
    "                )\n",
    "\n",
    "                # Store network-level snap items: enum, loss, labels, input, target\n",
    "                for k, v in test_out.items():\n",
    "                    if k=='enum':\n",
    "                        snap[k] = v\n",
    "                    elif k not in snap.keys(): \n",
    "                        snap[k] = np.expand_dims(v, axis=0)\n",
    "                    else:\n",
    "                        snap[k] = np.concatenate([snap[k], np.expand_dims(v, axis=0)], axis=0)\n",
    "\n",
    "                # Store layer-level snap items: weights, biases, net_input, activations and gradients\n",
    "                for layer in self.layers:\n",
    "                    layer_fetches, layer_name = layer.fetch_test_ops()\n",
    "                    snap.setdefault(layer_name, {})\n",
    "                    layer_out = session.run(\n",
    "                        fetches = layer_fetches, \n",
    "                        feed_dict = feed_dict\n",
    "                    )\n",
    "\n",
    "                    for k, v in layer_out.items():\n",
    "                        if k=='weights' or k=='biases':\n",
    "                            snap[layer_name][k] = v\n",
    "                            # TODO: Include dweights and dbiases (weight change applied without the momentum term)\n",
    "                            # if snap['enum'] == 0:\n",
    "                            #     self._prev_param[k] = v\n",
    "                            #     snap[layer_name]['d{}'.format(k)] = v*0\n",
    "                            # else:\n",
    "                            #     snap[layer_name]['d{}'.format(k)] =  v - self._prev_param[k]\n",
    "                        elif k not in snap[layer_name].keys():\n",
    "                            snap[layer_name][k] = np.expand_dims(v,axis = 0)\n",
    "                        else:\n",
    "                            snap[layer_name][k] = np.concatenate([snap[layer_name][k], np.expand_dims(v, axis=0)], axis=0)\n",
    "                loss_sum += test_out['loss']\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), loss_sum))\n",
    "            return loss_sum, snap\n",
    "\n",
    "    def train_epoch(self, session, verbose=False):\n",
    "        assert self.data['Train'] is not None, 'Provide train data to run a train epoch'\n",
    "        data = self.data['Train']\n",
    "        epoch_loss = 0\n",
    "        with tf.name_scope('Train'):\n",
    "            for mini_batch in range(data.data_len // data.batch_size):\n",
    "                examples_batch = session.run(data.examples_batch)\n",
    "                feed_list = [val for pair in zip(self.inp + [self.targ], examples_batch[1:]) for val in pair]\n",
    "                feed_dict = dict(feed_list[i:i + 2] for i in range(0, len(feed_list), 2))\n",
    "                evaled_ops = session.run(\n",
    "                    fetches = self._train_fetches,\n",
    "                    feed_dict = feed_dict\n",
    "                )\n",
    "                epoch_loss += evaled_ops['loss']\n",
    "\n",
    "        if verbose:\n",
    "            print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), epoch_loss))\n",
    "\n",
    "        session.run(self._step_incrementer)\n",
    "        return epoch_loss\n",
    "        \n",
    "\n",
    "def use_exercise_params(use):\n",
    "    if use:\n",
    "        all_vars = tf.global_variables()\n",
    "        hidden_W = [v for v in all_vars if 'hidden_layer/weights' in v.name][0]\n",
    "        hidden_b = [v for v in all_vars if 'hidden_layer/biases' in v.name][0]\n",
    "        output_W = [v for v in all_vars if 'output_layer/weights' in v.name][0]\n",
    "        output_b = [v for v in all_vars if 'output_layer/biases' in v.name][0]\n",
    "        restore_dict = {'w_1': hidden_W,'b_1': hidden_b,'w_2': output_W,'b_2': output_b}\n",
    "        tf.train.Saver(restore_dict, name='xor_exercise_saver').restore(tf.get_default_session(), 'exercise_params_old/exercise_params')\n",
    "\n",
    "        \n",
    "def new_logdir():\n",
    "    i=0\n",
    "    logdir = os.getcwd() + '/logdirs/ffbp_logdir_000'\n",
    "    while os.path.exists(logdir):\n",
    "        i+=.001\n",
    "        logdir = os.getcwd() + '/logdirs/ffbp_logdir_{}'.format(str(i)[2:5])\n",
    "    os.makedirs(logdir)\n",
    "    print('logdir path: {}'.format(logdir))\n",
    "    return logdir\n",
    "\n",
    "\n",
    "def save(sess, saver, logdir, model):\n",
    "    save_to = '/'.join([logdir,'checkpoint_files',model.name+'.ckpt'])\n",
    "    save_path = saver.save(sess, save_to)\n",
    "    print(\"Model saved in: {}\".format(save_path))\n",
    "    \n",
    "    \n",
    "def init_vars(session, checkpoint_dir=None):\n",
    "#     tf.reset_default_graph()\n",
    "    '''\n",
    "    Returns tf.train.Saver that can be used to checkpoint the graph\n",
    "    '''\n",
    "#     write_version=tf.train.SaverDef.V2\n",
    "    saver = tf.train.Saver(name='model_loader')\n",
    "    if checkpoint_dir:\n",
    "        checkpoint_dir = os.path.join(os.getcwd(), checkpoint_dir)\n",
    "        saved_files = os.listdir(checkpoint_dir)\n",
    "        for file in saved_files:\n",
    "            if 'ckpt.meta' in file:\n",
    "                metagraphdef = file.split(sep='.')[0]\n",
    "                print('trying to restore from this file:') #       <---------------\n",
    "                print(os.path.join(checkpoint_dir, metagraphdef))# <---------------\n",
    "                saver.restore(session, os.path.join(checkpoint_dir, metagraphdef))\n",
    "                continue\n",
    "        print('Restoring variables from {}'.format(checkpoint_dir))\n",
    "    else:\n",
    "        session.run(tf.local_variables_initializer())\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        print('Initializing variables from scratch')\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN CONFIGS\n",
    "num_epochs = 330\n",
    "batch_size = 2\n",
    "inp_size = 2\n",
    "targ_size = 1\n",
    "data_len = 4\n",
    "\n",
    "lr = 0.5\n",
    "m = 0.9\n",
    "\n",
    "xor_graph = tf.Graph()\n",
    "\n",
    "with xor_graph.as_default():\n",
    "    \n",
    "    with tf.name_scope('train_data'):\n",
    "        train_examples = InputData(\n",
    "            path_to_data_file = 'train_data_B.txt',\n",
    "            num_epochs = num_epochs,\n",
    "            batch_size = batch_size, \n",
    "            inp_size = 2, \n",
    "            targ_size = 1,\n",
    "            data_len = data_len,\n",
    "            shuffle = True, \n",
    "            shuffle_seed = 1\n",
    "        )\n",
    "\n",
    "    with tf.name_scope('test_data'):\n",
    "        test_examples = InputData(\n",
    "            path_to_data_file = 'train_data_B.txt',\n",
    "            num_epochs = num_epochs,\n",
    "            batch_size = data_len,\n",
    "            inp_size = 2, \n",
    "            targ_size = 1,\n",
    "            data_len = data_len,\n",
    "            shuffle = False\n",
    "        )\n",
    "\n",
    "    # NETWORK CONSTRUCTION\n",
    "    model_name = 'xor_model'\n",
    "    with tf.name_scope(model_name):\n",
    "\n",
    "        model_inp  = tf.placeholder(dtype = tf.float32, shape=[None, inp_size], name='model_inp')\n",
    "\n",
    "        hidden_layer = BasicLayer(\n",
    "            layer_name = 'hidden_layer', \n",
    "            layer_input = model_inp, \n",
    "            size = 2, \n",
    "            wrange = [-1,1], \n",
    "            nonlin=tf.nn.sigmoid, \n",
    "            bias=True, \n",
    "            seed=1, # Use None for random seed value\n",
    "            sparse_inp=False\n",
    "        )\n",
    "\n",
    "        output_layer = BasicLayer(\n",
    "            layer_name = 'output_layer', \n",
    "            layer_input = hidden_layer.output, \n",
    "            size = 1, \n",
    "            wrange = [-1,1], \n",
    "            nonlin=tf.nn.sigmoid, \n",
    "            bias=True, \n",
    "            seed=1, # Use None for random seed value\n",
    "            sparse_inp=False\n",
    "        )\n",
    "\n",
    "        target = tf.placeholder(dtype = tf.float32, shape=[None, targ_size], name='targets')\n",
    "\n",
    "        model = FFBPModel(\n",
    "            name = model_name,\n",
    "            layers = [hidden_layer, output_layer],\n",
    "            train_data = train_examples, \n",
    "            inp        = model_inp,\n",
    "            targ       = target,\n",
    "            loss       = tf.reduce_sum(tf.squared_difference(target, output_layer.output), name='loss_function'),\n",
    "            optimizer  = tf.train.MomentumOptimizer(lr, m),\n",
    "            test_data  = test_examples\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running and saving model\n",
    "This part should be general (not edited much by the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logdir path: /Users/alexten/Projects/pdpyflow/xor/logdirs/ffbp_logdir_000\n",
      "Initializing variables from scratch\n",
      "INFO:tensorflow:Restoring parameters from exercise_params2/exercise_params\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on exercise_params2/exercise_params: Not found: exercise_params2\n\t [[Node: xor_exercise_saver/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_xor_exercise_saver/Const_0_0, xor_exercise_saver/RestoreV2_2/tensor_names, xor_exercise_saver/RestoreV2_2/shape_and_slices)]]\n\nCaused by op 'xor_exercise_saver/RestoreV2_2', defined at:\n  File \"/Users/alexten/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/alexten/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-59a0caa71791>\", line 9, in <module>\n    use_exercise_params(True)\n  File \"<ipython-input-3-7cfdf4711483>\", line 270, in use_exercise_params\n    tf.train.Saver(restore_dict, name='xor_exercise_saver').restore(tf.get_default_session(), 'exercise_params2/exercise_params')\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1140, in __init__\n    self.build()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 663, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on exercise_params2/exercise_params: Not found: exercise_params2\n\t [[Node: xor_exercise_saver/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_xor_exercise_saver/Const_0_0, xor_exercise_saver/RestoreV2_2/tensor_names, xor_exercise_saver/RestoreV2_2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on exercise_params2/exercise_params: Not found: exercise_params2\n\t [[Node: xor_exercise_saver/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_xor_exercise_saver/Const_0_0, xor_exercise_saver/RestoreV2_2/tensor_names, xor_exercise_saver/RestoreV2_2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-59a0caa71791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;31m#'exercise_params'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muse_exercise_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# create coordinator and queue runners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7cfdf4711483>\u001b[0m in \u001b[0;36muse_exercise_params\u001b[0;34m(use)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0moutput_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'output_layer/biases'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mrestore_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'w_1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhidden_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b_1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhidden_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w_2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b_2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_b\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xor_exercise_saver'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exercise_params2/exercise_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1560\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on exercise_params2/exercise_params: Not found: exercise_params2\n\t [[Node: xor_exercise_saver/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_xor_exercise_saver/Const_0_0, xor_exercise_saver/RestoreV2_2/tensor_names, xor_exercise_saver/RestoreV2_2/shape_and_slices)]]\n\nCaused by op 'xor_exercise_saver/RestoreV2_2', defined at:\n  File \"/Users/alexten/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/alexten/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-59a0caa71791>\", line 9, in <module>\n    use_exercise_params(True)\n  File \"<ipython-input-3-7cfdf4711483>\", line 270, in use_exercise_params\n    tf.train.Saver(restore_dict, name='xor_exercise_saver').restore(tf.get_default_session(), 'exercise_params2/exercise_params')\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1140, in __init__\n    self.build()\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 663, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/alexten/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on exercise_params2/exercise_params: Not found: exercise_params2\n\t [[Node: xor_exercise_saver/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_xor_exercise_saver/Const_0_0, xor_exercise_saver/RestoreV2_2/tensor_names, xor_exercise_saver/RestoreV2_2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "test_epochs = [0,1,3,5,30,60,180,300]\n",
    "\n",
    "with tf.Session(graph=xor_graph) as sess:\n",
    "    # initialize variables\n",
    "    # TODO create (init) or restore model here\n",
    "    logdir = new_logdir()\n",
    "    ckpt = None#'exercise_params'\n",
    "    saver = init_vars(session=sess, checkpoint_dir=ckpt)\n",
    "    use_exercise_params(True)\n",
    "    \n",
    "    # create coordinator and queue runners\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    \n",
    "#     for i in range(num_epochs+1): #for i in range(start_epoch, num_epochs+1):\n",
    "#         # Do planned tests and test when num_epochs is reached\n",
    "#         if any([i==test_epoch for test_epoch in test_epochs]):\n",
    "#             loss, snap = model.test_epoch(session=sess, verbose=True)\n",
    "#             snap2pickle(logdir, snap)\n",
    "            \n",
    "#         # Run training epoch\n",
    "#         loss = model.train_epoch(session=sess, verbose=False)\n",
    "        \n",
    "#         # Test and break if loss < ecrit\n",
    "#         if loss < ecrit: \n",
    "#             loss, snap = model.test_epoch(session=sess, verbose=True)\n",
    "#             snap2pickle(logdir, snap)\n",
    "            \n",
    "#             coordinator.request_stop()\n",
    "#             coordinator.join(threads)\n",
    "            \n",
    "#             print('Stopped training due to loss < ecrit')\n",
    "#             break\n",
    "    save(sess, saver, logdir, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
