{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR problem\n",
    "\n",
    "## 1. Preliminaries\n",
    "### 1.1. Imports\n",
    "We begin by importing several python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.3.0\n",
      "numpy version: 1.12.1\n",
      "current working directory: /Users/alexten/Projects/pdpyflow/xor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('current working directory: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def snap2pickle(logdir, snap):\n",
    "    path = '/'.join([logdir,'snap.pkl'])\n",
    "    try:\n",
    "        with open(path, 'rb') as old_file:\n",
    "            old_snap = pickle.load(old_file)\n",
    "        with open(path, 'wb') as old_file:\n",
    "            old_snap.append(snap)\n",
    "            pickle.dump(old_snap, old_file)\n",
    "    except FileNotFoundError:\n",
    "        with open(path, 'wb') as new_file:\n",
    "            out = pickle.dump([snap], new_file)\n",
    "\n",
    "\n",
    "class InputData(object):\n",
    "    def __init__(self, path_to_data_file, num_epochs, batch_size, inp_size, targ_size, data_len,\n",
    "                 shuffle = False, shuffle_seed = None):\n",
    "        # Store useful params\n",
    "        self.path = path_to_data_file\n",
    "        self.batch_size = batch_size\n",
    "        self.inp_size = inp_size\n",
    "        self.targ_size = targ_size\n",
    "        self.data_len = data_len\n",
    "\n",
    "        # setup filename queue\n",
    "        filename_queue = tf.train.string_input_producer(string_tensor = [path_to_data_file], shuffle = False)\n",
    "\n",
    "        # create reader and setup defaul values to read from files in the filename queue\n",
    "        reader = tf.TextLineReader(skip_header_lines=True, name='csv_reader')\n",
    "        _, record_strings = reader.read_up_to(filename_queue, num_records=data_len)\n",
    "        defaults = [[0.0] for x in range(inp_size + targ_size)]\n",
    "        defaults.insert(0,[''])\n",
    "        examples = tf.decode_csv(record_strings, record_defaults=defaults)\n",
    "\n",
    "        # read and decode examples into tensors\n",
    "        pattern_labels = tf.transpose(examples.pop(0))\n",
    "        input_patterns = tf.transpose(tf.stack(examples[0:inp_size]))\n",
    "        target_patterns = tf.transpose(tf.stack(examples[inp_size:inp_size + targ_size]))\n",
    "        # setup a batch queue for tensor examples\n",
    "        examples_slice = tf.train.slice_input_producer(\n",
    "            tensor_list = [pattern_labels, input_patterns, target_patterns],\n",
    "            num_epochs = num_epochs,\n",
    "            shuffle = shuffle,\n",
    "            seed = shuffle_seed,\n",
    "            capacity = data_len\n",
    "        )\n",
    "        self.example_batches = tf.train.batch(\n",
    "            tensors = examples_slice,\n",
    "            batch_size = batch_size,\n",
    "            num_threads = 1,\n",
    "            capacity = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "class BasicLayer(object):\n",
    "    def __init__(self, layer_name, layer_input, size, wrange, nonlin=None, bias=True, seed=None, sparse_inp=False):\n",
    "        self.name = layer_name\n",
    "        with tf.name_scope(layer_name):\n",
    "            self.input_ = layer_input\n",
    "            if type(layer_input) != tf.Tensor and hasattr(layer_name, '__iter__'):\n",
    "                self.input_ = tf.concat(axis=1, values=[i for i in layer_input])\n",
    "            input_size = layer_input._shape[1]._value\n",
    "            with tf.name_scope('weights'):\n",
    "\n",
    "                self.weights = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        minval = wrange[0], \n",
    "                        maxval = wrange[1],\n",
    "                        seed = seed,\n",
    "                        shape = [input_size, size],\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            self.biases = 0\n",
    "            if bias:\n",
    "                with tf.name_scope('biases'):\n",
    "                    self.biases = tf.Variable(\n",
    "                        tf.random_uniform(\n",
    "                            minval = wrange[0],\n",
    "                            maxval = wrange[1],\n",
    "                            seed = seed,\n",
    "                            shape = [size],\n",
    "                            dtype = tf.float32\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            with tf.name_scope('net_input'):\n",
    "                self.net_input = tf.matmul(self.input_, self.weights, a_is_sparse=sparse_inp) + self.biases\n",
    "\n",
    "            with tf.name_scope('activations'):\n",
    "                self.nonlin = nonlin\n",
    "                if nonlin:\n",
    "                    self.output = nonlin(self.net_input)\n",
    "                else:\n",
    "                    self.output = self.net_input\n",
    "    \n",
    "    def add_gradient_ops(self, loss):\n",
    "        with tf.name_scope(self.name):\n",
    "            item_keys = ['net_input', 'activation', 'weights']\n",
    "            items = [self.net_input, self.output, self.weights]\n",
    "            if self.biases: \n",
    "                item_keys.append('biases')\n",
    "                items.append(self.biases)\n",
    "            grad_list = tf.gradients(loss, items)\n",
    "            grad_list_with_keys = [val for pair in zip(item_keys, grad_list) for val in pair]\n",
    "            self.gradient = {k:v for k,v in zip(*[iter(grad_list_with_keys)]*2)}\n",
    "            \n",
    "            for grad_op, str_key in zip(grad_list, item_keys):\n",
    "                self.__dict__['g{}'.format(str_key)] = grad_op\n",
    "    \n",
    "    def fetch_test_ops(self):\n",
    "        fetch_items = ['weights', 'biases', 'net_input', 'activation',\n",
    "                       'gweights', 'gbiases', 'gnet_input', 'gactivation']\n",
    "        fetch_ops = {}\n",
    "        for fi in fetch_items:\n",
    "            if fi in self.__dict__.keys():\n",
    "                fetch_ops[fi] = self.__dict__[fi]\n",
    "        return fetch_ops, self.name\n",
    "\n",
    "\n",
    "class FFBPModel(object):\n",
    "    def __init__(self, name, loss, optimizer, layers, inp, targ, train_data=None, test_data=None):\n",
    "        self.name = name\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self._train_step = self.optimizer.minimize(loss=self.loss, global_step=self._global_step)        \n",
    "        \n",
    "        self.layers = layers\n",
    "        for layer in self.layers: \n",
    "            layer.add_gradient_ops(loss=self.loss)\n",
    "\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.inp_labels = tf.placeholder(shape=(), dtype=tf.string)\n",
    "\n",
    "        self.data = {'Test': test_data, 'Train': train_data}\n",
    "        \n",
    "        if test_data:\n",
    "            self.test_data = self.data['Test'] = test_data\n",
    "            self._test_fetches = {\n",
    "                'loss'  : self.loss,\n",
    "                'enum'  : self._global_step,\n",
    "                'labels': self.inp_labels,\n",
    "                'input' : self.inp,\n",
    "                'target': self.targ\n",
    "            }\n",
    "            self._prev_param = {}\n",
    "        if train_data:  \n",
    "            self.data['Train'] = train_data\n",
    "            self._train_fetches = {'loss':self.loss, '_train_step': self._train_step}\n",
    "        \n",
    "    def test_epoch(self, session, verbose=False):\n",
    "        assert self.test_data is not None, 'Provide test data to run a test epoch'\n",
    "        \n",
    "        data = self.data['Test']\n",
    "        snap = {}\n",
    "        with tf.name_scope('Test'):\n",
    "            ps, xs, ts = session.run(data.example_batches)\n",
    "            loss_sum = 0\n",
    "            for p, x, t in zip(ps, xs, ts):\n",
    "                x = np.expand_dims(x,0)\n",
    "                t = np.expand_dims(t,0)\n",
    "                test_out = sess.run(\n",
    "                    fetches = self._test_fetches, \n",
    "                    feed_dict = {self.inp_labels: p, self.inp: x, self.targ: t}\n",
    "                )\n",
    "                \n",
    "                for k, v in test_out.items():\n",
    "                    if k=='enum':\n",
    "                        snap[k] = v\n",
    "                    elif k not in snap.keys(): \n",
    "                        snap[k] = np.expand_dims(\n",
    "                                v,\n",
    "                                axis = 0\n",
    "                            )\n",
    "                    else:\n",
    "                        snap[k] = np.concatenate(\n",
    "                                [snap[k], np.expand_dims(v, axis=0)],\n",
    "                                axis=0\n",
    "                            )\n",
    "                \n",
    "                for layer in self.layers:\n",
    "                    layer_fetches, layer_name = layer.fetch_test_ops()\n",
    "                    snap.setdefault(layer_name, {})\n",
    "                    layer_out = sess.run(\n",
    "                        fetches = layer_fetches, \n",
    "                        feed_dict = {self.inp_labels: p, self.inp: x, self.targ: t}\n",
    "                    )\n",
    "                    for k, v in layer_out.items():\n",
    "                        if k=='weights' or k=='biases':\n",
    "                            snap[layer_name][k] = v\n",
    "                            if snap['enum'] == 0:\n",
    "                                self._prev_param[k] = v\n",
    "                                snap[layer_name]['d{}'.format(k)] = v*0\n",
    "                            else:\n",
    "                                snap[layer_name]['d{}'.format(k)] =  v - self._prev_param[k]\n",
    "                        elif k not in snap[layer_name].keys():\n",
    "                            snap[layer_name][k] = np.expand_dims(\n",
    "                                v,\n",
    "                                axis = 0\n",
    "                            )\n",
    "                        else:\n",
    "                            snap[layer_name][k] = np.concatenate(\n",
    "                                [snap[layer_name][k], np.expand_dims(v, axis=0)],\n",
    "                                axis=0\n",
    "                            )\n",
    "                loss_sum += test_out['loss']\n",
    "\n",
    "            if verbose: \n",
    "                print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), loss_sum))\n",
    "#                 for k, v in snap.items():\n",
    "#                     if type(v) is dict:\n",
    "#                         print(k)\n",
    "#                         for kk, vv in v.items():\n",
    "#                             print('{}:\\n{}'.format(kk, vv))\n",
    "#                     else: \n",
    "#                         print('{}:\\n{}'.format(k, v))\n",
    "                        \n",
    "            \n",
    "            return loss_sum, snap\n",
    "\n",
    "    def train_epoch(self, session, verbose=False):\n",
    "        assert self.test_data is not None, 'Provide train data to run a train epoch'\n",
    "        data = self.data['Train']\n",
    "                                          \n",
    "        with tf.name_scope('Train'):\n",
    "            for mini_batch in range(data.data_len // data.batch_size):\n",
    "                p, x, t = session.run(data.example_batches)\n",
    "                evaled_ops = session.run(\n",
    "                    fetches = self._train_fetches,\n",
    "                    feed_dict = {self.inp: x, self.targ: t}\n",
    "                )\n",
    "            batch_loss = evaled_ops['loss']\n",
    "\n",
    "        if verbose:\n",
    "            print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), batch_loss))\n",
    "                                          \n",
    "        return batch_loss\n",
    "\n",
    "def use_exercise_params(use):\n",
    "    if use:\n",
    "        all_vars = tf.global_variables()\n",
    "        hidden_W = [v for v in all_vars if 'hidden_layer/weights' in v.name][0]\n",
    "        hidden_b = [v for v in all_vars if 'hidden_layer/biases' in v.name][0]\n",
    "        output_W = [v for v in all_vars if 'output_layer/weights' in v.name][0]\n",
    "        output_b = [v for v in all_vars if 'output_layer/biases' in v.name][0]\n",
    "        restore_dict = {'w_1': hidden_W,'b_1': hidden_b,'w_2': output_W,'b_2': output_b}\n",
    "        tf.train.Saver(restore_dict).restore(tf.get_default_session(), 'exercise_params/exercise_params')\n",
    "\n",
    "        \n",
    "def new_logdir():\n",
    "    i=0\n",
    "    logdir = os.getcwd() + '/train/ffbp_logir_000'\n",
    "    while os.path.exists(logdir):\n",
    "        i+=.001\n",
    "        logdir = os.getcwd() + '/train/ffbp_logir_{}'.format(str(i)[2:5])\n",
    "    os.makedirs(logdir)\n",
    "    print('logdir path: {}'.format(logdir))\n",
    "    return logdir\n",
    "    \n",
    "def save(sess, logdir, model):\n",
    "    save_to = '/'.join([logdir,'checkpoint_files',model.name+'.ckpt'])\n",
    "    save_path = tf.train.Saver.save(sess, save_to)\n",
    "    print(\"Model saved in: {}\".format(save_path))\n",
    "    \n",
    "\n",
    "def load(self, path):\n",
    "    tf.reset_default_graph()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "num_epochs = 330\n",
    "batch_size = 2\n",
    "inp_size = 2\n",
    "targ_size = 1\n",
    "data_len = 4\n",
    "\n",
    "hidden_size = 2\n",
    "wrange = [-1,1]\n",
    "seed = None \n",
    "lr = 0.5\n",
    "m = 0.9\n",
    "ckpt_freq = 1\n",
    "ecrit = 0.01\n",
    "\n",
    "# configs = {\n",
    "#     'batch_size': 4,\n",
    "#     'lrate': 0.5,\n",
    "#     'momentum': 0.9,\n",
    "#     'ecrit': 0.01,\n",
    "# }\n",
    "\n",
    "with tf.name_scope('train_data'):\n",
    "    train_examples = InputData(\n",
    "        path_to_data_file = 'train_data_B.txt',\n",
    "        num_epochs = num_epochs,\n",
    "        batch_size = batch_size, \n",
    "        inp_size = 2, \n",
    "        targ_size = 1,\n",
    "        data_len = data_len,\n",
    "        shuffle = True, \n",
    "        shuffle_seed = 1\n",
    "    )\n",
    "\n",
    "with tf.name_scope('test_data'):\n",
    "    test_examples = InputData(\n",
    "        path_to_data_file = 'train_data_B.txt',\n",
    "        num_epochs = num_epochs,\n",
    "        batch_size = data_len,\n",
    "        inp_size = 2, \n",
    "        targ_size = 1,\n",
    "        data_len = data_len,\n",
    "        shuffle = False\n",
    "    )\n",
    "\n",
    "# NETWORK CONSTRUCTION\n",
    "model_name = 'XOR_model'\n",
    "with tf.name_scope(model_name):\n",
    "    \n",
    "    model_inp  = tf.placeholder(dtype = tf.float32, shape=[None, inp_size], name='model_inp')\n",
    "    \n",
    "    hidden_layer = BasicLayer(\n",
    "        layer_name = 'hidden_layer', \n",
    "        layer_input = model_inp, \n",
    "        size = hidden_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, # Use None for random seed value\n",
    "        sparse_inp=False\n",
    "    )\n",
    "    \n",
    "    output_layer = BasicLayer(\n",
    "        layer_name = 'output_layer', \n",
    "        layer_input = hidden_layer.output, \n",
    "        size = targ_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, # Use None for random seed value\n",
    "        sparse_inp=False\n",
    "    )\n",
    "\n",
    "    target = tf.placeholder(dtype = tf.float32, shape=[None, targ_size], name='targets')\n",
    "    \n",
    "    xor_model = FFBPModel(\n",
    "        name = model_name,\n",
    "        layers = [hidden_layer, output_layer],\n",
    "        train_data = train_examples, \n",
    "        inp        = model_inp,\n",
    "        targ       = target,\n",
    "        loss       = tf.reduce_sum(tf.squared_difference(target, output_layer.output), name='loss_function'),\n",
    "        optimizer  = tf.train.MomentumOptimizer(lr, m),\n",
    "        test_data  = test_examples\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logdir path: /Users/alexten/Projects/pdpyflow/xor/train/ffbp_logir_000\n",
      "INFO:tensorflow:Restoring parameters from exercise_params/exercise_params\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 2: 0.9999788403511047\n",
      "epoch 6: 1.0006492733955383\n",
      "epoch 10: 1.0000060349702835\n",
      "epoch 60: 1.009777083992958\n",
      "epoch 120: 1.0018968880176544\n",
      "epoch 336: 0.021881087915971875\n",
      "Stopped training due to loss < ecrit\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9274d3b37d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stopped training due to loss < ecrit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxor_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-78f81c0a789a>\u001b[0m in \u001b[0;36msave\u001b[0;34m(sess, logdir, model)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0msave_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'checkpoint_files'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.ckpt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved in: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logir' is not defined"
     ]
    }
   ],
   "source": [
    "test_epochs = [0,1,3,5,30,60,180,300]\n",
    "logdir = new_logdir()\n",
    "\n",
    "with tf.Session().as_default() as sess:\n",
    "    # initialize variables\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    use_exercise_params(True) # input False to use custom params\n",
    "    \n",
    "    # create coordinator and queue runners\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    \n",
    "    for i in range(num_epochs+1):\n",
    "        # Periodically test before each train epoch, or when num_epochs is reached\n",
    "        if any([i==test_epoch for test_epoch in test_epochs]):\n",
    "            loss, snap = xor_model.test_epoch(session=sess, verbose=True)\n",
    "            snap2pickle(logdir, snap)\n",
    "            \n",
    "        # Run train epoch\n",
    "        loss = xor_model.train_epoch(session=sess, verbose=False)\n",
    "        \n",
    "        # Test and break if loss < ecrit\n",
    "        if loss < ecrit: \n",
    "            loss, snap = xor_model.test_epoch(session=sess, verbose=True)\n",
    "            snap2pickle(logdir, snap)\n",
    "            \n",
    "            coordinator.request_stop()\n",
    "            coordinator.join(threads)\n",
    "            \n",
    "            print('Stopped training due to loss < ecrit')\n",
    "            break\n",
    "    save(sess, logdir, xor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
