{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR problem\n",
    "\n",
    "## 1. Preliminaries\n",
    "### 1.1. Imports\n",
    "We begin by importing several python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.3.0\n",
      "numpy version: 1.12.1\n",
      "current working directory: /Users/alexten/Projects/pdpyflow/xor\n",
      "logdir path: /Users/alexten/Projects/pdpyflow/xor/train/np_log_000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "i=0\n",
    "logdir = os.getcwd() + '/train/np_log_000'\n",
    "while os.path.exists(logdir):\n",
    "    i+=.001\n",
    "    logdir = os.getcwd() + '/train/np_log_{}'.format(str(i)[2:5])\n",
    "os.makedirs(logdir)\n",
    "    \n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('current working directory: {}'.format(os.getcwd()))\n",
    "print('logdir path: {}'.format(logdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap2pickle(logdir, snap):\n",
    "    path = '/'.join([logdir,'snap.pkl'])\n",
    "    try:\n",
    "        with open(path, 'rb') as old_file:\n",
    "            old_snap = pickle.load(old_file)\n",
    "        with open(path, 'wb') as old_file:\n",
    "            old_snap.append(snap)\n",
    "            pickle.dump(old_snap, old_file)\n",
    "    except FileNotFoundError:\n",
    "        with open(path, 'wb') as new_file:\n",
    "            out = pickle.dump([snap], new_file)\n",
    "\n",
    "\n",
    "class InputData(object):\n",
    "    def __init__(self, path_to_data_file, batch_size, inp_size, targ_size, data_len,\n",
    "                 shuffle = False, buffer = None, batch_seed = None):\n",
    "        # Store useful params\n",
    "        self.path = path_to_data_file\n",
    "        self.batch_size = batch_size\n",
    "        self.inp_size = inp_size\n",
    "        self.targ_size = targ_size\n",
    "        self.data_len = data_len\n",
    "\n",
    "        # setup filename queue\n",
    "        filename_queue = tf.train.string_input_producer(string_tensor = [path_to_data_file], shuffle = False)\n",
    "\n",
    "        # create reader and setup defaul values to read from files in the filename queue\n",
    "        reader = tf.TextLineReader(skip_header_lines=True, name='csv_reader')\n",
    "        key, record_string = reader.read(filename_queue)\n",
    "        defaults = [[0.0] for x in range(inp_size + targ_size)]\n",
    "        defaults.insert(0,[''])\n",
    "        examples = tf.decode_csv(record_string, record_defaults=defaults)\n",
    "\n",
    "        # read and decode examples into tensors\n",
    "        pattern_labels = tf.transpose(examples.pop(0))\n",
    "        input_patterns = tf.transpose(tf.stack(examples[0:inp_size]))\n",
    "        target_patterns = tf.transpose(tf.stack(examples[inp_size:inp_size + targ_size]))\n",
    "\n",
    "        # setup a batch queue for tensor examples\n",
    "        if shuffle:\n",
    "            assert type(buffer) is int, 'provide buffer size if you want to shuffle batch'\n",
    "            min_after_dequeue = buffer\n",
    "            self.example_batches = tf.train.shuffle_batch(\n",
    "                tensors = [pattern_labels, input_patterns, target_patterns], \n",
    "                batch_size = batch_size, \n",
    "                capacity = min_after_dequeue + 3 * batch_size,\n",
    "                min_after_dequeue = min_after_dequeue,\n",
    "                seed = batch_seed,\n",
    "            )\n",
    "        else:\n",
    "            self.example_batches = tf.train.batch(\n",
    "                tensors = [pattern_labels, input_patterns, target_patterns],\n",
    "                batch_size = batch_size,\n",
    "                num_threads = 1,\n",
    "                capacity = 3 * batch_size,\n",
    "            )\n",
    "\n",
    "\n",
    "class BasicLayer(object):\n",
    "    def __init__(self, layer_name, layer_input, size, wrange, nonlin=None, bias=True, seed=None, sparse_inp=False):\n",
    "        self.name = layer_name\n",
    "        with tf.name_scope(layer_name):\n",
    "            self.input_ = layer_input\n",
    "            if type(layer_input) != tf.Tensor and hasattr(layer_name, '__iter__'):\n",
    "                self.input_ = tf.concat(axis=1, values=[i for i in layer_input])\n",
    "            input_size = layer_input._shape[1]._value\n",
    "            with tf.name_scope('weights'):\n",
    "\n",
    "                self.weights = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        minval = wrange[0], \n",
    "                        maxval = wrange[1],\n",
    "                        seed = seed,\n",
    "                        shape = [input_size, size],\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            self.biases = 0\n",
    "            if bias:\n",
    "                with tf.name_scope('biases'):\n",
    "                    self.biases = tf.Variable(\n",
    "                        tf.random_uniform(\n",
    "                            minval = wrange[0],\n",
    "                            maxval = wrange[1],\n",
    "                            seed = seed,\n",
    "                            shape = [size],\n",
    "                            dtype = tf.float32\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            with tf.name_scope('net_input'):\n",
    "                self.net_input = tf.matmul(self.input_, self.weights, a_is_sparse=sparse_inp) + self.biases\n",
    "\n",
    "            with tf.name_scope('activations'):\n",
    "                self.nonlin = nonlin\n",
    "                if nonlin:\n",
    "                    self.output = nonlin(self.net_input)\n",
    "                else:\n",
    "                    self.output = self.net_input\n",
    "    \n",
    "    def add_gradient_ops(self, loss):\n",
    "        with tf.name_scope(self.name):\n",
    "            item_keys = ['net_input', 'activation', 'weights']\n",
    "            items = [self.net_input, self.output, self.weights]\n",
    "            if self.biases: \n",
    "                item_keys.append('biases')\n",
    "                items.append(self.biases)\n",
    "            grad_list = tf.gradients(loss, items)\n",
    "            grad_list_with_keys = [val for pair in zip(item_keys, grad_list) for val in pair]\n",
    "            self.gradient = {k:v for k,v in zip(*[iter(grad_list_with_keys)]*2)}\n",
    "            \n",
    "            for grad_op, str_key in zip(grad_list, item_keys):\n",
    "                self.__dict__['g{}'.format(str_key)] = grad_op\n",
    "    \n",
    "    def fetch_test_ops(self):\n",
    "        fetch_items = ['weights', 'biases', 'net_input', 'activation',\n",
    "                       'gweights', 'gbiases', 'gnet_input', 'gactivation']\n",
    "        fetch_ops = {}\n",
    "        for fi in fetch_items:\n",
    "            if fi in self.__dict__.keys():\n",
    "                fetch_ops[fi] = self.__dict__[fi]\n",
    "        return fetch_ops, self.name\n",
    "\n",
    "\n",
    "class FFBPModel(object):\n",
    "    def __init__(self, loss, optimizer, layers, inp, targ, train_data=None, test_data=None):\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self._train_step = self.optimizer.minimize(loss=self.loss, global_step=self._global_step)        \n",
    "        \n",
    "        self.layers = layers\n",
    "        for layer in self.layers: \n",
    "            layer.add_gradient_ops(loss=self.loss)\n",
    "\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.inp_labels = tf.placeholder(shape=(), dtype=tf.string)\n",
    "\n",
    "        self.data = {'Test': test_data, 'Train': train_data}\n",
    "        \n",
    "        if test_data:\n",
    "            self.test_data = self.data['Test'] = test_data\n",
    "            self._test_fetches = {\n",
    "                'loss'  : self.loss,\n",
    "                'enum'  : self._global_step,\n",
    "                'labels': self.inp_labels,\n",
    "                'input' : self.inp,\n",
    "                'target': self.targ\n",
    "            }\n",
    "            self._prev_param = {}\n",
    "        if train_data:  \n",
    "            train_loss_summary = tf.summary.scalar('train_loss_summary', self.loss)\n",
    "            self.data['Train'] = train_data\n",
    "            self._train_fetches = {'loss':self.loss, 'summaries': tf.summary.merge([train_loss_summary]), '_train_step': self._train_step}\n",
    "\n",
    "    def test_epoch(self, session, verbose=False):\n",
    "        assert self.test_data is not None, 'Provide test data to run a test epoch'\n",
    "        \n",
    "        data = self.data['Test']\n",
    "        snap = {}\n",
    "        with tf.name_scope('Test'):\n",
    "            ps, xs, ts = session.run(data.example_batches)\n",
    "            loss_sum = 0\n",
    "            for p, x, t in zip(ps, xs, ts):\n",
    "                x = np.expand_dims(x,0)\n",
    "                t = np.expand_dims(t,0)\n",
    "                test_out = sess.run(\n",
    "                    fetches = self._test_fetches, \n",
    "                    feed_dict = {self.inp_labels: p, self.inp: x, self.targ: t}\n",
    "                )\n",
    "                \n",
    "                for k, v in test_out.items():\n",
    "                    if k=='enum':\n",
    "                        snap[k] = v\n",
    "                    elif k not in snap.keys(): \n",
    "                        snap[k] = np.expand_dims(\n",
    "                                v,\n",
    "                                axis = 0\n",
    "                            )\n",
    "                    else:\n",
    "                        snap[k] = np.concatenate(\n",
    "                                [snap[k], np.expand_dims(v, axis=0)],\n",
    "                                axis=0\n",
    "                            )\n",
    "                \n",
    "                for layer in self.layers:\n",
    "                    layer_fetches, layer_name = layer.fetch_test_ops()\n",
    "                    snap.setdefault(layer_name, {})\n",
    "                    layer_out = sess.run(\n",
    "                        fetches = layer_fetches, \n",
    "                        feed_dict = {self.inp_labels: p, self.inp: x, self.targ: t}\n",
    "                    )\n",
    "                    for k, v in layer_out.items():\n",
    "                        if k=='weights' or k=='biases':\n",
    "                            snap[layer_name][k] = v\n",
    "                            if snap['enum'] == 0:\n",
    "                                self._prev_param[k] = v\n",
    "                            else:\n",
    "                                snap[layer_name]['d{}'.format(k)] =  v - self._prev_param[k]\n",
    "                        elif k not in snap[layer_name].keys():\n",
    "                            snap[layer_name][k] = np.expand_dims(\n",
    "                                v,\n",
    "                                axis = 0\n",
    "                            )\n",
    "                        else:\n",
    "                            snap[layer_name][k] = np.concatenate(\n",
    "                                [snap[layer_name][k], np.expand_dims(v, axis=0)],\n",
    "                                axis=0\n",
    "                            )\n",
    "                loss_sum += test_out['loss']\n",
    "\n",
    "            if verbose: \n",
    "                print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), loss_sum))\n",
    "#                 for k, v in snap.items():\n",
    "#                     if type(v) is dict:\n",
    "#                         print(k)\n",
    "#                         for kk, vv in v.items():\n",
    "#                             print('{}:\\n{}'.format(kk, vv))\n",
    "#                     else: \n",
    "#                         print('{}:\\n{}'.format(k, v))\n",
    "                        \n",
    "            \n",
    "            return loss_sum, snap\n",
    "\n",
    "    def train_epoch(self, session, verbose=False):\n",
    "        assert self.test_data is not None, 'Provide train data to run a train epoch'\n",
    "        data = self.data['Train']\n",
    "                                          \n",
    "        with tf.name_scope('Train'):\n",
    "            for mini_batch in range(data.data_len // data.batch_size):\n",
    "                p, x, t = session.run([t for t in data.example_batches])\n",
    "                evaled_ops = sess.run(\n",
    "                    fetches = self._train_fetches,\n",
    "                    feed_dict = {self.inp: x, self.targ: t}\n",
    "                )\n",
    "            batch_loss, summary = evaled_ops['loss'], evaled_ops['summaries']\n",
    "\n",
    "        if verbose:\n",
    "            print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), loss_sum))\n",
    "                                          \n",
    "        return batch_loss, summary\n",
    "    \n",
    "    def save(self, path):\n",
    "        pass\n",
    "    \n",
    "    def load(self, path):\n",
    "        pass\n",
    "\n",
    "\n",
    "def use_exercise_params(use):\n",
    "    if use:\n",
    "        all_vars = tf.global_variables()\n",
    "        hidden_W = [v for v in all_vars if 'hidden_layer/weights' in v.name][0]\n",
    "        hidden_b = [v for v in all_vars if 'hidden_layer/biases' in v.name][0]\n",
    "        output_W = [v for v in all_vars if 'output_layer/weights' in v.name][0]\n",
    "        output_b = [v for v in all_vars if 'output_layer/biases' in v.name][0]\n",
    "        restore_dict = {'w_1': hidden_W,'b_1': hidden_b,'w_2': output_W,'b_2': output_b}\n",
    "        tf.train.Saver(restore_dict).restore(tf.get_default_session(), 'exercise_params/exercise_params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "num_epochs = 330\n",
    "batch_size = 4\n",
    "inp_size = 2\n",
    "targ_size = 1\n",
    "data_len = 4\n",
    "\n",
    "hidden_size = 2\n",
    "wrange = [-1,1]\n",
    "seed = None \n",
    "lr = 0.5\n",
    "m = 0.9\n",
    "ckpt_freq = 1\n",
    "ecrit = 0.01\n",
    "\n",
    "configs = {\n",
    "    'batch_size': 4,\n",
    "    'lrate': 0.5,\n",
    "    'momentum': 0.9,\n",
    "    'ecrit': 0.01,\n",
    "}\n",
    "\n",
    "with tf.name_scope('train_data'):\n",
    "    train_examples = InputData(\n",
    "        path_to_data_file = 'train_data_B.txt', \n",
    "        batch_size = batch_size, \n",
    "        inp_size = 2, \n",
    "        targ_size = 1,\n",
    "        data_len = data_len,\n",
    "        shuffle = False, \n",
    "        buffer = None,\n",
    "        batch_seed = None\n",
    "    )\n",
    "\n",
    "with tf.name_scope('test_data'):\n",
    "    test_examples = InputData(\n",
    "        path_to_data_file = 'train_data_B.txt', \n",
    "        batch_size = data_len, \n",
    "        inp_size = 2, \n",
    "        targ_size = 1,\n",
    "        data_len = data_len,\n",
    "        shuffle = False, \n",
    "        buffer = None,\n",
    "        batch_seed = None\n",
    "    )\n",
    "\n",
    "# NETWORK CONSTRUCTION\n",
    "with tf.name_scope('XOR_model'):\n",
    "    \n",
    "    model_inp  = tf.placeholder(dtype = tf.float32, shape=[None, inp_size], name='model_inp')\n",
    "    \n",
    "    hidden_layer = BasicLayer(\n",
    "        layer_name = 'hidden_layer', \n",
    "        layer_input = model_inp, \n",
    "        size = hidden_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, # Use None for random seed value\n",
    "        sparse_inp=False\n",
    "    )\n",
    "    \n",
    "    output_layer = BasicLayer(\n",
    "        layer_name = 'output_layer', \n",
    "        layer_input = hidden_layer.output, \n",
    "        size = targ_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, # Use None for random seed value\n",
    "        sparse_inp=False\n",
    "    )\n",
    "\n",
    "    target = tf.placeholder(dtype = tf.float32, shape=[None, targ_size], name='targets')\n",
    "    \n",
    "    xor_model = FFBPModel(\n",
    "        layers = [hidden_layer, output_layer],\n",
    "        train_data = train_examples, \n",
    "        inp        = model_inp,\n",
    "        targ       = target,\n",
    "        loss       = tf.reduce_sum(tf.squared_difference(target, output_layer.output), name='loss_function'),\n",
    "        optimizer  = tf.train.MomentumOptimizer(lr, m),\n",
    "        test_data  = test_examples\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from exercise_params/exercise_params\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 30: 1.0019647181034088\n",
      "epoch 60: 1.0000269263982773\n",
      "epoch 180: 0.9998661577701569\n",
      "epoch 300: 0.04396426538005471\n",
      "epoch 318: 0.009249473921954632\n",
      "Stopped training due to loss < ecrit\n"
     ]
    }
   ],
   "source": [
    "sum_freq = 30 # (num_epochs // 10)\n",
    "test_epochs = [0,30,60]+[180,300]\n",
    "\n",
    "with tf.Session().as_default() as sess:\n",
    "    summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    use_exercise_params(True) # input False to use custom params\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    \n",
    "    for i in range(num_epochs+1):\n",
    "        # Periodically test before each train epoch, or when num_epochs is reached\n",
    "        if any([i==test_epoch for test_epoch in test_epochs]):\n",
    "            loss, snap = xor_model.test_epoch(session=sess, verbose=True)\n",
    "            snap2pickle(logdir, snap)\n",
    "            \n",
    "        # Run train epoch\n",
    "        loss, summary = xor_model.train_epoch(session=sess, verbose=False)\n",
    "        \n",
    "        # Test and break if loss < ecrit\n",
    "        if loss < ecrit: \n",
    "            loss, snap = xor_model.test_epoch(session=sess, verbose=True)\n",
    "            snap2pickle(logdir, snap)\n",
    "            \n",
    "            coordinator.request_stop()\n",
    "            coordinator.join(threads)\n",
    "            \n",
    "            print('Stopped training due to loss < ecrit')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # might be needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
