{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR problem\n",
    "\n",
    "## 1. Preliminaries\n",
    "### 1.1. Imports\n",
    "We begin by importing several python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# rm -rf train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.3.0\n",
      "numpy version: 1.12.1\n",
      "current working directory: /Users/alexten/Projects/pdpyflow/xor\n",
      "tensorboard logdir path: /Users/alexten/Projects/pdpyflow/xor/train/log_003\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "i=0\n",
    "logdir = os.getcwd() + '/train/log_000'\n",
    "while os.path.exists(logdir):\n",
    "    i+=.001\n",
    "    logdir = os.getcwd() + '/train/log_{}'.format(str(i)[2:5])\n",
    "os.makedirs(logdir)\n",
    "    \n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('current working directory: {}'.format(os.getcwd()))\n",
    "print('tensorboard logdir path: {}'.format(logdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv_file(filename_queue, batch_size, default_val, inp_size, targ_size, pattern_labels):\n",
    "    reader = tf.TextLineReader(skip_header_lines=True, name='csv_reader')\n",
    "    _, csv_row = reader.read_up_to(filename_queue, batch_size)\n",
    "    defaults = [[default_val] for x in range(inp_size + targ_size)]\n",
    "    if pattern_labels is True: \n",
    "        defaults.insert(0,[''])\n",
    "    examples = tf.decode_csv(csv_row, record_defaults=defaults)\n",
    "    p = tf.transpose(examples.pop(0))\n",
    "#     tf.summary.text('pattern_labels', p)\n",
    "    x = tf.transpose(tf.stack(examples[0:inp_size]))\n",
    "    t = tf.transpose(tf.stack(examples[inp_size:inp_size + targ_size]))\n",
    "    return p, x, t\n",
    "\n",
    "\n",
    "def use_exercise_params(session=None):\n",
    "    if session:\n",
    "        all_vars = tf.global_variables()\n",
    "        hidden_W = [v for v in all_vars if 'hidden_layer/weights' in v.name][0]\n",
    "        hidden_b = [v for v in all_vars if 'hidden_layer/biases' in v.name][0]\n",
    "        output_W = [v for v in all_vars if 'output_layer/weights' in v.name][0]\n",
    "        output_b = [v for v in all_vars if 'output_layer/biases' in v.name][0]\n",
    "        restore_dict = {'w_1': hidden_W,'b_1': hidden_b,'w_2': output_W,'b_2': output_b}\n",
    "        tf.train.Saver(restore_dict).restore(session, 'exercise_params/exercise_params')\n",
    "\n",
    "def make_layer(layer_name, layer_input, size, wrange, nonlin=None, bias=True, seed=None, sparse_inp=False):\n",
    "    with tf.name_scope(layer_name):\n",
    "        if type(layer_input) != tf.Tensor and hasattr(layer_name, '__iter__'):\n",
    "            layer_input = tf.concat(axis=1, values=[i for i in layer_input])\n",
    "        input_size = layer_input._shape[1]._value\n",
    "        with tf.name_scope('weights'):\n",
    "            \n",
    "            weights = tf.Variable(\n",
    "                tf.random_uniform(\n",
    "                    minval = wrange[0], \n",
    "                    maxval = wrange[1],\n",
    "                    seed = seed,\n",
    "                    shape = [input_size, size],\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "            )\n",
    "            tf.summary.tensor_summary('params_summary', weights)\n",
    "        \n",
    "        biases = 0\n",
    "        if bias:\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        minval = wrange[0],\n",
    "                        maxval = wrange[1],\n",
    "                        seed = seed,\n",
    "                        shape = [1, size],\n",
    "                        dtype = tf.float32\n",
    "                    )\n",
    "                )\n",
    "                tf.summary.tensor_summary('params_summary', biases)\n",
    "\n",
    "        with tf.name_scope('net_input'):\n",
    "            net_input = tf.matmul(layer_input, weights, a_is_sparse=sparse_inp) + biases\n",
    "            tf.summary.tensor_summary('data_summary', net_input)\n",
    "        \n",
    "        with tf.name_scope('activations'):\n",
    "            if nonlin:\n",
    "                layer_output = nonlin(net_input)\n",
    "            else:\n",
    "                layer_output = net_input\n",
    "            tf.summary.tensor_summary('data_summary', layer_output)\n",
    "    print(weights)\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "def run_epoch(model, train=True):\n",
    "    name_scope = 'test'\n",
    "    if train:\n",
    "        name_scope = 'train'\n",
    "    with tf.name_scope(name_scope):\n",
    "        target = tf.placeholder(dtype = tf.float32, shape=[batch_size, targ_size], name='model_inp')\n",
    "        squared_error = tf.reduce_sum(tf.squared_difference(target, output_acts),\n",
    "                                    name='squared_error')\n",
    "        tf.summary.scalar('error_summary', squared_error)\n",
    "        train_step = tf.train.MomentumOptimizer(lr, m).minimize(squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Training environment and input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "num_epochs = 330\n",
    "batch_size = 4\n",
    "inp_size = 2\n",
    "targ_size = 1\n",
    "\n",
    "# QUEUES\n",
    "with tf.name_scope('Input_pipeline'):\n",
    "    input_queue = tf.train.string_input_producer(\n",
    "                    ['train_data_B.txt'], \n",
    "                    num_epochs = num_epochs, \n",
    "                    shuffle = False\n",
    "    )\n",
    "\n",
    "    pattern, inp_batch, targ_batch = read_csv_file(\n",
    "                                        filename_queue = input_queue,\n",
    "                                        batch_size = batch_size,\n",
    "                                        default_val = 0.0,\n",
    "                                        inp_size = inp_size,\n",
    "                                        targ_size = targ_size,\n",
    "                                        pattern_labels = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'XOR_model/hidden_layer/weights/Variable:0' shape=(2, 2) dtype=float32_ref>\n",
      "<tf.Variable 'XOR_model/output_layer/weights/Variable:0' shape=(2, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# CONFIGS\n",
    "hidden_size = 2\n",
    "wrange = [-1,1]\n",
    "seed = None # Use None for random seed value\n",
    "lr = 0.5\n",
    "m = 0.9\n",
    "# use_exercise_params = True\n",
    "ckpt_freq = 1\n",
    "ecrit = 0.01\n",
    "\n",
    "# NETWORK CONSTRUCTION\n",
    "with tf.name_scope('XOR_model'):\n",
    "    \n",
    "    model_inp  = tf.placeholder(dtype = tf.float32, shape=[batch_size, inp_size], name='model_inp')\n",
    "    pat_labels = tf.placeholder(dtype = tf.string, shape=[batch_size, ], name='pattern_labels')\n",
    "    tf.summary.tensor_summary('input_patterns/data_summary', model_inp)\n",
    "    tf.summary.tensor_summary('pattern_labels/data_summary', pat_labels)\n",
    "    \n",
    "    hidden_acts = make_layer(\n",
    "        layer_name = 'hidden_layer', \n",
    "        layer_input = model_inp, \n",
    "        size = hidden_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, \n",
    "        sparse_inp=False\n",
    "    )\n",
    "    \n",
    "    output_acts = make_layer(\n",
    "        layer_name = 'output_layer', \n",
    "        layer_input = hidden_acts, \n",
    "        size = targ_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, \n",
    "        sparse_inp=False\n",
    "    )\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        target = tf.placeholder(dtype = tf.float32, shape=[batch_size, targ_size], name='targets')\n",
    "        tf.summary.tensor_summary('target_patterns/data_summary', target)\n",
    "        squared_error = tf.reduce_sum(tf.squared_difference(target, output_acts),\n",
    "                                    name='squared_error')\n",
    "        tf.summary.scalar('error_summary', squared_error)\n",
    "        train_step = tf.train.MomentumOptimizer(lr, m).minimize(squared_error)\n",
    "\n",
    "merge_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from exercise_params/exercise_params\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 0: 1.0506523847579956\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 30: 1.001964807510376\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 60: 1.0000269412994385\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 90: 0.9999932050704956\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 120: 0.999982476234436\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 150: 0.9999575018882751\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 180: 0.9998661875724792\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 210: 0.9992285370826721\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 240: 0.9822090864181519\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 270: 0.6883606910705566\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "epoch 300: 0.043964266777038574\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "[b'p00' b'p01' b'p10' b'p11']\n",
      "stop epoch 317: 0.009680919349193573\n"
     ]
    }
   ],
   "source": [
    "sum_freq = 30 # (num_epochs // 10)\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    use_exercise_params(sess) # input None to use custom params, input current session to use exercise params\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    for i in range(num_epochs+1):\n",
    "#             try:\n",
    "                p, x, t = sess.run([pattern, inp_batch, targ_batch])\n",
    "                _, ppp , loss, summary = sess.run([pat_labels, train_step, squared_error, merge_summaries], \n",
    "                                            feed_dict={model_inp: x, target: t, pat_labels: p})\n",
    "                print(ppp)\n",
    "                if i % sum_freq == 0 or i == num_epochs - 1:\n",
    "                    summary_writer.add_summary(summary, i)\n",
    "                    print('epoch {}: {}'.format(i,loss))\n",
    "                if loss < ecrit:\n",
    "                    summary_writer.add_summary(summary, i)\n",
    "                    print('stop epoch {}: {}'.format(i,loss))\n",
    "                    summary_writer.close()\n",
    "                    break\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "#                 print('Reached the end of trainining set')\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # might be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ddd\n"
     ]
    }
   ],
   "source": [
    "bs = b'ddd'\n",
    "type(bs)\n",
    "\n",
    "ss = bs.decode(\"utf-8\")\n",
    "print(type(ss))\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
