{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR problem\n",
    "\n",
    "## 1. Preliminaries\n",
    "### 1.1. Imports\n",
    "We begin by importing several python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# rm -rf train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.3.0\n",
      "numpy version: 1.12.1\n",
      "current working directory: /Users/alexten/Projects/pdpyflow/xor\n",
      "logdir path: /Users/alexten/Projects/pdpyflow/xor/train/log_000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "i=0\n",
    "logdir = os.getcwd() + '/train/log_000'\n",
    "while os.path.exists(logdir):\n",
    "    i+=.001\n",
    "    logdir = os.getcwd() + '/train/log_{}'.format(str(i)[2:5])\n",
    "os.makedirs(logdir)\n",
    "    \n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('current working directory: {}'.format(os.getcwd()))\n",
    "print('logdir path: {}'.format(logdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv_file(filename_queue, batch_size, default_val, inp_size, targ_size, pattern_labels):\n",
    "    reader = tf.TextLineReader(skip_header_lines=True, name='csv_reader')\n",
    "    _, csv_row = reader.read_up_to(filename_queue, batch_size)\n",
    "    defaults = [[default_val] for x in range(inp_size + targ_size)]\n",
    "    if pattern_labels is True: \n",
    "        defaults.insert(0,[''])\n",
    "    examples = tf.decode_csv(csv_row, record_defaults=defaults)\n",
    "    p = tf.transpose(examples.pop(0))\n",
    "    x = tf.transpose(tf.stack(examples[0:inp_size]))\n",
    "    t = tf.transpose(tf.stack(examples[inp_size:inp_size+targ_size]))\n",
    "    return p, x, t\n",
    "\n",
    "\n",
    "def use_exercise_params(use):\n",
    "    if use:\n",
    "        all_vars = tf.global_variables()\n",
    "        hidden_W = [v for v in all_vars if 'hidden_layer/weights' in v.name][0]\n",
    "        hidden_b = [v for v in all_vars if 'hidden_layer/biases' in v.name][0]\n",
    "        output_W = [v for v in all_vars if 'output_layer/weights' in v.name][0]\n",
    "        output_b = [v for v in all_vars if 'output_layer/biases' in v.name][0]\n",
    "        restore_dict = {'w_1': hidden_W,'b_1': hidden_b,'w_2': output_W,'b_2': output_b}\n",
    "        tf.train.Saver(restore_dict).restore(tf.get_default_session(), 'exercise_params/exercise_params')\n",
    "\n",
    "\n",
    "class BasicLayer(object):\n",
    "    def __init__(self, layer_name, layer_input, size, wrange, nonlin=None, bias=True, seed=None, sparse_inp=False):\n",
    "        with tf.name_scope(layer_name):\n",
    "            self.input_ = layer_input\n",
    "            if type(layer_input) != tf.Tensor and hasattr(layer_name, '__iter__'):\n",
    "                self.input_ = tf.concat(axis=1, values=[i for i in layer_input])\n",
    "            input_size = layer_input._shape[1]._value\n",
    "            with tf.name_scope('weights'):\n",
    "\n",
    "                self.weights = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        minval = wrange[0], \n",
    "                        maxval = wrange[1],\n",
    "                        seed = seed,\n",
    "                        shape = [input_size, size],\n",
    "                        dtype=tf.float32\n",
    "                    )\n",
    "                )\n",
    "                tf.summary.tensor_summary('params_summary', self.weights)\n",
    "\n",
    "            self.biases = 0\n",
    "            if bias:\n",
    "                with tf.name_scope('biases'):\n",
    "                    self.biases = tf.Variable(\n",
    "                        tf.random_uniform(\n",
    "                            minval = wrange[0],\n",
    "                            maxval = wrange[1],\n",
    "                            seed = seed,\n",
    "                            shape = [1, size],\n",
    "                            dtype = tf.float32\n",
    "                        )\n",
    "                    )\n",
    "                    tf.summary.tensor_summary('params_summary', self.biases)\n",
    "\n",
    "            with tf.name_scope('net_input'):\n",
    "                self.net_input = tf.matmul(self.input_, self.weights, a_is_sparse=sparse_inp) + self.biases\n",
    "                tf.summary.tensor_summary('data_summary', self.net_input)\n",
    "\n",
    "            with tf.name_scope('activations'):\n",
    "                self.nonlin = nonlin\n",
    "                if nonlin:\n",
    "                    self.output = nonlin(self.net_input)\n",
    "                else:\n",
    "                    self.output = self.net_input\n",
    "                tf.summary.tensor_summary('data_summary', self.output)\n",
    "        \n",
    "class FFBP_Model(object):\n",
    "    def __init__(self, train_data, test_data, inp, targ, loss, optimizer):\n",
    "        self.data = {'Train': train_data, 'Test': test_data}\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.inp = inp,\n",
    "        self.targ = targ,\n",
    "        self.loss = loss; tf.summary.scalar('loss_summary', self.loss)\n",
    "        self.optimizer = optimizer\n",
    "        self._global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self._train_step = self.optimizer.minimize(loss=self.loss, global_step=self._global_step)\n",
    "    \n",
    "    def run_epoch(self, session, train=True, verbose=False):\n",
    "        fetches  = [self.loss, tf.summary.merge_all(), self._train_step]\n",
    "        mode, fetches = ('Train', fetches) if train else ('Test', fetches[0:-1])\n",
    "        with tf.name_scope(mode):\n",
    "            p, x, t = session.run(self.data[mode])\n",
    "            out = sess.run(\n",
    "                fetches = fetches, \n",
    "                feed_dict = {self.inp: x, self.targ: t}\n",
    "            )\n",
    "        loss, summary = out[0], out[1]\n",
    "        if verbose: \n",
    "            print('epoch {}: {}'.format(tf.train.global_step(session, self._global_step), loss))\n",
    "        return loss, summary\n",
    "    \n",
    "    def save(self, path):\n",
    "        pass\n",
    "    \n",
    "    def load(self, path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Setup training and testing environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "num_epochs = 330\n",
    "batch_size = 4\n",
    "inp_size = 2\n",
    "targ_size = 1\n",
    "\n",
    "# QUEUES\n",
    "with tf.name_scope('Train_input'):\n",
    "    train_input_queue = tf.train.string_input_producer(\n",
    "                    ['train_data_B.txt'], \n",
    "                    num_epochs = num_epochs, \n",
    "                    shuffle = False\n",
    "    )\n",
    "    \n",
    "    train_examples_batch = read_csv_file(\n",
    "        filename_queue = train_input_queue,\n",
    "        batch_size = batch_size,\n",
    "        default_val = 0.0,\n",
    "        inp_size = inp_size,\n",
    "        targ_size = targ_size,\n",
    "        pattern_labels = True\n",
    "    )\n",
    "\n",
    "with tf.name_scope('Test_input'):\n",
    "    test_input_queue = tf.train.string_input_producer(\n",
    "                    ['train_data_B.txt'], \n",
    "                    num_epochs = num_epochs, \n",
    "                    shuffle = False\n",
    "    )\n",
    "    \n",
    "    test_examples_batch = read_csv_file(\n",
    "        filename_queue = test_input_queue,\n",
    "        batch_size = batch_size,\n",
    "        default_val = 0.0,\n",
    "        inp_size = inp_size,\n",
    "        targ_size = targ_size,\n",
    "        pattern_labels = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "hidden_size = 2\n",
    "wrange = [-1,1]\n",
    "seed = None # Use None for random seed value\n",
    "lr = 0.5\n",
    "m = 0.9\n",
    "# use_exercise_params = True\n",
    "ckpt_freq = 1\n",
    "ecrit = 0.01\n",
    "\n",
    "# NETWORK CONSTRUCTION\n",
    "with tf.name_scope('XOR_model'):\n",
    "    \n",
    "    model_inp  = tf.placeholder(dtype = tf.float32, shape=[batch_size, inp_size], name='model_inp')\n",
    "#     pat_labels = tf.placeholder(dtype = tf.string, shape=[batch_size, ], name='pattern_labels')\n",
    "    tf.summary.tensor_summary('input_patterns/data_summary', model_inp)\n",
    "#     tf.summary.tensor_summary('pattern_labels/data_summary', pat_labels)\n",
    "    \n",
    "    hidden_layer = BasicLayer(\n",
    "        layer_name = 'hidden_layer', \n",
    "        layer_input = model_inp, \n",
    "        size = hidden_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, \n",
    "        sparse_inp=False\n",
    "    )\n",
    "    \n",
    "    output_layer = BasicLayer(\n",
    "        layer_name = 'output_layer', \n",
    "        layer_input = hidden_layer.output, \n",
    "        size = targ_size, \n",
    "        wrange = [-1,1], \n",
    "        nonlin=tf.nn.sigmoid, \n",
    "        bias=True, \n",
    "        seed=1, \n",
    "        sparse_inp=False\n",
    "    )\n",
    "\n",
    "    target = tf.placeholder(dtype = tf.float32, shape=[batch_size, targ_size], name='targets')\n",
    "    \n",
    "    xor_model = FFBP_Model(\n",
    "        train_data = train_examples_batch, \n",
    "        test_data  = test_examples_batch,\n",
    "        inp        = model_inp,\n",
    "        targ       = target,\n",
    "        loss       = tf.reduce_sum(tf.squared_difference(target, output_layer.output), name='loss_function'),\n",
    "        optimizer  = tf.train.MomentumOptimizer(lr, m)\n",
    "    )\n",
    "\n",
    "all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from exercise_params/exercise_params\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 30: 1.001964807510376\n",
      "epoch 60: 1.0000269412994385\n",
      "epoch 90: 0.9999932050704956\n",
      "epoch 120: 0.999982476234436\n",
      "epoch 150: 0.9999575018882751\n",
      "epoch 180: 0.9998661875724792\n",
      "epoch 210: 0.9992285370826721\n",
      "epoch 240: 0.9822090864181519\n",
      "epoch 270: 0.6883606910705566\n",
      "epoch 300: 0.043964266777038574\n",
      "epoch 318: 0.009249473921954632\n",
      "Stopped training due to loss < ecrit\n"
     ]
    }
   ],
   "source": [
    "sum_freq = 30 # (num_epochs // 10)\n",
    "\n",
    "with tf.Session().as_default() as sess:\n",
    "    summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    use_exercise_params(True) # input False to use custom params\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    for i in range(num_epochs+1):\n",
    "#             try:\n",
    "                if any([i % sum_freq == 0, i == num_epochs - 1]):\n",
    "                    loss, summary = xor_model.run_epoch(session=sess, train=False, verbose=True)\n",
    "                    summary_writer.add_summary(summary, i)\n",
    "                loss, summary = xor_model.run_epoch(session=sess, train=True, verbose=False)\n",
    "                if loss < ecrit: \n",
    "                    loss, summary = xor_model.run_epoch(session=sess, train=False, verbose=True)\n",
    "                    summary_writer.add_summary(summary, i)\n",
    "                    coordinator.request_stop()\n",
    "                    coordinator.join(threads)\n",
    "                    print('Stopped training due to loss < ecrit')\n",
    "                    break\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "#                 print('Reached the end of trainining set')\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # might be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "ddd\n"
     ]
    }
   ],
   "source": [
    "bs = b'ddd'\n",
    "type(bs)\n",
    "\n",
    "ss = bs.decode(\"utf-8\")\n",
    "print(type(ss))\n",
    "print(ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
