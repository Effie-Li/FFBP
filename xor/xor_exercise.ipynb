{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "xor_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLv14rVcKAV-"
      },
      "source": [
        "# XOR problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51JMqlLnKFWe"
      },
      "source": [
        "## Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuzv_4N7KHeC"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# make a simlink to save some clicks\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Stanford/Year2/TA/psych209/hw1/'\n",
        "SYM_PATH = '/content/hw1'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "    !ln -s $DRIVE_PATH $SYM_PATH\n",
        "\n",
        "sys.path.append(DRIVE_PATH+'xor/') # for FFBP\n",
        "print(sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bE5nQ3NKES7"
      },
      "source": [
        "\n",
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IAPXOPvDKAWG"
      },
      "source": [
        "import sys\n",
        "print(sys.version) # ensure that you're using python3\n",
        "import torch\n",
        "print(\"PyTorch version = {}\".format(torch.__version__))\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "import pickle\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GXIQnE6KAWH"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkGzz2Q1KAWH"
      },
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self):\n",
        "        self.train_data = {\"inputs\": [[0, 0], [0, 1], [1, 0], [1, 1]],\n",
        "                           \"targets\": [[0], [1], [1], [0]],\n",
        "                           \"names\": [\"p00\", \"p01\", \"p10\", \"p11\"]}\n",
        "        self.train_size = 4\n",
        "        self.test_data = self.train_data\n",
        "        \n",
        "    def reformat(self, x):\n",
        "        return Variable(torch.FloatTensor(x), requires_grad=True)\n",
        "    \n",
        "    def get_per_epoch_batches(self, batch_size):\n",
        "        assert self.train_size % batch_size == 0 # for this dataset, batches can be 1, 2, or 4\n",
        "        train_item_inds = list(range(self.train_size))\n",
        "        random.shuffle(train_item_inds)\n",
        "        batches = []\n",
        "        for batch_start in np.arange(0, self.train_size, batch_size):\n",
        "            batch_inds = train_item_inds[batch_start : batch_start + batch_size]\n",
        "            inputs = [self.train_data[\"inputs\"][i] for i in batch_inds]\n",
        "            targets = [self.train_data[\"targets\"][i] for i in batch_inds]\n",
        "            batches.append((self.reformat(inputs), self.reformat(targets)))\n",
        "        return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JygyGmIfKAWI"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmiuPNaKAWI"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, \n",
        "                 hidden_nonlinearity=\"sigmoid\", use_saved_params=False,\n",
        "                 wrange=[-0.5, 0.5]):\n",
        "        super(Model, self).__init__()\n",
        "        # specify network layers\n",
        "        self.linear_layers = nn.ModuleList(\n",
        "                                [nn.Linear(input_size, hidden_size),\n",
        "                                 nn.Linear(hidden_size, num_classes)])\n",
        "        self.nonlinearities = [self.select_nonlinearity(hidden_nonlinearity), \n",
        "                               self.select_nonlinearity(\"sigmoid\")] # final nonlinearity is always sigmoid\n",
        "            \n",
        "        # initialize weights\n",
        "        for i, layer in enumerate(self.linear_layers):\n",
        "            self.init_weights(i, layer, use_saved_params, wrange)\n",
        "\n",
        "    def record_gnet(self, grad):\n",
        "        self.gnet.append(grad)\n",
        "        \n",
        "    def record_gact(self, grad):\n",
        "        self.gact.append(grad)\n",
        "        \n",
        "    def select_nonlinearity(self, nonlinearity):\n",
        "        if nonlinearity == \"sigmoid\":\n",
        "            return nn.Sigmoid()\n",
        "        elif nonlinearity == \"tanh\":\n",
        "            return nn.Tanh()\n",
        "        elif nonlinearity == \"relu\":\n",
        "            return nn.ReLU()\n",
        "        \n",
        "    def init_weights(self, i, layer, use_saved_params, wrange):\n",
        "        if use_saved_params:\n",
        "            if i == 0: # initial params in original paper\n",
        "                layer.weight.data = torch.tensor([[0.432171, 0.448781],\n",
        "                                                 [-0.038413, 0.036489]])\n",
        "                layer.bias.data = torch.tensor([-0.27659, -0.4025])\n",
        "            elif i == 1:\n",
        "                layer.weight.data = torch.tensor([[0.27208, 0.081714]])\n",
        "                layer.bias.data = torch.tensor([0.2793])\n",
        "        else:\n",
        "            layer.weight.data.uniform_(wrange[0], wrange[1]) # inplace\n",
        "            layer.bias.data.uniform_(wrange[0], wrange[1])\n",
        "\n",
        "    def forward(self, inp, record_data=True):\n",
        "        if record_data:\n",
        "            # for visualization purposes; not necessary to train the model \n",
        "            self.layer_inputs, self.layer_outputs, self.layer_activations = [], [], []\n",
        "            self.gnet, self.gact = [], [] # these are recorded in backward order (the order of the gradient computations)\n",
        "        \n",
        "        out = inp\n",
        "        for layer, nonlinearity in zip(self.linear_layers, self.nonlinearities):\n",
        "            if record_data: # visualization purposes only\n",
        "                self.layer_inputs.append(out)\n",
        "                \n",
        "            # feed through linear layer\n",
        "            out = layer(out)\n",
        "            \n",
        "            if record_data:\n",
        "                self.layer_outputs.append(out)\n",
        "                out.register_hook(self.record_gnet) # dE/dnet\n",
        "                \n",
        "            # apply nonlinearity\n",
        "            out = nonlinearity(out)\n",
        "            \n",
        "            if record_data:\n",
        "                self.layer_activations.append(out)\n",
        "                out.register_hook(self.record_gact) # dE/da\n",
        "        return out           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6J6tYv0KAWJ"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy8LYrlfKAWK"
      },
      "source": [
        "# utils\n",
        "def to_np(t):\n",
        "    return t.data.numpy()\n",
        "\n",
        "def make_dir(path):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path)\n",
        "        \n",
        "def get_logdir():\n",
        "    results_dir = \"logdirs\"\n",
        "    prefix = \"logdir\"\n",
        "    fs = sorted([int(f.split(\"_\")[1]) for f in os.listdir(results_dir) if prefix in f])\n",
        "    if len(fs) == 0:\n",
        "        num = 0\n",
        "    else:\n",
        "        num = fs[-1] + 1\n",
        "    new_f = os.path.join(results_dir, prefix + \"_\" + str(num).zfill(3))\n",
        "    return new_f\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model,\n",
        "                 name=\"\",\n",
        "                 train_batch_size=4,\n",
        "                 num_training_epochs=500, \n",
        "                 stopping_criterion=0.04,\n",
        "                 learning_rate=0.25, momentum=0.9,\n",
        "                 save_freq=1, test_freq=10,\n",
        "                 checkpoint_freq=200, \n",
        "                 print_freq=0, show_plot=False,\n",
        "                 save_dir=None):\n",
        "        self.dataset = dataset\n",
        "        self.model = model\n",
        "        self.name = name\n",
        "        \n",
        "        # training parameters\n",
        "        self.train_batch_size = train_batch_size\n",
        "        assert self.dataset.train_size % self.train_batch_size == 0\n",
        "        self.num_training_epochs = num_training_epochs\n",
        "        self.stopping_criterion = stopping_criterion\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        \n",
        "        # loss function: sum of squared errors\n",
        "        self.criterion = lambda predictions, targets: torch.sum((targets - predictions) ** 2)\n",
        "\n",
        "        self.optimizer = optim.SGD(model.parameters(),\n",
        "                                   lr=self.learning_rate, \n",
        "                                   momentum=self.momentum)\n",
        "        \n",
        "        # viewing & saving parameters\n",
        "        self.save_dir = save_dir\n",
        "        self.print_freq = print_freq\n",
        "        self.save_freq = save_freq\n",
        "        self.test_freq = test_freq\n",
        "        self.checkpoint_freq = checkpoint_freq\n",
        "        self.show_plot = show_plot\n",
        "        self.setup()\n",
        "        \n",
        "    def setup(self):\n",
        "        if self.save_dir is None:\n",
        "            self.save_dir = os.path.join(get_logdir())\n",
        "        self.checkpoints_dir = os.path.join(self.save_dir, \"checkpoint_files_{}\".format(self.name))\n",
        "        [make_dir(p) for p in [self.save_dir, self.checkpoints_dir]]\n",
        "        self.log_file = os.path.join(os.getcwd(), self.save_dir, \"runlog_{}.pkl\".format(self.name))\n",
        "        self.checkpoint_file = os.path.join(self.checkpoints_dir, \"checkpoint.pth\".format(self.name))\n",
        "        \n",
        "        # load checkpoint if already exists\n",
        "        if os.path.exists(self.checkpoint_file):\n",
        "            print(\"Loading from checkpoint: {}\".format(self.checkpoint_file))\n",
        "            checkpoint = torch.load(self.checkpoint_file)\n",
        "            self.start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            self.model.load_state_dict(checkpoint[\"model_state\"])\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "            self.load_results()\n",
        "        else:\n",
        "            self.start_epoch = 0\n",
        "            self.loss_data = {\"epochs\": [], \"train_losses\": []}\n",
        "            self.log = []\n",
        "            \n",
        "    def save_pickle(self, d, fname):\n",
        "        with open(fname, 'wb') as f:\n",
        "            pickle.dump(d, f)\n",
        "\n",
        "    def load_pickle(self, fname):\n",
        "        with open(fname, 'rb') as f:\n",
        "            d = pickle.load(f)\n",
        "        return d\n",
        "\n",
        "    def save_results(self):\n",
        "        info = {\"test_data\": self.log,\n",
        "                \"loss_data\": self.loss_data}\n",
        "        self.save_pickle(info, self.log_file)\n",
        "\n",
        "    def load_results(self):\n",
        "        f = self.load_pickle(self.log_file)\n",
        "        self.log = f[\"test_data\"]\n",
        "        self.loss_data = f[\"loss_data\"]\n",
        "        \n",
        "    def save_checkpoint(self, epoch):\n",
        "        checkpoint = {\"epoch\": epoch,\n",
        "                      \"model_state\": self.model.state_dict(),\n",
        "                      \"optimizer_state\": self.optimizer.state_dict()}\n",
        "        torch.save(checkpoint, self.checkpoint_file)\n",
        "        \n",
        "    def final_save_and_view(self, epoch, loss):\n",
        "        self.eval_test_points(epoch)\n",
        "        self.print_final()\n",
        "        if self.show_plot:\n",
        "            self.plot_loss()\n",
        "        self.save_results()\n",
        "        self.save_checkpoint(epoch)\n",
        "\n",
        "    def init_info_dict(self, epoch, layer_names):\n",
        "        info = {\"enum\": epoch,\n",
        "                \"input\": to_np(self.dataset.reformat(\n",
        "                                    self.dataset.test_data[\"inputs\"])),\n",
        "                \"target\": to_np(self.dataset.reformat(\n",
        "                                    self.dataset.test_data[\"targets\"])),\n",
        "                \"labels\": self.dataset.test_data[\"names\"],\n",
        "                \"loss_sum\": 0., # summed loss over batch items\n",
        "                \"loss\": np.zeros(len(self.dataset.test_data[\"inputs\"]))} # losses each batch items (batch_size-long)\n",
        "        \n",
        "        for i in range(len(self.model.linear_layers)):\n",
        "            layer_info = {\"input_\": None, # layer input (batch_sz x layer_inp_sz), as recorded in self.model.layer_inputs\n",
        "                          \"weights\": copy.deepcopy(to_np(self.model.linear_layers[i].weight)),\n",
        "                          \"biases\": copy.deepcopy(to_np(self.model.linear_layers[i].bias)),\n",
        "                          \"net\": None, # output of linear layer, before nonlinearity (batch_sz x layer_output_sz), as recorded in self.model.layer_outputs\n",
        "                          \"act\": None, # layer activations (batch_sz x layer_output_sz), as recorded in self.model.layer_activations\n",
        "                          \"gweights\": None, # dE/dW (batch_sz x weight_sz[0] x weight_sz[1])\n",
        "                          \"gbiases\": None, # dE/dB (batch_sz x bias_sz[0])\n",
        "                          \"gnet\": None, #dE/dnet (batch_sz x layer_output_sz)\n",
        "                          \"gact\": None, #dE/da (batch_sz x layer_output_sz)\n",
        "                          \"sgweights\": None, # gweights summed across test items (weight_sz[0] x weight_sz[1])\n",
        "                          \"sgbiases\": None} # gbiases summed across test items (bias_sz-long)\n",
        "            info[layer_names[i]] = layer_info\n",
        "        return info\n",
        "\n",
        "    def eval_test_points(self, epoch, print_info=False):\n",
        "        # record gradients w.r.t. each datapoint for visualization purposes only\n",
        "        layer_names = [\"layer{}\".format(i) for i in range(len(self.model.linear_layers))]\n",
        "        info = self.init_info_dict(epoch, layer_names)  \n",
        "    \n",
        "        # enumerate the test data so that we can view the gradients \n",
        "        # for each test point individually. Normally, you will want to feed\n",
        "        # the test data through the model as a single batch.\n",
        "        for i in range(len(self.dataset.test_data[\"inputs\"])):\n",
        "            test_input = self.dataset.reformat(\n",
        "                            [self.dataset.test_data[\"inputs\"][i]])\n",
        "            test_target = self.dataset.reformat(\n",
        "                            [self.dataset.test_data[\"targets\"][i]])\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            test_prediction = self.model.forward(test_input, record_data=True)\n",
        "            test_loss = self.criterion(test_prediction, test_target)\n",
        "\n",
        "            # Here, we call the backward method in order to compute gradients for\n",
        "            # visualization purposes. In your models, do NOT do this during test\n",
        "            # evaluation! It can lead to training on the test set. \n",
        "            test_loss.backward()\n",
        "\n",
        "            info[\"loss\"][i] = to_np(test_loss)\n",
        "            info[\"loss_sum\"] += to_np(test_loss)\n",
        "            \n",
        "            # reverse to correspond to the forward direction\n",
        "            self.model.gnet.reverse()\n",
        "            self.model.gact.reverse()\n",
        "\n",
        "            # update info dict\n",
        "            for j in range(len(self.model.linear_layers)):\n",
        "                layer_name = layer_names[j]\n",
        "                \n",
        "                def update(k, new_v, expand=False):\n",
        "                    new_v = copy.deepcopy(to_np(new_v))\n",
        "                    if expand == True:\n",
        "                        new_v = np.expand_dims(new_v, 0)\n",
        "                    if i == 0:\n",
        "                        info[layer_name][k] = new_v\n",
        "                    else:\n",
        "                        info[layer_name][k] = np.vstack((info[layer_name][k], new_v))\n",
        "                        \n",
        "                update(\"input_\", self.model.layer_inputs[j])\n",
        "                update(\"net\", self.model.layer_outputs[j])\n",
        "                update(\"act\", self.model.layer_activations[j])\n",
        "                update(\"gweights\", self.model.linear_layers[j].weight.grad, expand=True)\n",
        "                update(\"gbiases\", self.model.linear_layers[j].bias.grad, expand=True)\n",
        "                update(\"gnet\", self.model.gnet[j])\n",
        "                update(\"gact\", self.model.gact[j])\n",
        "                    \n",
        "        for i in range(len(self.model.linear_layers)):\n",
        "            layer_name = layer_names[i]\n",
        "            info[layer_name][\"sgweights\"] = np.sum(info[layer_name][\"gweights\"] , 0)\n",
        "            info[layer_name][\"sgbiases\"] = np.sum(info[layer_name][\"gbiases\"] , 0)\n",
        "\n",
        "        if print_info:\n",
        "            print(\"\\n------------------------\")\n",
        "            self.print_dict(info)\n",
        "        self.log.append(info)\n",
        "\n",
        "    def print_dict(self, d):\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, dict):\n",
        "                print(\"\\n{}\".format(k))\n",
        "                self.print_dict(v)\n",
        "            else:\n",
        "                print(\"\\n{}\".format(k))\n",
        "                print(v)\n",
        "                if isinstance(v, np.ndarray):\n",
        "                    print(v.shape)\n",
        "            \n",
        "    def print_progress(self, epoch, loss):\n",
        "        print('Epoch {}: {}'.format(epoch, loss))\n",
        "        \n",
        "    def print_final(self):\n",
        "        out = \"Run {}:\".format(self.name)\n",
        "        out += \" Loss at epoch {}:\".format(self.loss_data[\"epochs\"][0])\n",
        "        out += \" {0:.4f};\".format(self.loss_data[\"train_losses\"][0])\n",
        "        out += \" Last epoch: {};\".format(self.loss_data[\"epochs\"][-1])\n",
        "        out += \" Loss on last epoch: {0:.4f}\".format(self.loss_data[\"train_losses\"][-1])\n",
        "        print(out)\n",
        "\n",
        "    def plot_loss(self):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(self.loss_data[\"epochs\"], self.loss_data[\"train_losses\"])\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        max_loss = max(self.loss_data[\"train_losses\"])\n",
        "        ax.set_xlim([0, self.num_training_epochs])\n",
        "        ax.set_ylim([0, max_loss + 0.1])\n",
        "        ax.set_title('Training Progress')\n",
        "\n",
        "    def train(self):\n",
        "        if self.start_epoch == self.num_training_epochs - 1:\n",
        "            # already trained\n",
        "            return\n",
        "        \n",
        "        for epoch in range(self.start_epoch, self.num_training_epochs):\n",
        "            # visualization purposes only\n",
        "            if (epoch % self.test_freq) == 0:\n",
        "                self.eval_test_points(epoch)\n",
        "            \n",
        "            # divide the training data into batches; \n",
        "            # we see each training item exactly once per epoch\n",
        "            train_batches = self.dataset.get_per_epoch_batches(self.train_batch_size)\n",
        "            \n",
        "            loss_per_batch = []\n",
        "            for batch in train_batches:\n",
        "                # grab batch of training data\n",
        "                inputs, targets = batch[0], batch[1]\n",
        "            \n",
        "                # zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # get model predictions given inputs\n",
        "                predictions = self.model.forward(inputs, record_data=False)\n",
        "\n",
        "                # compute loss\n",
        "                loss = self.criterion(predictions, targets)\n",
        "                loss_per_batch.append(loss) # record\n",
        "                \n",
        "                # update\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "            # compute epoch loss (for visualization purposes)\n",
        "            epoch_loss = sum(loss_per_batch) # add per-batch losses to get total loss this epoch\n",
        "            \n",
        "            # record\n",
        "            self.loss_data[\"epochs\"].append(epoch)\n",
        "            self.loss_data[\"train_losses\"].append(float(epoch_loss.data.numpy()))\n",
        "            \n",
        "            if self.print_freq > 0:\n",
        "                if (epoch % self.print_freq) == 0:\n",
        "                    self.print_progress(epoch, epoch_loss)\n",
        "                \n",
        "            if (epoch % self.save_freq) == 0:\n",
        "                self.save_results()\n",
        "                \n",
        "            if (epoch % self.checkpoint_freq) == 0:\n",
        "                self.save_checkpoint(epoch)\n",
        "            \n",
        "            if epoch_loss <= self.stopping_criterion:\n",
        "                self.final_save_and_view(epoch, epoch_loss)\n",
        "                return\n",
        "            \n",
        "            if epoch == self.num_training_epochs - 1:\n",
        "                self.final_save_and_view(epoch, epoch_loss)\n",
        "                \n",
        "def train_multiple_runs(num_runs, \n",
        "                        hidden_size, \n",
        "                        hidden_nonlinearity,\n",
        "                        use_saved_params, \n",
        "                        weight_initialization_range,\n",
        "                        num_training_epochs, \n",
        "                        learning_rate, momentum,\n",
        "                        stopping_criterion,\n",
        "                        train_batch_size):     \n",
        "    make_dir(\"logdirs\")\n",
        "    save_dir = get_logdir()\n",
        "    print('Results for this runset saved in: {}'.format(save_dir))\n",
        "    results = {r: {\"epochs\": None, \"train_losses\": None} for r in range(num_runs)}\n",
        "    dataset = Dataset()\n",
        "    \n",
        "    for r in range(num_runs):\n",
        "        model = Model(input_size=2, hidden_size=hidden_size, num_classes=1, \n",
        "                  hidden_nonlinearity=hidden_nonlinearity, \n",
        "                  use_saved_params=use_saved_params,\n",
        "                  wrange=weight_initialization_range)\n",
        "        \n",
        "        trainer = Trainer(dataset, model, \n",
        "                          name=\"{}\".format(r),\n",
        "                          train_batch_size=train_batch_size,\n",
        "                          num_training_epochs=num_training_epochs,\n",
        "                          stopping_criterion=stopping_criterion,\n",
        "                          learning_rate=learning_rate, \n",
        "                          momentum=momentum, \n",
        "                          save_dir=save_dir)\n",
        "        trainer.train()\n",
        "\n",
        "        results[r][\"epochs\"] = trainer.loss_data[\"epochs\"]\n",
        "        results[r][\"train_losses\"] = trainer.loss_data[\"train_losses\"]\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAFuSTmKKAWP"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "My7GajmTKAWV"
      },
      "source": [
        "def compute_mean_loss_across_runs(results, num_runs, num_training_epochs):\n",
        "    mean_train_losses = np.zeros(num_training_epochs)\n",
        "    \n",
        "    def pad_with_final_loss(epochs, losses):\n",
        "        # accounts for early stopping on some runs\n",
        "        last_recorded_epoch = epochs[-1]\n",
        "        padding = losses[-1] * np.ones(num_training_epochs - (last_recorded_epoch + 1))\n",
        "        return np.concatenate((losses, padding))\n",
        "    \n",
        "    for r in range(num_runs):\n",
        "        losses_this_run = results[r][\"train_losses\"]\n",
        "        if len(losses_this_run) < num_training_epochs:\n",
        "            losses_this_run = pad_with_final_loss(results[r][\"epochs\"], \n",
        "                                                  losses_this_run)\n",
        "        mean_train_losses += losses_this_run\n",
        "    return mean_train_losses/num_runs\n",
        "\n",
        "def plot_results_by_run(num_runs, num_training_epochs,\n",
        "                        results, mean_training_loss_across_runs):\n",
        "    fig, ax = plt.subplots(figsize=[12,8]) #width and height of plot â€“ change to suit your preference\n",
        "    for r in range(num_runs):\n",
        "        label = \"Run {}\".format(r)\n",
        "        if r == 0:\n",
        "            max_loss_across_runs = max(results[r][\"train_losses\"])\n",
        "        else:\n",
        "            max_loss_across_runs = max(max_loss_across_runs, \n",
        "                                       max(results[r][\"train_losses\"]))\n",
        "        ax.plot(results[r][\"epochs\"], results[r][\"train_losses\"], label=label)\n",
        "        \n",
        "    # add mean across runs\n",
        "    ax.plot(range(num_training_epochs), mean_training_loss_across_runs, \n",
        "            '--', label=\"mean\")\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_ylim([0., max_loss_across_runs])\n",
        "    ax.legend()\n",
        "    ax.set_title('Results by Run')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agKrypXxKAWV"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "71F8FZO3KAWW"
      },
      "source": [
        "# run a single runset\n",
        "\n",
        "def main():\n",
        "    num_runs = 2 # 1 for Ex. 1; 8 for Ex 2.1; 10 for Ex. 2.2\n",
        "    \n",
        "    # PARAMETERS \n",
        "    # training params - don't change these\n",
        "    num_training_epochs = 500\n",
        "    stopping_criterion = 0.04\n",
        "    \n",
        "    # optimization params\n",
        "    learning_rate = 0.25 # 0.25\n",
        "    momentum = 0.9 # 0.9\n",
        "    \n",
        "    # initialization/scheduling params\n",
        "    use_saved_params = True # set to True for Exercise 1, then False for Exercise 2\n",
        "    weight_initialization_range = [-0.5, 0.5] # [-0.5, 0.5] used to initialize weights if used_save_params is False\n",
        "    if use_saved_params:\n",
        "        weight_initialization_range = None\n",
        "    train_batch_size = 4 # 4\n",
        "    \n",
        "    # model params\n",
        "    hidden_size = 2 # 2\n",
        "    hidden_nonlinearity = \"sigmoid\" # \"sigmoid\"; other options: \"relu\" or \"tanh\"    \n",
        "\n",
        "    # TRAIN (1 RUNSET)\n",
        "    results_by_run = train_multiple_runs(num_runs, \n",
        "                                         hidden_size, hidden_nonlinearity,\n",
        "                                         use_saved_params, \n",
        "                                         weight_initialization_range,\n",
        "                                         num_training_epochs, \n",
        "                                         learning_rate, momentum,\n",
        "                                         stopping_criterion,\n",
        "                                         train_batch_size)\n",
        "\n",
        "    # VISUALIZE\n",
        "    mean_training_loss_across_runs = compute_mean_loss_across_runs(\n",
        "                                         results_by_run, num_runs, \n",
        "                                         num_training_epochs)\n",
        "\n",
        "    plot_results_by_run(num_runs, num_training_epochs,\n",
        "                        results_by_run, mean_training_loss_across_runs)\n",
        "    \n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LXtJWSeZjzr"
      },
      "source": [
        "# Visualize XOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNS_pUMbKAWW"
      },
      "source": [
        "from FFBP.vis_utils import view_layers, view_layers_colab\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def logistic(net):\n",
        "    return np.divide(1, (1 + np.exp(-net)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npXNDqvUZnod"
      },
      "source": [
        "logdir = 'logdirs/logdir_000' # change the digits to visualize results in the desired log\n",
        "\n",
        "view_layers_colab(\n",
        "    logdir = logdir, \n",
        "    mode = 2,\n",
        "    show_values=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPWHpfGsiLSE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}