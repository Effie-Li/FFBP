{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR problem\n",
    "\n",
    "## 1. Preliminaries\n",
    "### 1.1. Imports\n",
    "We begin by importing several python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.1.0\n",
      "numpy version: 1.12.1\n",
      "current working directory: /Users/alexten/Projects/pdpyflow/xor\n",
      "tensorboard logdir path: /Users/alexten/Projects/pdpyflow/xor/train/log0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "i=0\n",
    "logdir = os.getcwd()+'/train/log0'\n",
    "while os.path.exists(logdir):\n",
    "    i+=1\n",
    "    logdir = os.getcwd()+'/train/log{}'.format(i)\n",
    "os.makedirs(logdir)\n",
    "    \n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('current working directory: {}'.format(os.getcwd()))\n",
    "print('tensorboard logdir path: {}'.format(logdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv_file(filename_queue, batch_size, default_val, inp_size, targ_size, labels):\n",
    "    reader = tf.TextLineReader(skip_header_lines=True, name='csv_reader')\n",
    "    _, csv_row = reader.read_up_to(filename_queue, batch_size)\n",
    "    defaults = [[default_val] for x in range(inp_size + targ_size)]\n",
    "    if labels is True: \n",
    "        defaults.insert(0,[''])\n",
    "    examples = tf.decode_csv(csv_row, record_defaults=defaults)\n",
    "    l = tf.transpose(examples.pop(0))\n",
    "    x = tf.transpose(tf.stack(examples[0:inp_size]))\n",
    "    t = tf.transpose(tf.stack(examples[inp_size:inp_size + targ_size]))\n",
    "    return l, x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Training environment and input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "num_epochs = 330\n",
    "batch_size = 4\n",
    "inp_size = 2\n",
    "targ_size = 1\n",
    "\n",
    "# QUEUES\n",
    "with tf.name_scope('Input_pipeline'):\n",
    "    input_queue = tf.train.string_input_producer(\n",
    "                    ['train_data_B.txt'], \n",
    "                    num_epochs = num_epochs, \n",
    "                    shuffle = False\n",
    "    )\n",
    "\n",
    "    labels, inp_batch, targ_batch = read_csv_file(\n",
    "                                        filename_queue = input_queue,\n",
    "                                        batch_size = batch_size,\n",
    "                                        default_val = 0.0,\n",
    "                                        inp_size = inp_size,\n",
    "                                        targ_size = targ_size,\n",
    "                                        labels = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "hidden_size = 2\n",
    "wrange = [-1,1]\n",
    "seed = None # Use None for random seed value\n",
    "lr = 0.5\n",
    "m = 0.9\n",
    "use_exercise_params = True\n",
    "ckpt_freq = 1\n",
    "ecrit = 0.01\n",
    "\n",
    "# NETWORK CONSTRUCTION\n",
    "with tf.name_scope('XOR_model'):\n",
    "    model_inp = tf.placeholder(dtype = tf.float32, shape=[batch_size, inp_size], name='model_inp')\n",
    "\n",
    "    # HIDDEN LAYER\n",
    "    with tf.name_scope('hidden_layer'):\n",
    "        \n",
    "        with tf.name_scope('weights'):\n",
    "            hidden_W = tf.Variable(\n",
    "                        tf.random_uniform(\n",
    "                            minval = wrange[0], \n",
    "                            maxval = wrange[1],\n",
    "                            seed = seed,\n",
    "                            shape = [inp_size, hidden_size],\n",
    "                            dtype=tf.float32\n",
    "                        )\n",
    "            )\n",
    "            tf.summary.tensor_summary('tensor_summary', hidden_W)\n",
    "                             \n",
    "        with tf.name_scope('biases'):\n",
    "            hidden_b = tf.Variable(\n",
    "                        tf.random_uniform(\n",
    "                            minval = wrange[0],\n",
    "                            maxval = wrange[1],\n",
    "                            seed = seed,\n",
    "                            shape = [hidden_size],\n",
    "                            dtype = tf.float32\n",
    "                        )\n",
    "            )\n",
    "            tf.summary.tensor_summary('tensor_summary', hidden_b)\n",
    "\n",
    "        with tf.name_scope('net_input'):\n",
    "            hidden_net = tf.nn.xw_plus_b(model_inp, hidden_W, hidden_b)\n",
    "            tf.summary.tensor_summary('tensor_summary', hidden_net)\n",
    "        \n",
    "        with tf.name_scope('activations'):\n",
    "            hidden_acts = tf.nn.sigmoid(hidden_net)\n",
    "            tf.summary.tensor_summary('tensor_summary', hidden_acts)\n",
    "        \n",
    "    # OUTPUT LAYER\n",
    "    with tf.name_scope('output_layer'):\n",
    "        \n",
    "        with tf.name_scope('weights'):\n",
    "            output_W = tf.Variable(\n",
    "                        tf.random_uniform(\n",
    "                            minval = wrange[0], \n",
    "                            maxval = wrange[1],\n",
    "                            seed = seed,\n",
    "                            shape = [hidden_size, targ_size],\n",
    "                            dtype=tf.float32\n",
    "                        )\n",
    "            )\n",
    "            tf.summary.tensor_summary('tensor_summary', output_W)\n",
    "                          \n",
    "        with tf.name_scope('biases'):\n",
    "            output_b = tf.Variable(\n",
    "                        tf.random_uniform(\n",
    "                            minval = wrange[0],\n",
    "                            maxval = wrange[1],\n",
    "                            seed = seed,\n",
    "                            shape = [targ_size],\n",
    "                            dtype = tf.float32\n",
    "                        )\n",
    "            )\n",
    "            tf.summary.tensor_summary('tensor_summary', output_b)\n",
    "        \n",
    "        with tf.name_scope('net_input'):\n",
    "            output_net = tf.nn.xw_plus_b(hidden_acts, output_W, output_b)\n",
    "            tf.summary.tensor_summary('tensor_summary', output_net)\n",
    "        \n",
    "        with tf.name_scope('activations'):\n",
    "            output_acts = tf.nn.sigmoid(output_net, name='activations')\n",
    "            tf.summary.tensor_summary('tensor_summary', output_acts)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    target = tf.placeholder(dtype = tf.float32, shape=[batch_size, targ_size], name='model_inp')\n",
    "    squared_error = tf.reduce_sum(tf.squared_difference(target, output_acts),\n",
    "                                name='squared_error')\n",
    "    tf.summary.scalar('error_summary', squared_error)\n",
    "    train_step = tf.train.MomentumOptimizer(lr, m).minimize(squared_error)\n",
    "\n",
    "merge_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from exercise_params/exercise_params\n",
      "epoch 0: 1.0506523847579956\n",
      "epoch 30: 1.0019646883010864\n",
      "epoch 60: 1.0000269412994385\n",
      "epoch 90: 0.9999931454658508\n",
      "epoch 120: 0.999982476234436\n",
      "epoch 150: 0.9999575018882751\n",
      "epoch 180: 0.9998661279678345\n",
      "epoch 210: 0.9992285370826721\n",
      "epoch 240: 0.9822084903717041\n",
      "epoch 270: 0.6883569359779358\n",
      "epoch 300: 0.043961603194475174\n",
      "stop epoch 317: 0.009680739603936672\n"
     ]
    }
   ],
   "source": [
    "sum_freq = 30 # (num_epochs // 10)\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if use_exercise_params:\n",
    "        restore_dict = {'w_1': hidden_W,'b_1': hidden_b,'w_2': output_W,'b_2': output_b}\n",
    "        restore_op = tf.train.Saver(restore_dict)\n",
    "        restore_op.restore(sess, 'exercise_params/exercise_params')\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coordinator)\n",
    "    for i in range(num_epochs+1):\n",
    "            try:\n",
    "                x, t = sess.run([inp_batch, targ_batch])\n",
    "                _, loss, summary = sess.run([train_step, squared_error, merge_summaries], feed_dict={model_inp: x, target: t})\n",
    "                if i % sum_freq == 0 or i == num_epochs - 1:\n",
    "                    summary_writer.add_summary(summary, i)\n",
    "                    print('epoch {}: {}'.format(i,loss))\n",
    "                if loss < ecrit:\n",
    "                    summary_writer.add_summary(summary, i)\n",
    "                    print('stop epoch {}: {}'.format(i,loss))\n",
    "                    summary_writer.close()\n",
    "                    break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Reached the end of trainining set')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # might be needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
